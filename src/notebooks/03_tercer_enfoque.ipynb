{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TERCER ENFOQUE DE MI PROYECTO\n",
    "Cambiar a CLASIFICACIÓN, tener como variable target la variedad del vino y predecirla gracias a las variables country, designation y winery\n",
    "\n",
    "            (Aquí también puedo basarme en mi story telling para justificar esta elección.\n",
    "            Siempre fiel al hilo de querer crear un recomendador de vinos, esta vez quiero predecir la variedad del vino, \n",
    "            igual para recomendar a amigos, y por supuesto no quiero recomendar vinos que sean super raros y difíciles de encontrar. \n",
    "            Por lo tanto acotaré primero por el \"variety\", quedándome SOLO con aquellas variedades de las que hay más de 100.\n",
    "            Depues, analizaré el nuevo dataframe que se ha quedado y veré si allí quedan \"designation\" o \"winery\" también poco frecuentes, \n",
    "            y si se diera el caso, cortaría también de allí)\n",
    "\n",
    "            Finalmente, con estos datos relimpiados y recortados, escogería los 3 mejores modelos de los que ya tenía de antes (de regresión),\n",
    "            y los pasaría a clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Primero, cargo todas las librerias \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import re\n",
    "\n",
    "from scipy.stats import iqr   ### para función que quita los outliers\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RepeatedKFold, ParameterGrid\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix , r2_score, mean_absolute_error, mean_squared_error    \n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, HistGradientBoostingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import multiprocessing\n",
    "from sklearn.svm import SVC\n",
    "import xgboost\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "## PARA LAS TRANSFORMACIONES\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import skew\n",
    "from scipy import stats, special\n",
    "\n",
    "#para guardar el modelo de ML\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cargo el dataframe ya limpio del primer análisis\n",
    "df = pd.read_csv(\"../data/processed/vinos_editado.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n",
       "      <td>Reserve Late Harvest</td>\n",
       "      <td>87</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>Lake Michigan Shore</td>\n",
       "      <td>Alexander Peartree</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>St. Julian</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>Much like the regular bottling from 2012, this...</td>\n",
       "      <td>Vintner's Reserve Wild Child Block</td>\n",
       "      <td>87</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Sweet Cheeks</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spain</td>\n",
       "      <td>Blackberry and raspberry aromas show a typical...</td>\n",
       "      <td>Ars In Vitro</td>\n",
       "      <td>87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Northern Spain</td>\n",
       "      <td>Navarra</td>\n",
       "      <td>Michael Schachner</td>\n",
       "      <td>Tempranillo-Merlot</td>\n",
       "      <td>Tandem</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Here's a bright, informal red that opens with ...</td>\n",
       "      <td>Belsito</td>\n",
       "      <td>87</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Sicily &amp; Sardinia</td>\n",
       "      <td>Vittoria</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>Frappato</td>\n",
       "      <td>Terre di Giurfo</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>France</td>\n",
       "      <td>This has great depth of flavor with its fresh ...</td>\n",
       "      <td>Les Natures</td>\n",
       "      <td>87</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>Pinot Gris</td>\n",
       "      <td>Jean-Baptiste Adam</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country                                        description  \\\n",
       "0      US  Pineapple rind, lemon pith and orange blossom ...   \n",
       "1      US  Much like the regular bottling from 2012, this...   \n",
       "2   Spain  Blackberry and raspberry aromas show a typical...   \n",
       "3   Italy  Here's a bright, informal red that opens with ...   \n",
       "4  France  This has great depth of flavor with its fresh ...   \n",
       "\n",
       "                          designation  points  price           province  \\\n",
       "0                Reserve Late Harvest      87   13.0           Michigan   \n",
       "1  Vintner's Reserve Wild Child Block      87   65.0             Oregon   \n",
       "2                        Ars In Vitro      87   15.0     Northern Spain   \n",
       "3                             Belsito      87   16.0  Sicily & Sardinia   \n",
       "4                         Les Natures      87   27.0             Alsace   \n",
       "\n",
       "              region_1         taster_name             variety  \\\n",
       "0  Lake Michigan Shore  Alexander Peartree            Riesling   \n",
       "1    Willamette Valley        Paul Gregutt          Pinot Noir   \n",
       "2              Navarra   Michael Schachner  Tempranillo-Merlot   \n",
       "3             Vittoria       Kerin O’Keefe            Frappato   \n",
       "4               Alsace          Roger Voss          Pinot Gris   \n",
       "\n",
       "               winery  year  \n",
       "0          St. Julian  2013  \n",
       "1        Sweet Cheeks  2012  \n",
       "2              Tandem  2011  \n",
       "3     Terre di Giurfo  2013  \n",
       "4  Jean-Baptiste Adam  2012  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REDUCIR DATOS\n",
    "Para que el recomendador de vinos se adapte mejor a mi entorno más cercano, que es dónde se usará, y así poder recomendar con bastante confianza unos vinos que sean relativamente fáciles de encontrar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIMPIEZA INICIAL \"VARIETY\"  --> me quedo con las que producen más de 100 botellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Haré una \"limpieza inicial\", quedándome solo con los vinos de los que haya más de 100 botellas por variedad\n",
    "## (si quiero recomendar vinos, quiero que sean vinos relativamente fáciles de encontrar)\n",
    "## luego ya iré viendo si necesito acotar más"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pinot Noir</th>\n",
       "      <td>6076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chardonnay</th>\n",
       "      <td>4329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Red Blend</th>\n",
       "      <td>4248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabernet Sauvignon</th>\n",
       "      <td>2872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bordeaux-style Red Blend</th>\n",
       "      <td>2044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Picapoll</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Syrah-Bonarda</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tinta Madeira</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Centesimino</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bobal-Cabernet Sauvignon</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          variety\n",
       "Pinot Noir                   6076\n",
       "Chardonnay                   4329\n",
       "Red Blend                    4248\n",
       "Cabernet Sauvignon           2872\n",
       "Bordeaux-style Red Blend     2044\n",
       "...                           ...\n",
       "Picapoll                        1\n",
       "Syrah-Bonarda                   1\n",
       "Tinta Madeira                   1\n",
       "Centesimino                     1\n",
       "Bobal-Cabernet Sauvignon        1\n",
       "\n",
       "[428 rows x 1 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frecuencias_variedades = pd.DataFrame(df['variety'].value_counts().sort_values(ascending=False))\n",
    "frecuencias_variedades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total variedades de las que hay más de 100 unidades: 47\n"
     ]
    }
   ],
   "source": [
    "mask = frecuencias_variedades[\"variety\"] > 100  ## defino la máscara\n",
    "\n",
    "print(\"Total variedades de las que hay más de 100 unidades:\",len(frecuencias_variedades[mask]))  ## como ya sabía de mi análisis anterior, son 47 las variedades de vino que tienen más de 100 unidades\n",
    "                                   ## me voy a quedar solo con éstas para seguir con mis modelos\n",
    "vars_100 = frecuencias_variedades[mask]\n",
    "# print(\"Primeras 3 variedades de la lista:\\n\", vars_100.head(3))\n",
    "# print(\"Últimas 3 variedades de la lista:\\n\", vars_100.tail(3))  ## me quedo hasta la variedad \"G-S-M\" incluida\n",
    "## quiero pasar el índice (los nombres de las variedades) a la columna normal, para luego unirlo al dataframe grade y quedarme solo con estas variedades\n",
    "\n",
    "# vars_100[\"variety_100\"] = vars_100.index\n",
    "# vars_100.head()   ### añado la columna \"variety_100\" con el nombre de las 47 variedades con las que me quedo\n",
    "#                     ### no pierdo tiempo en resetear el index, ya que ahora fusionaré esta columa con el dataframe principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pinot Noir': 0, 'Chardonnay': 1, 'Red Blend': 2, 'Cabernet Sauvignon': 3, 'Bordeaux-style Red Blend': 4, 'Syrah': 5, 'Riesling': 6, 'Malbec': 7, 'Rosé': 8, 'Tempranillo': 9, 'Nebbiolo': 10, 'Sauvignon Blanc': 11, 'Zinfandel': 12, 'White Blend': 13, 'Rhône-style Red Blend': 14, 'Sangiovese': 15, 'Merlot': 16, 'Pinot Gris': 17, 'Cabernet Franc': 18, 'Gamay': 19, 'Sparkling Blend': 20, 'Tempranillo Blend': 21, 'Gewürztraminer': 22, 'Shiraz': 23, 'Viognier': 24, 'Grenache': 25, 'Champagne Blend': 26, 'Rhône-style White Blend': 27, 'Petite Sirah': 28, 'Chenin Blanc': 29, 'Barbera': 30, 'Garnacha': 31, 'Melon': 32, 'Bordeaux-style White Blend': 33, 'Albariño': 34, 'Pinot Grigio': 35, 'Torrontés': 36, 'Pinot Blanc': 37, 'Glera': 38, 'Tinta de Toro': 39, \"Nero d'Avola\": 40, 'Verdejo': 41, 'Petit Verdot': 42, 'Mourvèdre': 43, 'Aglianico': 44, 'Mencía': 45, 'G-S-M': 46}\n"
     ]
    }
   ],
   "source": [
    "# creo un diccionario con las etiquetas de los vinos y sus respectivos números\n",
    "# ordenados según la cantidad de cada vino\n",
    "char2idx = {u:i for i, u in enumerate(vars_100.index)}\n",
    "print(char2idx)   ### para ver el diccionario que me ha creado\n",
    "\n",
    "#### AHORA TENGO QUE PASARLOS A DATAFRAME, AL NUEVO DATAFRAME DE TODOS VALORES NUMERICOS\n",
    "#### QUE USARÉ PARA LOS MODELOS DE MACHINE LEARNING\n",
    "\n",
    "df_variety_num = pd.DataFrame([[key, char2idx[key]] for key in char2idx.keys()], columns=['variety', 'variety_100'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_variety_num[\"variety_100\"] = (df_variety_num[\"variety_100\"] + 1)\n",
    "vars_100 = df_variety_num.sort_values(by= \"variety_100\")\n",
    " #incremento de uno para que si luego tuviera que escalar esta columna no me de error (me pasó en el anterior análisis así que por si acaso lo hago ya)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variety</th>\n",
       "      <th>variety_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Red Blend</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bordeaux-style Red Blend</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    variety  variety_100\n",
       "0                Pinot Noir            1\n",
       "1                Chardonnay            2\n",
       "2                 Red Blend            3\n",
       "3        Cabernet Sauvignon            4\n",
       "4  Bordeaux-style Red Blend            5"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars_100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(vars_100, how=\"outer\")  ## ahora los tengo unidos, después haré un dropna de va columna variety_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 47142 entries, 0 to 47141\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   country      47142 non-null  object \n",
      " 1   description  47142 non-null  object \n",
      " 2   designation  47142 non-null  object \n",
      " 3   points       47142 non-null  int64  \n",
      " 4   price        47142 non-null  float64\n",
      " 5   province     47142 non-null  object \n",
      " 6   region_1     47142 non-null  object \n",
      " 7   taster_name  47142 non-null  object \n",
      " 8   variety      47142 non-null  object \n",
      " 9   winery       47142 non-null  object \n",
      " 10  year         47142 non-null  int64  \n",
      " 11  variety_100  42770 non-null  float64\n",
      "dtypes: float64(2), int64(2), object(8)\n",
      "memory usage: 4.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()  ## variety_100  42770 non-null -->> mi dataframe tendrá que tener este largo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"variety_100\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 42770 entries, 0 to 45436\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   country      42770 non-null  object \n",
      " 1   description  42770 non-null  object \n",
      " 2   designation  42770 non-null  object \n",
      " 3   points       42770 non-null  int64  \n",
      " 4   price        42770 non-null  float64\n",
      " 5   province     42770 non-null  object \n",
      " 6   region_1     42770 non-null  object \n",
      " 7   taster_name  42770 non-null  object \n",
      " 8   variety      42770 non-null  object \n",
      " 9   winery       42770 non-null  object \n",
      " 10  year         42770 non-null  int64  \n",
      " 11  variety_100  42770 non-null  float64\n",
      "dtypes: float64(2), int64(2), object(8)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop([\"variety\"], axis = 1, inplace=True)  ## ahora la columna \"variety\" la tengo doble, la borro y me quedo con la que se llama \"variety_100\"\n",
    "### la borro luego, al final de esta seccion (para tener ambas versiones de csv y que sean de fácil acceso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "      <th>year</th>\n",
       "      <th>variety_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>42770</td>\n",
       "      <td>42770</td>\n",
       "      <td>42770</td>\n",
       "      <td>42770.00000</td>\n",
       "      <td>42770.000000</td>\n",
       "      <td>42770</td>\n",
       "      <td>42770</td>\n",
       "      <td>42770</td>\n",
       "      <td>42770</td>\n",
       "      <td>42770</td>\n",
       "      <td>42770.000000</td>\n",
       "      <td>42770.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7</td>\n",
       "      <td>42764</td>\n",
       "      <td>20498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>903</td>\n",
       "      <td>17</td>\n",
       "      <td>47</td>\n",
       "      <td>7827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>US</td>\n",
       "      <td>In 2009 this single vineyard offering includes...</td>\n",
       "      <td>Reserve</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>California</td>\n",
       "      <td>Columbia Valley (WA)</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Columbia Crest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>22094</td>\n",
       "      <td>2</td>\n",
       "      <td>1030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11887</td>\n",
       "      <td>2141</td>\n",
       "      <td>6971</td>\n",
       "      <td>6076</td>\n",
       "      <td>137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89.09509</td>\n",
       "      <td>40.540659</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011.445055</td>\n",
       "      <td>10.106664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.02400</td>\n",
       "      <td>36.387473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.290666</td>\n",
       "      <td>10.058636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1904.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87.00000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89.00000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.00000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>850.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country                                        description designation  \\\n",
       "count    42770                                              42770       42770   \n",
       "unique       7                                              42764       20498   \n",
       "top         US  In 2009 this single vineyard offering includes...     Reserve   \n",
       "freq     22094                                                  2        1030   \n",
       "mean       NaN                                                NaN         NaN   \n",
       "std        NaN                                                NaN         NaN   \n",
       "min        NaN                                                NaN         NaN   \n",
       "25%        NaN                                                NaN         NaN   \n",
       "50%        NaN                                                NaN         NaN   \n",
       "75%        NaN                                                NaN         NaN   \n",
       "max        NaN                                                NaN         NaN   \n",
       "\n",
       "             points         price    province              region_1  \\\n",
       "count   42770.00000  42770.000000       42770                 42770   \n",
       "unique          NaN           NaN          58                   903   \n",
       "top             NaN           NaN  California  Columbia Valley (WA)   \n",
       "freq            NaN           NaN       11887                  2141   \n",
       "mean       89.09509     40.540659         NaN                   NaN   \n",
       "std         3.02400     36.387473         NaN                   NaN   \n",
       "min        80.00000      4.000000         NaN                   NaN   \n",
       "25%        87.00000     20.000000         NaN                   NaN   \n",
       "50%        89.00000     30.000000         NaN                   NaN   \n",
       "75%        91.00000     50.000000         NaN                   NaN   \n",
       "max       100.00000    850.000000         NaN                   NaN   \n",
       "\n",
       "       taster_name     variety          winery          year   variety_100  \n",
       "count        42770       42770           42770  42770.000000  42770.000000  \n",
       "unique          17          47            7827           NaN           NaN  \n",
       "top     Roger Voss  Pinot Noir  Columbia Crest           NaN           NaN  \n",
       "freq          6971        6076             137           NaN           NaN  \n",
       "mean           NaN         NaN             NaN   2011.445055     10.106664  \n",
       "std            NaN         NaN             NaN      3.290666     10.058636  \n",
       "min            NaN         NaN             NaN   1904.000000      1.000000  \n",
       "25%            NaN         NaN             NaN   2010.000000      3.000000  \n",
       "50%            NaN         NaN             NaN   2012.000000      7.000000  \n",
       "75%            NaN         NaN             NaN   2014.000000     14.000000  \n",
       "max            NaN         NaN             NaN   2017.000000     47.000000  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include=\"all\")   ## comprobación de los valores únicos. Ya sé que de \"variety_num\" tengo 47\n",
    "                                ## de designation tengo más de 20000 y de winery tengo casi 8000, tengo que hacer la misma limpieza con estas 2 columnas para quitarme del medio los valores raros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIMPIEZA INICIAL \"DESIGNATION\"  --> de las que producen más de 100 botellas, me quedo con las que tienen más de 20 viñedos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "      <th>year</th>\n",
       "      <th>variety_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n",
       "      <td>Reserve Late Harvest</td>\n",
       "      <td>87</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>Lake Michigan Shore</td>\n",
       "      <td>Alexander Peartree</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>St. Julian</td>\n",
       "      <td>2013</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>A wisp of bramble extends a savory tone from n...</td>\n",
       "      <td>Ingle Vineyard</td>\n",
       "      <td>88</td>\n",
       "      <td>20.0</td>\n",
       "      <td>New York</td>\n",
       "      <td>Finger Lakes</td>\n",
       "      <td>Anna Lee C. Iijima</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>Heron Hill</td>\n",
       "      <td>2015</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>Dusty mineral, smoke and struck flint lend a s...</td>\n",
       "      <td>Red Oak Vineyard</td>\n",
       "      <td>87</td>\n",
       "      <td>20.0</td>\n",
       "      <td>New York</td>\n",
       "      <td>Finger Lakes</td>\n",
       "      <td>Anna Lee C. Iijima</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>Lamoreaux Landing</td>\n",
       "      <td>2014</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>Intensely smoky tones of struck flint and ash ...</td>\n",
       "      <td>Yellow Dog Vineyard</td>\n",
       "      <td>87</td>\n",
       "      <td>20.0</td>\n",
       "      <td>New York</td>\n",
       "      <td>Finger Lakes</td>\n",
       "      <td>Anna Lee C. Iijima</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>Lamoreaux Landing</td>\n",
       "      <td>2014</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>More complex than the winery's appellation ble...</td>\n",
       "      <td>Claiborne Vineyard</td>\n",
       "      <td>91</td>\n",
       "      <td>28.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Edna Valley</td>\n",
       "      <td>Matt Kettmann</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>Claiborne &amp; Churchill</td>\n",
       "      <td>2014</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45432</th>\n",
       "      <td>US</td>\n",
       "      <td>The aromas are brooding, with notes of red fru...</td>\n",
       "      <td>L'Idiot du Village</td>\n",
       "      <td>93</td>\n",
       "      <td>42.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Columbia Valley (WA)</td>\n",
       "      <td>Sean P. Sullivan</td>\n",
       "      <td>Mourvèdre</td>\n",
       "      <td>Gramercy</td>\n",
       "      <td>2014</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45433</th>\n",
       "      <td>Australia</td>\n",
       "      <td>The vineyard was planted in 1853, so these are...</td>\n",
       "      <td>Old Garden Vineyard</td>\n",
       "      <td>93</td>\n",
       "      <td>82.0</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>Barossa Valley</td>\n",
       "      <td>Joe Czerwinski</td>\n",
       "      <td>Mourvèdre</td>\n",
       "      <td>Hewitson</td>\n",
       "      <td>2013</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45434</th>\n",
       "      <td>US</td>\n",
       "      <td>A blend of Ciel du Cheval and Force Majeure vi...</td>\n",
       "      <td>Crazy Mary</td>\n",
       "      <td>93</td>\n",
       "      <td>48.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Red Mountain</td>\n",
       "      <td>Sean P. Sullivan</td>\n",
       "      <td>Mourvèdre</td>\n",
       "      <td>Mark Ryan</td>\n",
       "      <td>2012</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45435</th>\n",
       "      <td>Australia</td>\n",
       "      <td>For a wine that's 100% Mourvèdre, this has pre...</td>\n",
       "      <td>The Twentyeight Road</td>\n",
       "      <td>89</td>\n",
       "      <td>35.0</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>McLaren Vale</td>\n",
       "      <td>Joe Czerwinski</td>\n",
       "      <td>Mourvèdre</td>\n",
       "      <td>D'Arenberg</td>\n",
       "      <td>2006</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45436</th>\n",
       "      <td>US</td>\n",
       "      <td>The aromas pop, with notes of freshly ground w...</td>\n",
       "      <td>Boushey Vineyard</td>\n",
       "      <td>91</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Yakima Valley</td>\n",
       "      <td>Sean P. Sullivan</td>\n",
       "      <td>Mourvèdre</td>\n",
       "      <td>W.T. Vintners</td>\n",
       "      <td>2014</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42770 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         country                                        description  \\\n",
       "0             US  Pineapple rind, lemon pith and orange blossom ...   \n",
       "1             US  A wisp of bramble extends a savory tone from n...   \n",
       "2             US  Dusty mineral, smoke and struck flint lend a s...   \n",
       "3             US  Intensely smoky tones of struck flint and ash ...   \n",
       "4             US  More complex than the winery's appellation ble...   \n",
       "...          ...                                                ...   \n",
       "45432         US  The aromas are brooding, with notes of red fru...   \n",
       "45433  Australia  The vineyard was planted in 1853, so these are...   \n",
       "45434         US  A blend of Ciel du Cheval and Force Majeure vi...   \n",
       "45435  Australia  For a wine that's 100% Mourvèdre, this has pre...   \n",
       "45436         US  The aromas pop, with notes of freshly ground w...   \n",
       "\n",
       "                designation  points  price         province  \\\n",
       "0      Reserve Late Harvest      87   13.0         Michigan   \n",
       "1            Ingle Vineyard      88   20.0         New York   \n",
       "2          Red Oak Vineyard      87   20.0         New York   \n",
       "3       Yellow Dog Vineyard      87   20.0         New York   \n",
       "4        Claiborne Vineyard      91   28.0       California   \n",
       "...                     ...     ...    ...              ...   \n",
       "45432    L'Idiot du Village      93   42.0       Washington   \n",
       "45433   Old Garden Vineyard      93   82.0  South Australia   \n",
       "45434            Crazy Mary      93   48.0       Washington   \n",
       "45435  The Twentyeight Road      89   35.0  South Australia   \n",
       "45436      Boushey Vineyard      91   49.0       Washington   \n",
       "\n",
       "                   region_1         taster_name    variety  \\\n",
       "0       Lake Michigan Shore  Alexander Peartree   Riesling   \n",
       "1              Finger Lakes  Anna Lee C. Iijima   Riesling   \n",
       "2              Finger Lakes  Anna Lee C. Iijima   Riesling   \n",
       "3              Finger Lakes  Anna Lee C. Iijima   Riesling   \n",
       "4               Edna Valley       Matt Kettmann   Riesling   \n",
       "...                     ...                 ...        ...   \n",
       "45432  Columbia Valley (WA)    Sean P. Sullivan  Mourvèdre   \n",
       "45433        Barossa Valley      Joe Czerwinski  Mourvèdre   \n",
       "45434          Red Mountain    Sean P. Sullivan  Mourvèdre   \n",
       "45435          McLaren Vale      Joe Czerwinski  Mourvèdre   \n",
       "45436         Yakima Valley    Sean P. Sullivan  Mourvèdre   \n",
       "\n",
       "                      winery  year  variety_100  \n",
       "0                 St. Julian  2013          7.0  \n",
       "1                 Heron Hill  2015          7.0  \n",
       "2          Lamoreaux Landing  2014          7.0  \n",
       "3          Lamoreaux Landing  2014          7.0  \n",
       "4      Claiborne & Churchill  2014          7.0  \n",
       "...                      ...   ...          ...  \n",
       "45432               Gramercy  2014         44.0  \n",
       "45433               Hewitson  2013         44.0  \n",
       "45434              Mark Ryan  2012         44.0  \n",
       "45435             D'Arenberg  2006         44.0  \n",
       "45436          W.T. Vintners  2014         44.0  \n",
       "\n",
       "[42770 rows x 12 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Reserve</th>\n",
       "      <td>1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estate</th>\n",
       "      <td>797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reserva</th>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estate Grown</th>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Riserva</th>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              designation\n",
       "Reserve              1030\n",
       "Estate                797\n",
       "Reserva               435\n",
       "Estate Grown          365\n",
       "Riserva               348"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Partiendo del último dataframe con las variedades más comunes, voy a ver la frecuencia de los viñedos (= designation)\n",
    "\n",
    "frec_designation = pd.DataFrame(df['designation'].value_counts().sort_values(ascending=False))\n",
    "frec_designation.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total variedades de las que hay más de 20 viñedos: 99\n"
     ]
    }
   ],
   "source": [
    "### me quedaré con aquellas variedades de las que hay más de 20 viñedos\n",
    "mask = frec_designation[\"designation\"] >= 20  ## defino la máscara\n",
    "\n",
    "print(\"Total variedades de las que hay más de 20 viñedos:\",len(frec_designation[mask])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros 3 viñedos de la lista:\n",
      "          designation\n",
      "Reserve         1030\n",
      "Estate           797\n",
      "Reserva          435\n",
      "Últimos 3 viñedos de la lista:\n",
      "                        designation\n",
      "Coro Mendocino                  21\n",
      "La Encantada Vineyard           21\n",
      "Buissonnier                     20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>designation_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Reserve</th>\n",
       "      <td>1030</td>\n",
       "      <td>Reserve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estate</th>\n",
       "      <td>797</td>\n",
       "      <td>Estate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reserva</th>\n",
       "      <td>435</td>\n",
       "      <td>Reserva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estate Grown</th>\n",
       "      <td>365</td>\n",
       "      <td>Estate Grown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Riserva</th>\n",
       "      <td>348</td>\n",
       "      <td>Riserva</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              designation designation_20\n",
       "Reserve              1030        Reserve\n",
       "Estate                797         Estate\n",
       "Reserva               435        Reserva\n",
       "Estate Grown          365   Estate Grown\n",
       "Riserva               348        Riserva"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desig_20 = frec_designation[mask]\n",
    "print(\"Primeros 3 viñedos de la lista:\\n\", desig_20.head(3))\n",
    "print(\"Últimos 3 viñedos de la lista:\\n\", desig_20.tail(3))  ## me quedo hasta el viñedo \"Buissonnier\" incluido\n",
    "## quiero pasar el índice (los nombres de las \"designation\") a la columna normal, para luego unirlo al dataframe grande y quedarme solo con estos\n",
    "\n",
    "desig_20[\"designation_20\"] = desig_20.index\n",
    "desig_20.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>designation_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Reserve</th>\n",
       "      <td>Reserve</td>\n",
       "      <td>Reserve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estate</th>\n",
       "      <td>Estate</td>\n",
       "      <td>Estate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reserva</th>\n",
       "      <td>Reserva</td>\n",
       "      <td>Reserva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estate Grown</th>\n",
       "      <td>Estate Grown</td>\n",
       "      <td>Estate Grown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Riserva</th>\n",
       "      <td>Riserva</td>\n",
       "      <td>Riserva</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               designation designation_20\n",
       "Reserve            Reserve        Reserve\n",
       "Estate              Estate         Estate\n",
       "Reserva            Reserva        Reserva\n",
       "Estate Grown  Estate Grown   Estate Grown\n",
       "Riserva            Riserva        Riserva"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desig_20[\"designation\"] = desig_20.index  ## tengo que volver a hacer esto y pasar los nombres a la columna \"designation\" (ya no me interesa quedarme con las frecuencias\n",
    "                                        ## y esto es necesario porque luego lo quiero mergear con el dataframe principal y ahora no me dejaba)\n",
    "desig_20.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(desig_20, how=\"outer\")  ## ahora los tengo unidos, después haré un dropna de la columna designation_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 42770 entries, 0 to 42769\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   country         42770 non-null  object \n",
      " 1   description     42770 non-null  object \n",
      " 2   designation     42770 non-null  object \n",
      " 3   points          42770 non-null  int64  \n",
      " 4   price           42770 non-null  float64\n",
      " 5   province        42770 non-null  object \n",
      " 6   region_1        42770 non-null  object \n",
      " 7   taster_name     42770 non-null  object \n",
      " 8   variety         42770 non-null  object \n",
      " 9   winery          42770 non-null  object \n",
      " 10  year            42770 non-null  int64  \n",
      " 11  variety_100     42770 non-null  float64\n",
      " 12  designation_20  7465 non-null   object \n",
      "dtypes: float64(2), int64(2), object(9)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()  ## designation_20  7465 non-null    -->> mi dataframe tendrá que tener este largo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"designation_20\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"designation\"], axis = 1, inplace=True)  ## ahora la columna \"designation\" la tengo doble, la borro y me quedo con la que se llama \"designation_20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7465 entries, 55 to 41204\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   country         7465 non-null   object \n",
      " 1   description     7465 non-null   object \n",
      " 2   points          7465 non-null   int64  \n",
      " 3   price           7465 non-null   float64\n",
      " 4   province        7465 non-null   object \n",
      " 5   region_1        7465 non-null   object \n",
      " 6   taster_name     7465 non-null   object \n",
      " 7   variety         7465 non-null   object \n",
      " 8   winery          7465 non-null   object \n",
      " 9   year            7465 non-null   int64  \n",
      " 10  variety_100     7465 non-null   float64\n",
      " 11  designation_20  7465 non-null   object \n",
      "dtypes: float64(2), int64(2), object(8)\n",
      "memory usage: 758.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "      <th>year</th>\n",
       "      <th>variety_100</th>\n",
       "      <th>designation_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7465</td>\n",
       "      <td>7465</td>\n",
       "      <td>7465.000000</td>\n",
       "      <td>7465.000000</td>\n",
       "      <td>7465</td>\n",
       "      <td>7465</td>\n",
       "      <td>7465</td>\n",
       "      <td>7465</td>\n",
       "      <td>7465</td>\n",
       "      <td>7465.000000</td>\n",
       "      <td>7465.000000</td>\n",
       "      <td>7465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7</td>\n",
       "      <td>7464</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>409</td>\n",
       "      <td>16</td>\n",
       "      <td>47</td>\n",
       "      <td>2517</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>US</td>\n",
       "      <td>Cigar box, café au lait, and dried tobacco aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>California</td>\n",
       "      <td>Columbia Valley (WA)</td>\n",
       "      <td>Michael Schachner</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>Columbia Crest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reserve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4408</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2159</td>\n",
       "      <td>481</td>\n",
       "      <td>1411</td>\n",
       "      <td>984</td>\n",
       "      <td>76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.486135</td>\n",
       "      <td>33.613262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011.218218</td>\n",
       "      <td>9.944943</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.945531</td>\n",
       "      <td>29.591160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.437458</td>\n",
       "      <td>9.567975</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1929.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country                                        description  \\\n",
       "count     7465                                               7465   \n",
       "unique       7                                               7464   \n",
       "top         US  Cigar box, café au lait, and dried tobacco aro...   \n",
       "freq      4408                                                  2   \n",
       "mean       NaN                                                NaN   \n",
       "std        NaN                                                NaN   \n",
       "min        NaN                                                NaN   \n",
       "25%        NaN                                                NaN   \n",
       "50%        NaN                                                NaN   \n",
       "75%        NaN                                                NaN   \n",
       "max        NaN                                                NaN   \n",
       "\n",
       "             points        price    province              region_1  \\\n",
       "count   7465.000000  7465.000000        7465                  7465   \n",
       "unique          NaN          NaN          50                   409   \n",
       "top             NaN          NaN  California  Columbia Valley (WA)   \n",
       "freq            NaN          NaN        2159                   481   \n",
       "mean      88.486135    33.613262         NaN                   NaN   \n",
       "std        2.945531    29.591160         NaN                   NaN   \n",
       "min       80.000000     4.000000         NaN                   NaN   \n",
       "25%       86.000000    18.000000         NaN                   NaN   \n",
       "50%       88.000000    26.000000         NaN                   NaN   \n",
       "75%       91.000000    40.000000         NaN                   NaN   \n",
       "max      100.000000   800.000000         NaN                   NaN   \n",
       "\n",
       "              taster_name     variety          winery         year  \\\n",
       "count                7465        7465            7465  7465.000000   \n",
       "unique                 16          47            2517          NaN   \n",
       "top     Michael Schachner  Chardonnay  Columbia Crest          NaN   \n",
       "freq                 1411         984              76          NaN   \n",
       "mean                  NaN         NaN             NaN  2011.218218   \n",
       "std                   NaN         NaN             NaN     3.437458   \n",
       "min                   NaN         NaN             NaN  1929.000000   \n",
       "25%                   NaN         NaN             NaN  2010.000000   \n",
       "50%                   NaN         NaN             NaN  2012.000000   \n",
       "75%                   NaN         NaN             NaN  2014.000000   \n",
       "max                   NaN         NaN             NaN  2017.000000   \n",
       "\n",
       "        variety_100 designation_20  \n",
       "count   7465.000000           7465  \n",
       "unique          NaN             99  \n",
       "top             NaN        Reserve  \n",
       "freq            NaN           1030  \n",
       "mean       9.944943            NaN  \n",
       "std        9.567975            NaN  \n",
       "min        1.000000            NaN  \n",
       "25%        2.000000            NaN  \n",
       "50%        7.000000            NaN  \n",
       "75%       15.000000            NaN  \n",
       "max       47.000000            NaN  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include=\"all\")   ## comprobación de los valores únicos. Ya veo que de \"variety_100\" tengo 47, de \"designation_20\" tengo 99\n",
    "                             ## ahora toca ver si puedo acotar aún más con las bodegas (winery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIMPIEZA INICIAL \"WINERY\"  --> de las que producen más de 100 botellas y tienen más de 20 viñedos, me quedo con las que tienen por lo menos 3 bodegas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "      <th>year</th>\n",
       "      <th>variety_100</th>\n",
       "      <th>designation_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>France</td>\n",
       "      <td>Subtle notes of clean, fresh lemon zest promis...</td>\n",
       "      <td>92</td>\n",
       "      <td>39.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Anne Krebiehl MW</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "      <td>2014</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>France</td>\n",
       "      <td>The tart but tropical charm of passion fruit p...</td>\n",
       "      <td>94</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Anne Krebiehl MW</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>Domaine Zinck</td>\n",
       "      <td>2015</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>France</td>\n",
       "      <td>Incredibly pure notes of freshly cut Cox's Ora...</td>\n",
       "      <td>94</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Anne Krebiehl MW</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>Martin Schaetzel</td>\n",
       "      <td>2012</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>France</td>\n",
       "      <td>Floral aromas lead into a rich, not quite dry ...</td>\n",
       "      <td>92</td>\n",
       "      <td>48.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>Domaine Zinck</td>\n",
       "      <td>2011</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>France</td>\n",
       "      <td>Softly scented with tart apple and fresh citru...</td>\n",
       "      <td>94</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Anne Krebiehl MW</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>Martin Schaetzel</td>\n",
       "      <td>2013</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41200</th>\n",
       "      <td>France</td>\n",
       "      <td>This is just right, balanced with plenty of cr...</td>\n",
       "      <td>87</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Loire Valley</td>\n",
       "      <td>Muscadet Sèvre et Maine</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>Melon</td>\n",
       "      <td>Domaine du Fief aux Dames</td>\n",
       "      <td>2013</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Sur Lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41201</th>\n",
       "      <td>France</td>\n",
       "      <td>Crisp lemon aromas follow through to an equall...</td>\n",
       "      <td>86</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Loire Valley</td>\n",
       "      <td>Muscadet Sèvre et Maine</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>Melon</td>\n",
       "      <td>Domaine du Colombier</td>\n",
       "      <td>2016</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Sur Lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41202</th>\n",
       "      <td>France</td>\n",
       "      <td>This soft and fruity wine has open white fruit...</td>\n",
       "      <td>87</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Loire Valley</td>\n",
       "      <td>Muscadet Sèvre et Maine</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>Melon</td>\n",
       "      <td>Chateau de l'Oiselinière</td>\n",
       "      <td>2015</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Sur Lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41203</th>\n",
       "      <td>France</td>\n",
       "      <td>This crisp Muscadet offers an attractive bite ...</td>\n",
       "      <td>86</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Loire Valley</td>\n",
       "      <td>Muscadet Sèvre et Maine</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>Melon</td>\n",
       "      <td>Domaine Michel Brégeon</td>\n",
       "      <td>2012</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Sur Lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41204</th>\n",
       "      <td>France</td>\n",
       "      <td>From two parcels of old vines, this structured...</td>\n",
       "      <td>86</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Loire Valley</td>\n",
       "      <td>Muscadet Sèvre et Maine</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>Melon</td>\n",
       "      <td>Domaine du Colombier</td>\n",
       "      <td>2015</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Sur Lie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7465 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      country                                        description  points  \\\n",
       "55     France  Subtle notes of clean, fresh lemon zest promis...      92   \n",
       "56     France  The tart but tropical charm of passion fruit p...      94   \n",
       "57     France  Incredibly pure notes of freshly cut Cox's Ora...      94   \n",
       "58     France  Floral aromas lead into a rich, not quite dry ...      92   \n",
       "59     France  Softly scented with tart apple and fresh citru...      94   \n",
       "...       ...                                                ...     ...   \n",
       "41200  France  This is just right, balanced with plenty of cr...      87   \n",
       "41201  France  Crisp lemon aromas follow through to an equall...      86   \n",
       "41202  France  This soft and fruity wine has open white fruit...      87   \n",
       "41203  France  This crisp Muscadet offers an attractive bite ...      86   \n",
       "41204  France  From two parcels of old vines, this structured...      86   \n",
       "\n",
       "       price      province                 region_1       taster_name  \\\n",
       "55      39.0        Alsace                   Alsace  Anne Krebiehl MW   \n",
       "56      49.0        Alsace                   Alsace  Anne Krebiehl MW   \n",
       "57      40.0        Alsace                   Alsace  Anne Krebiehl MW   \n",
       "58      48.0        Alsace                   Alsace        Roger Voss   \n",
       "59      40.0        Alsace                   Alsace  Anne Krebiehl MW   \n",
       "...      ...           ...                      ...               ...   \n",
       "41200   13.0  Loire Valley  Muscadet Sèvre et Maine        Roger Voss   \n",
       "41201   14.0  Loire Valley  Muscadet Sèvre et Maine        Roger Voss   \n",
       "41202   12.0  Loire Valley  Muscadet Sèvre et Maine        Roger Voss   \n",
       "41203   18.0  Loire Valley  Muscadet Sèvre et Maine        Roger Voss   \n",
       "41204   13.0  Loire Valley  Muscadet Sèvre et Maine        Roger Voss   \n",
       "\n",
       "        variety                     winery  year  variety_100  \\\n",
       "55     Riesling                 Kuentz-Bas  2014          7.0   \n",
       "56     Riesling              Domaine Zinck  2015          7.0   \n",
       "57     Riesling           Martin Schaetzel  2012          7.0   \n",
       "58     Riesling              Domaine Zinck  2011          7.0   \n",
       "59     Riesling           Martin Schaetzel  2013          7.0   \n",
       "...         ...                        ...   ...          ...   \n",
       "41200     Melon  Domaine du Fief aux Dames  2013         33.0   \n",
       "41201     Melon       Domaine du Colombier  2016         33.0   \n",
       "41202     Melon   Chateau de l'Oiselinière  2015         33.0   \n",
       "41203     Melon     Domaine Michel Brégeon  2012         33.0   \n",
       "41204     Melon       Domaine du Colombier  2015         33.0   \n",
       "\n",
       "              designation_20  \n",
       "55     Pfersigberg Grand Cru  \n",
       "56     Pfersigberg Grand Cru  \n",
       "57     Pfersigberg Grand Cru  \n",
       "58     Pfersigberg Grand Cru  \n",
       "59     Pfersigberg Grand Cru  \n",
       "...                      ...  \n",
       "41200                Sur Lie  \n",
       "41201                Sur Lie  \n",
       "41202                Sur Lie  \n",
       "41203                Sur Lie  \n",
       "41204                Sur Lie  \n",
       "\n",
       "[7465 rows x 12 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Columbia Crest</th>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trapiche</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chateau Ste. Michelle</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Novelty Hill</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maryhill</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Peter Franus</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maison Hebrard</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Château Lapinesse</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Château Camarsac</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domaine Michel Brégeon</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2517 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        winery\n",
       "Columbia Crest              76\n",
       "Trapiche                    55\n",
       "Chateau Ste. Michelle       54\n",
       "Novelty Hill                49\n",
       "Maryhill                    34\n",
       "...                        ...\n",
       "Peter Franus                 1\n",
       "Maison Hebrard               1\n",
       "Château Lapinesse            1\n",
       "Château Camarsac             1\n",
       "Domaine Michel Brégeon       1\n",
       "\n",
       "[2517 rows x 1 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Partiendo del último dataframe con las variedades más comunes, voy a ver la frecuencia de las bodegas (= winery)\n",
    "\n",
    "frec_winery = pd.DataFrame(df['winery'].value_counts().sort_values(ascending=False))\n",
    "frec_winery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total variedades de las que hay más de 3 bodegas: 839\n"
     ]
    }
   ],
   "source": [
    "### me quedaré con aquellas variedades de las que hay más de 3 bodegas  (que por lo menos haya 3 bodagas que producen cada tipo de vino)\n",
    "mask = frec_winery[\"winery\"] >= 3  ## defino la máscara\n",
    "\n",
    "print(\"Total variedades de las que hay más de 3 bodegas:\",len(frec_winery[mask])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras 3 bodegas de la lista:\n",
      "                        winery\n",
      "Columbia Crest             76\n",
      "Trapiche                   55\n",
      "Chateau Ste. Michelle      54\n",
      "Primeras 3 bodegas de la lista:\n",
      "                     winery\n",
      "Hightower                3\n",
      "Elizabeth Chambers       3\n",
      "Les Frères Perroud       3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winery</th>\n",
       "      <th>winery_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Columbia Crest</th>\n",
       "      <td>76</td>\n",
       "      <td>Columbia Crest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trapiche</th>\n",
       "      <td>55</td>\n",
       "      <td>Trapiche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chateau Ste. Michelle</th>\n",
       "      <td>54</td>\n",
       "      <td>Chateau Ste. Michelle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Novelty Hill</th>\n",
       "      <td>49</td>\n",
       "      <td>Novelty Hill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maryhill</th>\n",
       "      <td>34</td>\n",
       "      <td>Maryhill</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       winery               winery_3\n",
       "Columbia Crest             76         Columbia Crest\n",
       "Trapiche                   55               Trapiche\n",
       "Chateau Ste. Michelle      54  Chateau Ste. Michelle\n",
       "Novelty Hill               49           Novelty Hill\n",
       "Maryhill                   34               Maryhill"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winery_3 = frec_winery[mask]\n",
    "print(\"Primeras 3 bodegas de la lista:\\n\", winery_3.head(3))\n",
    "print(\"Primeras 3 bodegas de la lista:\\n\", winery_3.tail(3))  ## me quedo hasta el viñedo \"Les Frères Perroud\" incluida\n",
    "## quiero pasar el índice (los nombres de las \"winery\") a la columna normal, para luego unirlo al dataframe grande y quedarme solo con estos\n",
    "\n",
    "winery_3[\"winery_3\"] = winery_3.index\n",
    "winery_3.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winery</th>\n",
       "      <th>winery_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Columbia Crest</th>\n",
       "      <td>Columbia Crest</td>\n",
       "      <td>Columbia Crest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trapiche</th>\n",
       "      <td>Trapiche</td>\n",
       "      <td>Trapiche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chateau Ste. Michelle</th>\n",
       "      <td>Chateau Ste. Michelle</td>\n",
       "      <td>Chateau Ste. Michelle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Novelty Hill</th>\n",
       "      <td>Novelty Hill</td>\n",
       "      <td>Novelty Hill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maryhill</th>\n",
       "      <td>Maryhill</td>\n",
       "      <td>Maryhill</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      winery               winery_3\n",
       "Columbia Crest                Columbia Crest         Columbia Crest\n",
       "Trapiche                            Trapiche               Trapiche\n",
       "Chateau Ste. Michelle  Chateau Ste. Michelle  Chateau Ste. Michelle\n",
       "Novelty Hill                    Novelty Hill           Novelty Hill\n",
       "Maryhill                            Maryhill               Maryhill"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winery_3[\"winery\"] = winery_3.index  ## tengo que volver a hacer esto y pasar los nombres a la columna \"winery\" (ya no me interesa quedarme con las frecuencias\n",
    "                                        ## y esto es necesario porque luego lo quiero mergear con el dataframe principal y ahora no me dejaba )\n",
    "winery_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(winery_3, how=\"outer\")  ## ahora los tengo unidos, después haré un dropna de la columna winery_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7465 entries, 0 to 7464\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   country         7465 non-null   object \n",
      " 1   description     7465 non-null   object \n",
      " 2   points          7465 non-null   int64  \n",
      " 3   price           7465 non-null   float64\n",
      " 4   province        7465 non-null   object \n",
      " 5   region_1        7465 non-null   object \n",
      " 6   taster_name     7465 non-null   object \n",
      " 7   variety         7465 non-null   object \n",
      " 8   winery          7465 non-null   object \n",
      " 9   year            7465 non-null   int64  \n",
      " 10  variety_100     7465 non-null   float64\n",
      " 11  designation_20  7465 non-null   object \n",
      " 12  winery_3        5297 non-null   object \n",
      "dtypes: float64(2), int64(2), object(9)\n",
      "memory usage: 816.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()  ## winery_3        5297 non-null   -->> mi dataframe tendrá que tener este largo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"winery_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"winery\"], axis = 1, inplace=True)  ## ahora la columna \"winery\" la tengo doble, la borro y me quedo con la que se llama \"winery_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>variety</th>\n",
       "      <th>year</th>\n",
       "      <th>variety_100</th>\n",
       "      <th>designation_20</th>\n",
       "      <th>winery_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>Subtle notes of clean, fresh lemon zest promis...</td>\n",
       "      <td>92</td>\n",
       "      <td>39.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Anne Krebiehl MW</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>2014</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France</td>\n",
       "      <td>This is a rich wine, labeled sweet but with a ...</td>\n",
       "      <td>93</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>Gewürztraminer</td>\n",
       "      <td>2012</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>France</td>\n",
       "      <td>Softly hinted aromatics of dried peach suggest...</td>\n",
       "      <td>90</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Anne Krebiehl MW</td>\n",
       "      <td>Gewürztraminer</td>\n",
       "      <td>2014</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>France</td>\n",
       "      <td>Still very closed up, this is a potentially ri...</td>\n",
       "      <td>91</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>Gewürztraminer</td>\n",
       "      <td>2013</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>France</td>\n",
       "      <td>A delicate, crisp, very pure wine. It has perf...</td>\n",
       "      <td>88</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>2011</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Tradition</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7439</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Truffle, underbrush, dark berry, mocha and gri...</td>\n",
       "      <td>91</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Piedmont</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>Nebbiolo</td>\n",
       "      <td>2011</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Bel Colle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7440</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Fragrant and loaded with finesse, this opens w...</td>\n",
       "      <td>94</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Piedmont</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>Nebbiolo</td>\n",
       "      <td>2012</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Bel Colle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7442</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Subdued aromas of menthol, dark spice and a wh...</td>\n",
       "      <td>92</td>\n",
       "      <td>76.0</td>\n",
       "      <td>Piedmont</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>Nebbiolo</td>\n",
       "      <td>2012</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Fratelli Alessandria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7443</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Scents of woodland berry, rose petal, grilled ...</td>\n",
       "      <td>91</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Piedmont</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>Nebbiolo</td>\n",
       "      <td>2011</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Fratelli Alessandria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7444</th>\n",
       "      <td>Italy</td>\n",
       "      <td>New leather, rose, violet, menthol and dark sp...</td>\n",
       "      <td>95</td>\n",
       "      <td>78.0</td>\n",
       "      <td>Piedmont</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>Nebbiolo</td>\n",
       "      <td>2013</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Fratelli Alessandria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5297 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     country                                        description  points  \\\n",
       "0     France  Subtle notes of clean, fresh lemon zest promis...      92   \n",
       "1     France  This is a rich wine, labeled sweet but with a ...      93   \n",
       "2     France  Softly hinted aromatics of dried peach suggest...      90   \n",
       "3     France  Still very closed up, this is a potentially ri...      91   \n",
       "4     France  A delicate, crisp, very pure wine. It has perf...      88   \n",
       "...      ...                                                ...     ...   \n",
       "7439   Italy  Truffle, underbrush, dark berry, mocha and gri...      91   \n",
       "7440   Italy  Fragrant and loaded with finesse, this opens w...      94   \n",
       "7442   Italy  Subdued aromas of menthol, dark spice and a wh...      92   \n",
       "7443   Italy  Scents of woodland berry, rose petal, grilled ...      91   \n",
       "7444   Italy  New leather, rose, violet, menthol and dark sp...      95   \n",
       "\n",
       "      price  province region_1       taster_name         variety  year  \\\n",
       "0      39.0    Alsace   Alsace  Anne Krebiehl MW        Riesling  2014   \n",
       "1      59.0    Alsace   Alsace        Roger Voss  Gewürztraminer  2012   \n",
       "2      45.0    Alsace   Alsace  Anne Krebiehl MW  Gewürztraminer  2014   \n",
       "3      59.0    Alsace   Alsace        Roger Voss  Gewürztraminer  2013   \n",
       "4      17.0    Alsace   Alsace        Roger Voss        Riesling  2011   \n",
       "...     ...       ...      ...               ...             ...   ...   \n",
       "7439   70.0  Piedmont   Barolo     Kerin O’Keefe        Nebbiolo  2011   \n",
       "7440   70.0  Piedmont   Barolo     Kerin O’Keefe        Nebbiolo  2012   \n",
       "7442   76.0  Piedmont   Barolo     Kerin O’Keefe        Nebbiolo  2012   \n",
       "7443   75.0  Piedmont   Barolo     Kerin O’Keefe        Nebbiolo  2011   \n",
       "7444   78.0  Piedmont   Barolo     Kerin O’Keefe        Nebbiolo  2013   \n",
       "\n",
       "      variety_100         designation_20              winery_3  \n",
       "0             7.0  Pfersigberg Grand Cru            Kuentz-Bas  \n",
       "1            23.0  Pfersigberg Grand Cru            Kuentz-Bas  \n",
       "2            23.0  Pfersigberg Grand Cru            Kuentz-Bas  \n",
       "3            23.0  Pfersigberg Grand Cru            Kuentz-Bas  \n",
       "4             7.0              Tradition            Kuentz-Bas  \n",
       "...           ...                    ...                   ...  \n",
       "7439         11.0            Monvigliero             Bel Colle  \n",
       "7440         11.0            Monvigliero             Bel Colle  \n",
       "7442         11.0            Monvigliero  Fratelli Alessandria  \n",
       "7443         11.0            Monvigliero  Fratelli Alessandria  \n",
       "7444         11.0            Monvigliero  Fratelli Alessandria  \n",
       "\n",
       "[5297 rows x 12 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5297 entries, 0 to 7444\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   country         5297 non-null   object \n",
      " 1   description     5297 non-null   object \n",
      " 2   points          5297 non-null   int64  \n",
      " 3   price           5297 non-null   float64\n",
      " 4   province        5297 non-null   object \n",
      " 5   region_1        5297 non-null   object \n",
      " 6   taster_name     5297 non-null   object \n",
      " 7   variety         5297 non-null   object \n",
      " 8   year            5297 non-null   int64  \n",
      " 9   variety_100     5297 non-null   float64\n",
      " 10  designation_20  5297 non-null   object \n",
      " 11  winery_3        5297 non-null   object \n",
      "dtypes: float64(2), int64(2), object(8)\n",
      "memory usage: 538.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>variety</th>\n",
       "      <th>year</th>\n",
       "      <th>variety_100</th>\n",
       "      <th>designation_20</th>\n",
       "      <th>winery_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5297</td>\n",
       "      <td>5297</td>\n",
       "      <td>5297.000000</td>\n",
       "      <td>5297.000000</td>\n",
       "      <td>5297</td>\n",
       "      <td>5297</td>\n",
       "      <td>5297</td>\n",
       "      <td>5297</td>\n",
       "      <td>5297.000000</td>\n",
       "      <td>5297.000000</td>\n",
       "      <td>5297</td>\n",
       "      <td>5297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7</td>\n",
       "      <td>5296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33</td>\n",
       "      <td>258</td>\n",
       "      <td>16</td>\n",
       "      <td>47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>US</td>\n",
       "      <td>Cigar box, café au lait, and dried tobacco aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>California</td>\n",
       "      <td>Columbia Valley (WA)</td>\n",
       "      <td>Michael Schachner</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reserve</td>\n",
       "      <td>Columbia Crest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3397</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1496</td>\n",
       "      <td>441</td>\n",
       "      <td>1020</td>\n",
       "      <td>762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>767</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.552011</td>\n",
       "      <td>32.156315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011.348310</td>\n",
       "      <td>9.805928</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.889712</td>\n",
       "      <td>26.211018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.202104</td>\n",
       "      <td>9.512737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country                                        description  \\\n",
       "count     5297                                               5297   \n",
       "unique       7                                               5296   \n",
       "top         US  Cigar box, café au lait, and dried tobacco aro...   \n",
       "freq      3397                                                  2   \n",
       "mean       NaN                                                NaN   \n",
       "std        NaN                                                NaN   \n",
       "min        NaN                                                NaN   \n",
       "25%        NaN                                                NaN   \n",
       "50%        NaN                                                NaN   \n",
       "75%        NaN                                                NaN   \n",
       "max        NaN                                                NaN   \n",
       "\n",
       "             points        price    province              region_1  \\\n",
       "count   5297.000000  5297.000000        5297                  5297   \n",
       "unique          NaN          NaN          33                   258   \n",
       "top             NaN          NaN  California  Columbia Valley (WA)   \n",
       "freq            NaN          NaN        1496                   441   \n",
       "mean      88.552011    32.156315         NaN                   NaN   \n",
       "std        2.889712    26.211018         NaN                   NaN   \n",
       "min       80.000000     4.000000         NaN                   NaN   \n",
       "25%       87.000000    17.000000         NaN                   NaN   \n",
       "50%       88.000000    26.000000         NaN                   NaN   \n",
       "75%       91.000000    40.000000         NaN                   NaN   \n",
       "max      100.000000   800.000000         NaN                   NaN   \n",
       "\n",
       "              taster_name     variety         year  variety_100  \\\n",
       "count                5297        5297  5297.000000  5297.000000   \n",
       "unique                 16          47          NaN          NaN   \n",
       "top     Michael Schachner  Chardonnay          NaN          NaN   \n",
       "freq                 1020         762          NaN          NaN   \n",
       "mean                  NaN         NaN  2011.348310     9.805928   \n",
       "std                   NaN         NaN     3.202104     9.512737   \n",
       "min                   NaN         NaN  1994.000000     1.000000   \n",
       "25%                   NaN         NaN  2010.000000     2.000000   \n",
       "50%                   NaN         NaN  2012.000000     7.000000   \n",
       "75%                   NaN         NaN  2014.000000    15.000000   \n",
       "max                   NaN         NaN  2017.000000    47.000000   \n",
       "\n",
       "       designation_20        winery_3  \n",
       "count            5297            5297  \n",
       "unique             99             839  \n",
       "top           Reserve  Columbia Crest  \n",
       "freq              767              76  \n",
       "mean              NaN             NaN  \n",
       "std               NaN             NaN  \n",
       "min               NaN             NaN  \n",
       "25%               NaN             NaN  \n",
       "50%               NaN             NaN  \n",
       "75%               NaN             NaN  \n",
       "max               NaN             NaN  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include=\"all\")   ## comprobación de los valores únicos. Ya veo que de \"variety_100\" tengo 47, de \"winery_10\" he bajado a  839\n",
    "                             ## y de designation me he quedado con 99 valos únicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "## guardo una version de csv CON LA COLUMNA \"variety\" original, me servirá luego si uso el NLP y tokenizer\n",
    "df.to_csv(\"..\\\\data\\\\processed\\\\vinos_para_NLP_model.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"variety\"], axis = 1, inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>year</th>\n",
       "      <th>variety_100</th>\n",
       "      <th>designation_20</th>\n",
       "      <th>winery_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>Subtle notes of clean, fresh lemon zest promis...</td>\n",
       "      <td>92</td>\n",
       "      <td>39.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Anne Krebiehl MW</td>\n",
       "      <td>2014</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France</td>\n",
       "      <td>This is a rich wine, labeled sweet but with a ...</td>\n",
       "      <td>93</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>2012</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>France</td>\n",
       "      <td>Softly hinted aromatics of dried peach suggest...</td>\n",
       "      <td>90</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Anne Krebiehl MW</td>\n",
       "      <td>2014</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>France</td>\n",
       "      <td>Still very closed up, this is a potentially ri...</td>\n",
       "      <td>91</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>2013</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>France</td>\n",
       "      <td>A delicate, crisp, very pure wine. It has perf...</td>\n",
       "      <td>88</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>2011</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Tradition</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7439</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Truffle, underbrush, dark berry, mocha and gri...</td>\n",
       "      <td>91</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Piedmont</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>2011</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Bel Colle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7440</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Fragrant and loaded with finesse, this opens w...</td>\n",
       "      <td>94</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Piedmont</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>2012</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Bel Colle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7442</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Subdued aromas of menthol, dark spice and a wh...</td>\n",
       "      <td>92</td>\n",
       "      <td>76.0</td>\n",
       "      <td>Piedmont</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>2012</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Fratelli Alessandria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7443</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Scents of woodland berry, rose petal, grilled ...</td>\n",
       "      <td>91</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Piedmont</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>2011</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Fratelli Alessandria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7444</th>\n",
       "      <td>Italy</td>\n",
       "      <td>New leather, rose, violet, menthol and dark sp...</td>\n",
       "      <td>95</td>\n",
       "      <td>78.0</td>\n",
       "      <td>Piedmont</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>2013</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Fratelli Alessandria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5297 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     country                                        description  points  \\\n",
       "0     France  Subtle notes of clean, fresh lemon zest promis...      92   \n",
       "1     France  This is a rich wine, labeled sweet but with a ...      93   \n",
       "2     France  Softly hinted aromatics of dried peach suggest...      90   \n",
       "3     France  Still very closed up, this is a potentially ri...      91   \n",
       "4     France  A delicate, crisp, very pure wine. It has perf...      88   \n",
       "...      ...                                                ...     ...   \n",
       "7439   Italy  Truffle, underbrush, dark berry, mocha and gri...      91   \n",
       "7440   Italy  Fragrant and loaded with finesse, this opens w...      94   \n",
       "7442   Italy  Subdued aromas of menthol, dark spice and a wh...      92   \n",
       "7443   Italy  Scents of woodland berry, rose petal, grilled ...      91   \n",
       "7444   Italy  New leather, rose, violet, menthol and dark sp...      95   \n",
       "\n",
       "      price  province region_1       taster_name  year  variety_100  \\\n",
       "0      39.0    Alsace   Alsace  Anne Krebiehl MW  2014          7.0   \n",
       "1      59.0    Alsace   Alsace        Roger Voss  2012         23.0   \n",
       "2      45.0    Alsace   Alsace  Anne Krebiehl MW  2014         23.0   \n",
       "3      59.0    Alsace   Alsace        Roger Voss  2013         23.0   \n",
       "4      17.0    Alsace   Alsace        Roger Voss  2011          7.0   \n",
       "...     ...       ...      ...               ...   ...          ...   \n",
       "7439   70.0  Piedmont   Barolo     Kerin O’Keefe  2011         11.0   \n",
       "7440   70.0  Piedmont   Barolo     Kerin O’Keefe  2012         11.0   \n",
       "7442   76.0  Piedmont   Barolo     Kerin O’Keefe  2012         11.0   \n",
       "7443   75.0  Piedmont   Barolo     Kerin O’Keefe  2011         11.0   \n",
       "7444   78.0  Piedmont   Barolo     Kerin O’Keefe  2013         11.0   \n",
       "\n",
       "             designation_20              winery_3  \n",
       "0     Pfersigberg Grand Cru            Kuentz-Bas  \n",
       "1     Pfersigberg Grand Cru            Kuentz-Bas  \n",
       "2     Pfersigberg Grand Cru            Kuentz-Bas  \n",
       "3     Pfersigberg Grand Cru            Kuentz-Bas  \n",
       "4                 Tradition            Kuentz-Bas  \n",
       "...                     ...                   ...  \n",
       "7439            Monvigliero             Bel Colle  \n",
       "7440            Monvigliero             Bel Colle  \n",
       "7442            Monvigliero  Fratelli Alessandria  \n",
       "7443            Monvigliero  Fratelli Alessandria  \n",
       "7444            Monvigliero  Fratelli Alessandria  \n",
       "\n",
       "[5297 rows x 11 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  ## ahora veo que ya no está la columna \"variety\", solo está su versión numérica\n",
    "### guardo este dataframe aquí abajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creo que ya he acotado suficientes datos como para poder empezar con los modelos de machine learning\n",
    "## antes guardaré este dataframe como csv para tenerlo más cómodo\n",
    "\n",
    "\n",
    "df.to_csv(\"..\\\\data\\\\processed\\\\vinos_var100_des20_win3.csv\", index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PASAR LAS COLUMNAS DE CATEGÓRICAS A NUMÉRICAS --> get dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Antes de transformar a numéricas las columnas, me quedo solo con aquellas que usaré para mi modelo, así qeu voy a cargarme el csv y prescindir de las columnas\n",
    "## description y taster_name\n",
    "## al principio pensaba quitar también province y region_1, pero de momento las dejo, las uso para empezar los modelos de clasificación y las iré quitando poco a poco según los resultados que me salgan\n",
    "\n",
    "df_3 = pd.read_csv(\"..\\\\data\\\\processed\\\\vinos_var100_des20_win3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>year</th>\n",
       "      <th>variety_100</th>\n",
       "      <th>designation_20</th>\n",
       "      <th>winery_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>Subtle notes of clean, fresh lemon zest promis...</td>\n",
       "      <td>92</td>\n",
       "      <td>39.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Anne Krebiehl MW</td>\n",
       "      <td>2014</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>This is a rich wine, labeled sweet but with a ...</td>\n",
       "      <td>93</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>2012</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>Softly hinted aromatics of dried peach suggest...</td>\n",
       "      <td>90</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Anne Krebiehl MW</td>\n",
       "      <td>2014</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>Still very closed up, this is a potentially ri...</td>\n",
       "      <td>91</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>2013</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>A delicate, crisp, very pure wine. It has perf...</td>\n",
       "      <td>88</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>2011</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Tradition</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               description  points  price  \\\n",
       "country                                                                     \n",
       "France   Subtle notes of clean, fresh lemon zest promis...      92   39.0   \n",
       "France   This is a rich wine, labeled sweet but with a ...      93   59.0   \n",
       "France   Softly hinted aromatics of dried peach suggest...      90   45.0   \n",
       "France   Still very closed up, this is a potentially ri...      91   59.0   \n",
       "France   A delicate, crisp, very pure wine. It has perf...      88   17.0   \n",
       "\n",
       "        province region_1       taster_name  year  variety_100  \\\n",
       "country                                                          \n",
       "France    Alsace   Alsace  Anne Krebiehl MW  2014          7.0   \n",
       "France    Alsace   Alsace        Roger Voss  2012         23.0   \n",
       "France    Alsace   Alsace  Anne Krebiehl MW  2014         23.0   \n",
       "France    Alsace   Alsace        Roger Voss  2013         23.0   \n",
       "France    Alsace   Alsace        Roger Voss  2011          7.0   \n",
       "\n",
       "                designation_20    winery_3  \n",
       "country                                     \n",
       "France   Pfersigberg Grand Cru  Kuentz-Bas  \n",
       "France   Pfersigberg Grand Cru  Kuentz-Bas  \n",
       "France   Pfersigberg Grand Cru  Kuentz-Bas  \n",
       "France   Pfersigberg Grand Cru  Kuentz-Bas  \n",
       "France               Tradition  Kuentz-Bas  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.drop([\"description\", \"taster_name\"], axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>year</th>\n",
       "      <th>variety_100</th>\n",
       "      <th>designation_20</th>\n",
       "      <th>winery_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>92</td>\n",
       "      <td>39.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>2014</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>93</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>2012</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>90</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>2014</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>91</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>2013</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>88</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>2011</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Tradition</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>91</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Piedmont</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>2011</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Bel Colle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>94</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Piedmont</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>2012</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Bel Colle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>92</td>\n",
       "      <td>76.0</td>\n",
       "      <td>Piedmont</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>2012</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Fratelli Alessandria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>91</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Piedmont</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>2011</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Fratelli Alessandria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>95</td>\n",
       "      <td>78.0</td>\n",
       "      <td>Piedmont</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>2013</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Fratelli Alessandria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5297 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         points  price  province region_1  year  variety_100  \\\n",
       "country                                                        \n",
       "France       92   39.0    Alsace   Alsace  2014          7.0   \n",
       "France       93   59.0    Alsace   Alsace  2012         23.0   \n",
       "France       90   45.0    Alsace   Alsace  2014         23.0   \n",
       "France       91   59.0    Alsace   Alsace  2013         23.0   \n",
       "France       88   17.0    Alsace   Alsace  2011          7.0   \n",
       "...         ...    ...       ...      ...   ...          ...   \n",
       "Italy        91   70.0  Piedmont   Barolo  2011         11.0   \n",
       "Italy        94   70.0  Piedmont   Barolo  2012         11.0   \n",
       "Italy        92   76.0  Piedmont   Barolo  2012         11.0   \n",
       "Italy        91   75.0  Piedmont   Barolo  2011         11.0   \n",
       "Italy        95   78.0  Piedmont   Barolo  2013         11.0   \n",
       "\n",
       "                designation_20              winery_3  \n",
       "country                                               \n",
       "France   Pfersigberg Grand Cru            Kuentz-Bas  \n",
       "France   Pfersigberg Grand Cru            Kuentz-Bas  \n",
       "France   Pfersigberg Grand Cru            Kuentz-Bas  \n",
       "France   Pfersigberg Grand Cru            Kuentz-Bas  \n",
       "France               Tradition            Kuentz-Bas  \n",
       "...                        ...                   ...  \n",
       "Italy              Monvigliero             Bel Colle  \n",
       "Italy              Monvigliero             Bel Colle  \n",
       "Italy              Monvigliero  Fratelli Alessandria  \n",
       "Italy              Monvigliero  Fratelli Alessandria  \n",
       "Italy              Monvigliero  Fratelli Alessandria  \n",
       "\n",
       "[5297 rows x 8 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ahora procedo a hacer un get_dummies\n",
    "df_3_dumm = pd.get_dummies(data = df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>variety_100</th>\n",
       "      <th>province_Alsace</th>\n",
       "      <th>province_Australia Other</th>\n",
       "      <th>province_Beaujolais</th>\n",
       "      <th>province_Bordeaux</th>\n",
       "      <th>province_British Columbia</th>\n",
       "      <th>province_Burgundy</th>\n",
       "      <th>...</th>\n",
       "      <th>winery_3_Yering Station</th>\n",
       "      <th>winery_3_Ysios</th>\n",
       "      <th>winery_3_ZD</th>\n",
       "      <th>winery_3_Zaca Mesa</th>\n",
       "      <th>winery_3_Zenith</th>\n",
       "      <th>winery_3_Zinfandelic</th>\n",
       "      <th>winery_3_Zolo</th>\n",
       "      <th>winery_3_Zotovich Cellars</th>\n",
       "      <th>winery_3_Zuani</th>\n",
       "      <th>winery_3_àMaurice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>92</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>93</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>90</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>91</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>88</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>91</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>94</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>92</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>91</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>95</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5297 rows × 1233 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         points  price  year  variety_100  province_Alsace  \\\n",
       "country                                                      \n",
       "France       92   39.0  2014          7.0                1   \n",
       "France       93   59.0  2012         23.0                1   \n",
       "France       90   45.0  2014         23.0                1   \n",
       "France       91   59.0  2013         23.0                1   \n",
       "France       88   17.0  2011          7.0                1   \n",
       "...         ...    ...   ...          ...              ...   \n",
       "Italy        91   70.0  2011         11.0                0   \n",
       "Italy        94   70.0  2012         11.0                0   \n",
       "Italy        92   76.0  2012         11.0                0   \n",
       "Italy        91   75.0  2011         11.0                0   \n",
       "Italy        95   78.0  2013         11.0                0   \n",
       "\n",
       "         province_Australia Other  province_Beaujolais  province_Bordeaux  \\\n",
       "country                                                                     \n",
       "France                          0                    0                  0   \n",
       "France                          0                    0                  0   \n",
       "France                          0                    0                  0   \n",
       "France                          0                    0                  0   \n",
       "France                          0                    0                  0   \n",
       "...                           ...                  ...                ...   \n",
       "Italy                           0                    0                  0   \n",
       "Italy                           0                    0                  0   \n",
       "Italy                           0                    0                  0   \n",
       "Italy                           0                    0                  0   \n",
       "Italy                           0                    0                  0   \n",
       "\n",
       "         province_British Columbia  province_Burgundy  ...  \\\n",
       "country                                                ...   \n",
       "France                           0                  0  ...   \n",
       "France                           0                  0  ...   \n",
       "France                           0                  0  ...   \n",
       "France                           0                  0  ...   \n",
       "France                           0                  0  ...   \n",
       "...                            ...                ...  ...   \n",
       "Italy                            0                  0  ...   \n",
       "Italy                            0                  0  ...   \n",
       "Italy                            0                  0  ...   \n",
       "Italy                            0                  0  ...   \n",
       "Italy                            0                  0  ...   \n",
       "\n",
       "         winery_3_Yering Station  winery_3_Ysios  winery_3_ZD  \\\n",
       "country                                                         \n",
       "France                         0               0            0   \n",
       "France                         0               0            0   \n",
       "France                         0               0            0   \n",
       "France                         0               0            0   \n",
       "France                         0               0            0   \n",
       "...                          ...             ...          ...   \n",
       "Italy                          0               0            0   \n",
       "Italy                          0               0            0   \n",
       "Italy                          0               0            0   \n",
       "Italy                          0               0            0   \n",
       "Italy                          0               0            0   \n",
       "\n",
       "         winery_3_Zaca Mesa  winery_3_Zenith  winery_3_Zinfandelic  \\\n",
       "country                                                              \n",
       "France                    0                0                     0   \n",
       "France                    0                0                     0   \n",
       "France                    0                0                     0   \n",
       "France                    0                0                     0   \n",
       "France                    0                0                     0   \n",
       "...                     ...              ...                   ...   \n",
       "Italy                     0                0                     0   \n",
       "Italy                     0                0                     0   \n",
       "Italy                     0                0                     0   \n",
       "Italy                     0                0                     0   \n",
       "Italy                     0                0                     0   \n",
       "\n",
       "         winery_3_Zolo  winery_3_Zotovich Cellars  winery_3_Zuani  \\\n",
       "country                                                             \n",
       "France               0                          0               0   \n",
       "France               0                          0               0   \n",
       "France               0                          0               0   \n",
       "France               0                          0               0   \n",
       "France               0                          0               0   \n",
       "...                ...                        ...             ...   \n",
       "Italy                0                          0               0   \n",
       "Italy                0                          0               0   \n",
       "Italy                0                          0               0   \n",
       "Italy                0                          0               0   \n",
       "Italy                0                          0               0   \n",
       "\n",
       "         winery_3_àMaurice  \n",
       "country                     \n",
       "France                   0  \n",
       "France                   0  \n",
       "France                   0  \n",
       "France                   0  \n",
       "France                   0  \n",
       "...                    ...  \n",
       "Italy                    0  \n",
       "Italy                    0  \n",
       "Italy                    0  \n",
       "Italy                    0  \n",
       "Italy                    0  \n",
       "\n",
       "[5297 rows x 1233 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3_dumm   ## ahora tengo 5297 rows × 1233 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUTLIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## los outliers de \"years\" no me preocupan, pero sí quiero quitar los de \"price\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\seaborn\\_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABG0AAAJNCAYAAACLEojLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAh20lEQVR4nO3de5CldZ3f8c+PGdQRjSBeiuC6AxlQrCW6BlNasaxBccMlilYRIzE1RjeguAGUshIvUwqpwarNBXUxUcHdhVkR3JBNQCVsMFBlJamogwvCMoi9MioUCtveVp2owC9/9Om2p6d7Znqmp8+3+3m9qqY45zm337efM+c07376TOu9BwAAAIBaDhn3AgAAAADYnWgDAAAAUJBoAwAAAFCQaAMAAABQkGgDAAAAUJBoAwAAAFDQ2sVc+RnPeEZfv379QVoKAAAAwPDcfvvtf917f+bc7YuKNuvXr8+2bduWblUAAAAAA9da+/Z82/16FAAAAEBBog0AAABAQaINAAAAQEGiDQAAAEBBog0AAABAQaINAAAAQEGiDQAAAEBBog0AAABAQaINAAAAQEGiDQAAAEBBog0AAABAQaINAAAAQEGiDQAAAEBBog0AAABAQaINAAAAQEGiDQAAAEBBog0AAABAQaINAAAAQEGiDQAAAEBBog0AAABAQaINAAAAQEGiDQAAAEBBog0AAABAQaINAAAAQEGiDQAAAEBBog0AAABAQaINAAAAQEGiDQAAAEBBog0AAABAQaINAAAAQEGiDQAAAEBBog0AAABAQaINAAAAQEGiDQAAAEBBog0AAABAQaINAAAAQEGiDQAAAEBBog0AAABAQaINAAAAQEGiDQAAAEBBog0AAABAQaINAAAAQEGiDQAAAEBBog0AAABAQaINAAAAQEGiDQAAAEBBog0AAABAQaINAAAAQEGiDQAAAEBBog0AAABAQaINAAAAQEGiDQAAAEBBa8e9gJXo8ssvz8TExD5d98EHH0ySHH300Qf8uBs2bMj5559/wPcDAAAA1Cfa7IeJiYnccff2PPbkp+/1umt+/uMkyfd+cWBf6jU//8EB3R4AAABYWUSb/fTYk5+enc8/fa/XW3fvTUmyT9fdl/sBAAAAhsFn2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQ0uGhz+eWX5/LLLx/3MlY9X2cAAAA4MGvHvYDlNjExMe4lDIKvMwAAAByYwR1pAwAAALASiDYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2HDR33nlnNm7cOPPnkksu2eX8G97whmzbti2vfOUr85a3vCWTk5M5++yzZy5/61vfmsnJyUxMTOxyu9tvvz3nnXde3vGOd+Td7353Nm7cmA984ANJkltvvTUbN27Mxz72sZnrv+51r8t5552XiYmJXHDBBbnhhhuycePGbNq0KRMTEzP3NTk5mSuvvDIbN27M61//+px22mk555xzMjk5OXO/n/vc53LBBRfM3NdnPvOZme1zTUxM5IwzzsjExMQ+fb2uueaabNy4Mdddd91ut51+/Ntuu223xzjttNNy7rnnZnJyMkkyOTmZCy64IDfeeOO8t5m+fPr6+2J/brPY+1rs9qVe61LOWNV8Mw5h7iFZ7OsOUNNyvfcBrGRDee0TbVg2c+PBww8/nIsvvjiPP/547r///mzdujUPPfTQzOXf+ta3snXr1mzZsmWX233wgx/M9u3bc88992Tbtm1Jki996UtJkg996ENJkuuvv37m+j/60Y+yffv2bNmyJXfddVc+8pGPJEm+853vZMuWLTP3tXXr1lxzzTVJkh/+8IfZuXNnvvnNb2br1q0z93vZZZflrrvumrmvK664Ymb7XFu2bMnPfvaz3da/kCuvvDJJ8olPfGK3204//qWXXrrbY+zcuTP33Xdftm7dmiS5+uqrc9ddd+XDH/7wvLeZvnz6+vtif26z2Pta7PalXutSzljVfDMOYe4hWezrDlDTcr33AaxkQ3ntE204KO688859ut5Pf/rTmdM33HDDbpd/4QtfyI4dOxa8zWznnntuHn300QUfa8eOHem9p/e+y7Zp8x0tkyQ33njjzP1O3376vqb13ne5/cTExMx979ixY68/9Z6ORXPXtWPHjlx77bUzj//oo4/OxK/Zj5EkN910UyYmJnLzzTfvMufs20xOTs5cfvPNN+9Tld6f2yz2vha7fanXupQzVjXfjEOYe0gW+7oD1LRc730AK9mQXvvWjnsBy+3BBx/Mzp07c+GFF+73fUxMTOSQX/a9X3EJHfL/fpKJib85oHWvRHuKMHPdd999B/RYjz/++LzbZ8eZPbnsssvymte8Jkl2+yn3li1bctVVVy142+mjbObzyU9+cpfzl156aU4++eTdHuNXv/pVtmzZMu8c07e5+uqrZy5/7LHHsnXr1rzrXe/a41z7c5vF3tdity/1Wpdyxqrmm7H3vurnHpLFvu4ANS3Xex/ASjak1769HmnTWju3tbattbbtkUceWY41wYq00BE8850/ENMha7773LFjx7yha3rbF7/4xV2O2rnlllv2+nj7c5vF3tdity/1Wpdyxqrmm3EIcw/JwXzdAZbPcr33AaxkQ3rt2+uRNr33K5JckSQnnXTS8h5echAcffTRSZKPfvSj+30fF154YW7/1veXakn75PEn/a1sOPbZB7Tu5bRx48ZxL2HZtdZmTq9fv36X/2Fav379kj3O2rVr532M6W0PPPDAbuFm+jannHJKbrrppjz66KNZu3ZtXv3qV+/18fbnNou9r8VuX+q1LuWMVc03Y+991c89JAfzdQdYPsv13gewkg3ptc9n2lDadGzYF8cff/wBPdYhh8z/12F2jNmTiy66aOb05s2bd7ls7vm5zjnnnAUve9vb3rbL+fe///3z3uehhx6azZs3zzvH9G3e/OY3z1y+Zs2abNq0aY/r2t/bLPa+Frt9qde6lDNWNd+MQ5h7SBb7ugPUtFzvfQAr2ZBe+0QbDooXvvCF+3S9pzzlKTOnzzzzzN0uP+OMM3b7afHs28x2xRVX7DHyrF+/Pq213Y6ImTb9eTRzvfa1r5253+nbT9/XtNbaLrffsGHDzH2vX78+GzZsWHBdSfKmN71pt7VO//fss8+eefy1a9fm5JNP3u0xkuT000/Phg0bcuqpp+4y5+zbHHnkkTOXn3rqqTnyyCP3uK79vc1i72ux25d6rUs5Y1XzzTiEuYdksa87QE3L9d4HsJIN6bVPtGHZTIeDac961rNy8cUX55BDDskxxxyTTZs25aijjpq5/Nhjj82mTZt2+2nxJZdckhNOOCEveMELctJJJyVJXvGKVyRJ3ve+9yVJzjrrrJnrH3744TnhhBOyefPmnHjiiXnnO9+ZJHnuc5+bzZs3z9zXpk2bZuLJEUcckXXr1uW4447Lpk2bZu73oosuyoknnjhzX+eee+7M9rk2b96cww47bJ9/2j19tM3b3/723W47/fjTR8zMfox169bl+OOP3+XolBNPPHHmg7jm3mb68sXU6P25zWLva7Hbl3qtSzljVfPNOIS5h2SxrztATcv13gewkg3lta/t67+Mk0x9ps22bdsO4nIOvul/fWkpPtNm5/NP3+t11917U5Ls03X3dj9/bwV9ps1SfJ0BAABgCFprt/feT5q73ZE2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAWtHfcCltuGDRvGvYRB8HUGAACAAzO4aHP++eePewmD4OsMAAAAB8avRwEAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABQk2gAAAAAUJNoAAAAAFCTaAAAAABS0dtwLWKnW/PwHWXfvTftwvckk2afr7u3xkmcf0H0AAAAAK4dosx82bNiwz9d98MFHkyRHH32gweXZi3pcAAAAYGUTbfbD+eefP+4lAAAAAKucz7QBAAAAKEi0AQAAAChItAEAAAAoSLQBAAAAKEi0AQAAAChItAEAAAAoSLQBAAAAKEi0AQAAAChItAEAAAAoSLQBAAAAKEi0AQAAAChItAEAAAAoSLQBAAAAKEi0AQAAAChItAEAAAAoSLQBAAAAKEi0AQAAAChItAEAAAAoSLQBAAAAKEi0AQAAAChItAEAAAAoSLQBAAAAKEi0AQAAAChItAEAAAAoSLQBAAAAKEi0AQAAAChItAEAAAAoSLQBAAAAKEi0AQAAAChItAEAAAAoSLQBAAAAKEi0AQAAAChItAEAAAAoSLQBAAAAKEi0AQAAAChItAEAAAAoSLQBAAAAKEi0AQAAAChItAEAAAAoSLQBAAAAKEi0AQAAAChItAEAAAAoSLQBAAAAKEi0AQAAAChItAEAAAAoSLQBAAAAKEi0AQAAAChItAEAAAAoSLQBAAAAKEi0AQAAAChItAEAAAAoSLQBAAAAKEi0AQAAACio9d73/cqtPZLk2wdvOQfFM5L89bgXMUbmN/9Q5x/y7In5hzz/kGdPzG/+4c4/5NkT85t/uPMPefZkdc3/m733Z87duKhosxK11rb13k8a9zrGxfzmH+r8Q549Mf+Q5x/y7In5zT/c+Yc8e2J+8w93/iHPngxjfr8eBQAAAFCQaAMAAABQ0BCizRXjXsCYmX/Yhjz/kGdPzD/k+Yc8e2J+8w/XkGdPzG/+4Rry7MkA5l/1n2kDAAAAsBIN4UgbAAAAgBVnVUeb1tqprbVvtNYmWmvvGfd6lkNrbUdr7a7W2h2ttW2jbU9vrd3SWvvm6L9HjHudS6G19kettYdba3fP2jbvrG3KH4yeC19vrb14fCtfGgvMf3Fr7cHR/r+jtXb6rMveO5r/G621fzieVS+d1tpvtNZua63d01r7y9bahaPtq/45sIfZB7H/W2tPaq19pbV252j+S0bbj2mtfXk052dba08YbX/i6PzE6PL1Yx3gAO1h/qtaa/fP2v8vGm1fNc/9aa21Na21v2itfX50fhD7fto88w9p3+/z9zkDmn8or/2Ht9aub63d21rb3lp72cD2/XzzD2XfP2/WjHe01n7SWnvnUPb/HuYfyv5/V5v6fufu1tq1ber7oGPagN7303tflX+SrEnyV0mOTfKEJHcmecG417UMc+9I8ow52/5tkveMTr8nye+Pe51LNOsrkrw4yd17mzXJ6Un+e5KW5KVJvjzu9R+k+S9O8u55rvuC0d+BJyY5ZvR3Y824ZzjA+Y9K8uLR6acmuW8056p/Duxh9kHs/9E+fMro9KFJvjzap3+a5I2j7Z9Ict7o9DuSfGJ0+o1JPjvuGQ7S/FclOWue66+a5/6smS5K8pkknx+dH8S+38P8Q9r3O7KP3+cMaP6hvPZfneRfjE4/IcnhA9v3880/iH0/Z7Y1Sb6X5DeHtP8XmH/V7/8kRye5P8m60fk/TfLPh/a+v5qPtPn7SSZ679/qvf8yyXVJzhzzmsblzEy90Gf039eNbylLp/f+pSQ/mLN5oVnPTLK1T/m/SQ5vrR21LAs9SBaYfyFnJrmu9/6L3vv9SSYy9Xdkxeq9P9R7/9ro9N8k2Z6pF/ZV/xzYw+wLWVX7f7QPfzo6e+joT0/yyiTXj7bP3ffTz4nrk7yqtdaWZ7VLbw/zL2TVPPeTpLX2nCRnJPnU6HzLQPZ9svv8e7Gq9v0erPrX/f20al77W2tPy9QPq/4wSXrvv+y9/ygD2fd7mH8hq2bfz+NVSf6q9/7tDGT/zzF7/oWstv2/Nsm61traJE9O8lAG9L6frO5fjzo6yXdnnX8ge/6fmtWiJ/kfrbXbW2vnjrY9u/f+0Oj095I8ezxLWxYLzTqk58O/HB0K+kft178Kt6rnHx36+NuZOuJgUM+BObMnA9n/berXQ+5I8nCSWzL1U6Qf9d4fHV1l9owz848u/3GSI5d1wUts7vy99+n9f+lo/3+4tfbE0bbVtv8/kuRfJXl8dP7IDGjfZ/f5pw1h3yeL+z5nKPMnq/+1/5gkjyT54zb1q4Gfaq0dluHs+4XmT1b/vp/rjUmuHZ0eyv6fbfb8ySrf/733B5P8+yTfyVSs+XGS2zOs9/1VHW2G6uW99xcnOS3J77XWXjH7wt57z55/IrtqDGnWWT6e5O8keVGmXtj+w1hXswxaa09J8l+SvLP3/pPZl63258A8sw9m//feH+u9vyjJczL106Pnj3dFy2vu/K2130ry3kx9HV6S5OlJ/vX4VnhwtNb+UZKHe++3j3st47CH+Vf9vp9l6N/nzDf/EF7712bqV8I/3nv/7SQ/y9Svw8xY5ft+ofmHsO9njD635LVJ/vPcy1b5/k8y7/yrfv+PQtSZmQqXfzvJYUlOHeuixmA1R5sHk/zGrPPPGW1b1UY1Mr33h5P810z9z8z3pw8JHP334fGt8KBbaNZBPB96798f/c/c40muzK8PhVyV87fWDs1UtLim9/5no82DeA7MN/vQ9n+SjA4Pvy3JyzJ1+PPa0UWzZ5yZf3T505JMLu9KD45Z8586+rW53nv/RZI/zurc//8gyWtbazsy9WvPr0zy0Qxn3+82f2vt0wPZ90kW/X3OIOYfyGv/A0kemHVU4fWZihhD2ffzzj+QfT/baUm+1nv//uj8UPb/tF3mH8j+PyXJ/b33R3rvv0ryZ5l6LxzK+36S1R1tvprkuNEnSz8hU4eS3TjmNR1UrbXDWmtPnT6d5HeS3J2pud88utqbk9wwnhUui4VmvTHJpjblpUl+POtwylVjzu/rvj5T+z+Zmv+No09UPybJcUm+stzrW0qj30/9wyTbe++Xzbpo1T8HFpp9KPu/tfbM1trho9Prkrw6U5/rc1uSs0ZXm7vvp58TZyW5dfQTuRVpgfnvnfWNa8vU73bP3v+r4rnfe39v7/05vff1mXpfv7X3/qYMZN8vMP8/G8K+T/br+5xBzD+E1/7e+/eSfLe19rzRplcluScD2fcLzT+EfT/H2dn1V4MGsf9n2WX+gez/7yR5aWvtyaP3uOm/+4N435/RC3wa8sH6k6lPDr8vU5918P5xr2cZ5j02U58UfmeSv5yeOVO/x/c/k3wzyReTPH3ca12iea/N1KGAv8rUTyB+d6FZM/Xp8f9x9Fy4K8lJ417/QZr/T0bzfT1TL1pHzbr++0fzfyPJaeNe/xLM//JMHQb79SR3jP6cPoTnwB5mH8T+T/J3k/zFaM67k3xgtP3YTH1TMpGpQ4efONr+pNH5idHlx457hoM0/62j/X93kk/n1//C1Kp57s/5OmzMr//1pEHs+z3MP4h9n0V+nzOg+Yfy2v+iJNtGc/63JEcMZd/vYf5B7PvRPIdl6oiJp83aNqT9P9/8g9j/SS5Jcu/oPe5PMvWvYg3qfb+NhgMAAACgkNX861EAAAAAK5ZoAwAAAFCQaAMAAABQkGgDAAAAUJBoAwAAAFCQaAMArCqttX/TWjtl3OsAADhQ/slvAGDVaK2t6b0/Nu51AAAsBUfaAAArQmttfWvt3tbaNa217a2161trT26t7Wit/X5r7WtJ/nFr7arW2lmj27yktfZ/Wmt3tta+0lp7amttTWvt37XWvtpa+3pr7W1jHg0AYF6iDQCwkjwvyX/qvZ+Q5CdJ3jHaPtl7f3Hv/brpK7bWnpDks0ku7L2/MMkpSXYm+d0kP+69vyTJS5Kc01o7ZjmHAADYF6INALCSfLf3/r9Hpz+d5OWj05+d57rPS/JQ7/2rSdJ7/0nv/dEkv5NkU2vtjiRfTnJkkuMO6qoBAPbD2nEvAABgEeZ+GN/0+Z8t4j5akvN773++NEsCADg4HGkDAKwkz22tvWx0+p8m+V97uO43khzVWntJkow+z2Ztkj9Pcl5r7dDR9uNba4cdzEUDAOwP0QYAWEm+keT3WmvbkxyR5OMLXbH3/ssk/yTJ5a21O5PckuRJST6V5J4kX2ut3Z3kk3H0MQBQkH/yGwBYEVpr65N8vvf+W+NeCwDAcnCkDQAAAEBBjrQBAAAAKMiRNgAAAAAFiTYAAAAABYk2AAAAAAWJNgAAAAAFiTYAAAAABYk2AAAAAAX9f4UMrfkhJ38UAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.boxplot(df_3_dumm[\"price\"]);\n",
    "plt.xticks(np.arange(0,850, 50));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len original: 5297\n",
      "Len sin outliers en price: 5076\n"
     ]
    }
   ],
   "source": [
    "## quito outliers de la columna price\n",
    "def outliers_quantie(df, feature, param=1.5):  \n",
    "        \n",
    "    iqr_ = iqr(df[feature], nan_policy='omit')\n",
    "    q1 = np.nanpercentile(df[feature], 25)\n",
    "    q3 = np.nanpercentile(df[feature], 75)\n",
    "    \n",
    "    th1 = q1 - iqr_*param\n",
    "    th2 = q3 + iqr_*param\n",
    "    \n",
    "    return df[(df[feature] >= th1) & (df[feature] <= th2)].reset_index(drop=True)\n",
    "\n",
    "df_3_dumm_no_out = outliers_quantie(df_3_dumm, 'price')  ## dejo el parámetro por defect de 1.5, así veo que me ha quitado muchos más outliers que en las pruebas anteriores\n",
    "print(\"Len original:\", len(df_3_dumm))\n",
    "print(\"Len sin outliers en price:\", len(df_3_dumm_no_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\seaborn\\_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.boxplot(df_3_dumm_no_out[\"price\"]);\n",
    "plt.xticks(np.arange(0,100, 50));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## voy a usar este dataframe (y también lo guardo)\n",
    "df_3_dumm_no_out.to_csv(\"..\\\\data\\\\processed\\\\vinos_var100_des20_win3_noOutliers.csv\", index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRUEBAS VARIAS PARA MODELOS DE MACHINE LEARNING\n",
    "Ahora mi problema es de CLASIFICACIÓN MULTICLASE, ya que quiero predecir la variedad del vino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ahora lo cargo con un nombre más corto y lo uso para los modelos de ML\n",
    "df_3 = pd.read_csv(\"..\\\\data\\\\processed\\\\vinos_var100_des20_win3_noOutliers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>variety_100</th>\n",
       "      <th>province_Alsace</th>\n",
       "      <th>province_Australia Other</th>\n",
       "      <th>province_Beaujolais</th>\n",
       "      <th>province_Bordeaux</th>\n",
       "      <th>province_British Columbia</th>\n",
       "      <th>province_Burgundy</th>\n",
       "      <th>...</th>\n",
       "      <th>winery_3_Yering Station</th>\n",
       "      <th>winery_3_Ysios</th>\n",
       "      <th>winery_3_ZD</th>\n",
       "      <th>winery_3_Zaca Mesa</th>\n",
       "      <th>winery_3_Zenith</th>\n",
       "      <th>winery_3_Zinfandelic</th>\n",
       "      <th>winery_3_Zolo</th>\n",
       "      <th>winery_3_Zotovich Cellars</th>\n",
       "      <th>winery_3_Zuani</th>\n",
       "      <th>winery_3_àMaurice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5071</th>\n",
       "      <td>90</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5072</th>\n",
       "      <td>91</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5073</th>\n",
       "      <td>95</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5074</th>\n",
       "      <td>91</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5075</th>\n",
       "      <td>94</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5076 rows × 1233 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      points  price  year  variety_100  province_Alsace  \\\n",
       "0         92   39.0  2014          7.0                1   \n",
       "1         93   59.0  2012         23.0                1   \n",
       "2         90   45.0  2014         23.0                1   \n",
       "3         91   59.0  2013         23.0                1   \n",
       "4         88   17.0  2011          7.0                1   \n",
       "...      ...    ...   ...          ...              ...   \n",
       "5071      90   49.0  2013         11.0                0   \n",
       "5072      91   70.0  2012         11.0                0   \n",
       "5073      95   65.0  2010         11.0                0   \n",
       "5074      91   70.0  2011         11.0                0   \n",
       "5075      94   70.0  2012         11.0                0   \n",
       "\n",
       "      province_Australia Other  province_Beaujolais  province_Bordeaux  \\\n",
       "0                            0                    0                  0   \n",
       "1                            0                    0                  0   \n",
       "2                            0                    0                  0   \n",
       "3                            0                    0                  0   \n",
       "4                            0                    0                  0   \n",
       "...                        ...                  ...                ...   \n",
       "5071                         0                    0                  0   \n",
       "5072                         0                    0                  0   \n",
       "5073                         0                    0                  0   \n",
       "5074                         0                    0                  0   \n",
       "5075                         0                    0                  0   \n",
       "\n",
       "      province_British Columbia  province_Burgundy  ...  \\\n",
       "0                             0                  0  ...   \n",
       "1                             0                  0  ...   \n",
       "2                             0                  0  ...   \n",
       "3                             0                  0  ...   \n",
       "4                             0                  0  ...   \n",
       "...                         ...                ...  ...   \n",
       "5071                          0                  0  ...   \n",
       "5072                          0                  0  ...   \n",
       "5073                          0                  0  ...   \n",
       "5074                          0                  0  ...   \n",
       "5075                          0                  0  ...   \n",
       "\n",
       "      winery_3_Yering Station  winery_3_Ysios  winery_3_ZD  \\\n",
       "0                           0               0            0   \n",
       "1                           0               0            0   \n",
       "2                           0               0            0   \n",
       "3                           0               0            0   \n",
       "4                           0               0            0   \n",
       "...                       ...             ...          ...   \n",
       "5071                        0               0            0   \n",
       "5072                        0               0            0   \n",
       "5073                        0               0            0   \n",
       "5074                        0               0            0   \n",
       "5075                        0               0            0   \n",
       "\n",
       "      winery_3_Zaca Mesa  winery_3_Zenith  winery_3_Zinfandelic  \\\n",
       "0                      0                0                     0   \n",
       "1                      0                0                     0   \n",
       "2                      0                0                     0   \n",
       "3                      0                0                     0   \n",
       "4                      0                0                     0   \n",
       "...                  ...              ...                   ...   \n",
       "5071                   0                0                     0   \n",
       "5072                   0                0                     0   \n",
       "5073                   0                0                     0   \n",
       "5074                   0                0                     0   \n",
       "5075                   0                0                     0   \n",
       "\n",
       "      winery_3_Zolo  winery_3_Zotovich Cellars  winery_3_Zuani  \\\n",
       "0                 0                          0               0   \n",
       "1                 0                          0               0   \n",
       "2                 0                          0               0   \n",
       "3                 0                          0               0   \n",
       "4                 0                          0               0   \n",
       "...             ...                        ...             ...   \n",
       "5071              0                          0               0   \n",
       "5072              0                          0               0   \n",
       "5073              0                          0               0   \n",
       "5074              0                          0               0   \n",
       "5075              0                          0               0   \n",
       "\n",
       "      winery_3_àMaurice  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "...                 ...  \n",
       "5071                  0  \n",
       "5072                  0  \n",
       "5073                  0  \n",
       "5074                  0  \n",
       "5075                  0  \n",
       "\n",
       "[5076 rows x 1233 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1º MODELO - REGRESIÓN LOGISTICA\n",
    "- Acierto: 56.69 %\n",
    "- Error: 43.31 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features shape: (5076, 1232)\n",
      "Train features shape: (4060, 1232)\n",
      "Train target shape: (1016, 1232)\n",
      "Test features shape: (4060,)\n",
      "Test target shape: (1016,)\n"
     ]
    }
   ],
   "source": [
    "## definición \"X\" e \"y\"\n",
    "X1 = df_3.drop([\"variety_100\"], axis = 1)\n",
    "y1 = np.array(df_3[\"variety_100\"]).reshape(-1,)\n",
    "\n",
    "## separo en train y test\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.2, random_state=12)\n",
    "\n",
    "## compruebo las divisiones:\n",
    "print(\"Total features shape:\", X1.shape)\n",
    "print(\"Train features shape:\", X1_train.shape)\n",
    "print(\"Train target shape:\", X1_test.shape)\n",
    "print(\"Test features shape:\", y1_train.shape)\n",
    "print(\"Test target shape:\", y1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  2., ...,  7., 29., 11.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## defino el modelo\n",
    "log_reg1 = LogisticRegression(max_iter = 1000)\n",
    "\n",
    "# Creo el scaler con los datos de train\n",
    "scal = StandardScaler() # Declaro el scaler\n",
    "scal.fit(X1_train) # Lo \"entreno\"\n",
    "X1_train = scal.transform(X1_train) # Aplico el scaler y sobreescribo los datos de train\n",
    "\n",
    "# Aplico el mismo scaler con los datos de test\n",
    "X1_test = scal.transform(X1_test)\n",
    "\n",
    "## entreno el modelo\n",
    "log_reg1.fit(X1_train, y1_train)\n",
    "\n",
    "## hago la predicción\n",
    "predictions1 = log_reg1.predict(X1_test)\n",
    "predictions1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acierto: 56.69 %\n",
      "Error: 43.31 %\n",
      "------\n",
      "Score del modelo (accuracy): 0.567\n",
      "Recall score:\n",
      " [0.7751938  0.58571429 0.70588235 0.36956522 0.85       0.39285714\n",
      " 0.65625    0.55       0.4        0.78787879 1.         0.45454545\n",
      " 0.6875     0.25       1.         0.53846154 0.33333333 0.26666667\n",
      " 0.2        0.9        0.42857143 0.64       0.42857143 1.\n",
      " 0.375      0.375      0.66666667 1.         0.75       0.33333333\n",
      " 0.25       0.         1.         1.         1.         0.\n",
      " 0.57142857 0.33333333 0.         1.         0.         0.\n",
      " 0.25       1.         0.        ]\n",
      "Precision score:\n",
      " [0.70422535 0.64566929 0.70588235 0.45945946 0.80952381 0.23913043\n",
      " 0.72413793 0.55932203 0.57142857 0.8        1.         0.6\n",
      " 0.42307692 0.33333333 0.6        0.5        0.39393939 0.28571429\n",
      " 0.18181818 1.         0.5        0.66666667 0.35294118 0.5\n",
      " 0.46153846 0.27272727 0.5        0.5        0.15       1.\n",
      " 0.33333333 0.         1.         0.5        0.5        0.\n",
      " 1.         0.21428571 0.         1.         0.         0.\n",
      " 0.33333333 1.         0.        ]\n",
      "F1 score:\n",
      " [0.73800738 0.61423221 0.70588235 0.40963855 0.82926829 0.2972973\n",
      " 0.68852459 0.55462185 0.47058824 0.79389313 1.         0.51724138\n",
      " 0.52380952 0.28571429 0.75       0.51851852 0.36111111 0.27586207\n",
      " 0.19047619 0.94736842 0.46153846 0.65306122 0.38709677 0.66666667\n",
      " 0.4137931  0.31578947 0.57142857 0.66666667 0.25       0.5\n",
      " 0.28571429 0.         1.         0.66666667 0.66666667 0.\n",
      " 0.72727273 0.26086957 0.         1.         0.         0.\n",
      " 0.28571429 1.         0.        ]\n",
      "Prediccion proba: \n",
      " [[0.96 0.04 0.   ... 0.   0.   0.  ]\n",
      " [0.79 0.   0.   ... 0.   0.   0.  ]\n",
      " [0.23 0.77 0.   ... 0.   0.   0.  ]\n",
      " ...\n",
      " [0.   0.   0.   ... 0.   0.   0.  ]\n",
      " [0.   0.23 0.   ... 0.   0.   0.  ]\n",
      " [0.   0.   0.   ... 0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "### metricas\n",
    "\n",
    "acierto = accuracy_score(y1_test, predictions1)\n",
    "error = 1 - acierto\n",
    "\n",
    "print(\"Acierto:\", round(acierto*100, 2), \"%\")\n",
    "print(\"Error:\", round(error*100, 2), \"%\")\n",
    "print(\"------\")\n",
    "print(\"Score del modelo (accuracy):\", round(accuracy_score(y1_test, predictions1),3))\n",
    "print(\"Recall score:\\n\", recall_score(y1_test, predictions1,average=None, zero_division= 0))\n",
    "print(\"Precision score:\\n\", precision_score(y1_test, predictions1, average=None, zero_division= 0))\n",
    "print(\"F1 score:\\n\", f1_score(y1_test, predictions1, average=None))\n",
    "\n",
    "predicions_proba = log_reg1.predict_proba(X1_test)\n",
    "print(\"Prediccion proba: \\n\",np.round(np.array(predicions_proba), 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2º MODELO - RANDOM FOREST CLASSIFIER\n",
    "- Acierto: 52.17 %\n",
    "- Error: 47.83 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features shape: (5076, 1232)\n",
      "Train features shape: (4060, 1232)\n",
      "Train target shape: (1016, 1232)\n",
      "Test features shape: (4060,)\n",
      "Test target shape: (1016,)\n"
     ]
    }
   ],
   "source": [
    "## definición \"X\" e \"y\"\n",
    "X2 = df_3.drop([\"variety_100\"], axis = 1)\n",
    "y2 = np.array(df_3[\"variety_100\"]).reshape(-1,)\n",
    "\n",
    "## separo en train y test\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size = 0.2, random_state=12)\n",
    "\n",
    "## compruebo las divisiones:\n",
    "print(\"Total features shape:\", X2.shape)\n",
    "print(\"Train features shape:\", X2_train.shape)\n",
    "print(\"Train target shape:\", X2_test.shape)\n",
    "print(\"Test features shape:\", y2_train.shape)\n",
    "print(\"Test target shape:\", y2_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  7.,  2., ...,  7.,  2., 11.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## con el random forest no hace falta que escale pero anteriormente hice una prueba sin escalar y salió muy mal, así que hago un StandardScaler\n",
    "\n",
    "scal = StandardScaler() # Declaro el scaler\n",
    "scal.fit(X2_train) # Lo \"entreno\". Calculo su media y std para cada feature\n",
    "X2_train = scal.transform(X2_train) # Aplico el scaler y sobreescribo los datos de train\n",
    "\n",
    "# Aplico el mismo scaler con los datos de test\n",
    "X2_test = scal.transform(X2_test)\n",
    "\n",
    "# Creación del modelo\n",
    "rnd_for2 = RandomForestClassifier(random_state= 42) \n",
    "\n",
    "rnd_for2.fit(X2_train,y2_train)\n",
    "\n",
    "## hacer la prediccion\n",
    "predictions2 = rnd_for2.predict(X2_test)\n",
    "predictions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acierto: 52.17 %\n",
      "Error: 47.83 %\n",
      "------\n",
      "Score del modelo (accuracy): 0.522\n",
      "Recall score:\n",
      " [0.72093023 0.5        0.67647059 0.34782609 0.7        0.46428571\n",
      " 0.70833333 0.51666667 0.26666667 0.72727273 1.         0.24242424\n",
      " 0.625      0.25       1.         0.38461538 0.17948718 0.4\n",
      " 0.2        0.9        0.28571429 0.6        0.46428571 1.\n",
      " 0.1875     0.375      1.         0.         0.5        0.33333333\n",
      " 0.25       0.         1.         1.         1.         0.\n",
      " 0.57142857 0.11111111 1.         1.         0.         0.11111111\n",
      " 0.         1.         0.        ]\n",
      "Precision score:\n",
      " [0.62       0.49295775 0.76666667 0.34408602 0.60869565 0.30952381\n",
      " 0.70103093 0.55357143 0.66666667 0.68571429 1.         0.53333333\n",
      " 0.47619048 0.5        0.5        0.38461538 0.23333333 0.4137931\n",
      " 0.16666667 1.         0.66666667 0.625      0.40625    0.33333333\n",
      " 0.33333333 0.3        1.         0.         0.22222222 1.\n",
      " 0.2        0.         1.         0.5        0.5        0.\n",
      " 1.         0.07692308 1.         1.         0.         0.14285714\n",
      " 0.         1.         0.        ]\n",
      "F1 score:\n",
      " [0.66666667 0.4964539  0.71875    0.34594595 0.65116279 0.37142857\n",
      " 0.70466321 0.53448276 0.38095238 0.70588235 1.         0.33333333\n",
      " 0.54054054 0.33333333 0.66666667 0.38461538 0.20289855 0.40677966\n",
      " 0.18181818 0.94736842 0.4        0.6122449  0.43333333 0.5\n",
      " 0.24       0.33333333 1.         0.         0.30769231 0.5\n",
      " 0.22222222 0.         1.         0.66666667 0.66666667 0.\n",
      " 0.72727273 0.09090909 1.         1.         0.         0.125\n",
      " 0.         1.         0.        ]\n",
      "Prediccion proba: \n",
      " [[0.39 0.43 0.   ... 0.   0.   0.  ]\n",
      " [0.2  0.   0.   ... 0.   0.   0.  ]\n",
      " [0.3  0.42 0.   ... 0.01 0.   0.  ]\n",
      " ...\n",
      " [0.   0.   0.   ... 0.   0.   0.  ]\n",
      " [0.01 0.35 0.   ... 0.01 0.   0.  ]\n",
      " [0.   0.01 0.   ... 0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "### metricas\n",
    "\n",
    "acierto2 = accuracy_score(y2_test, predictions2)\n",
    "error2 = 1 - acierto2\n",
    "\n",
    "print(\"Acierto:\", round(acierto2*100, 2), \"%\")\n",
    "print(\"Error:\", round(error2*100, 2), \"%\")\n",
    "print(\"------\")\n",
    "print(\"Score del modelo (accuracy):\", round(accuracy_score(y2_test, predictions2),3))\n",
    "print(\"Recall score:\\n\", recall_score(y2_test, predictions2,average=None, zero_division= 0))\n",
    "print(\"Precision score:\\n\", precision_score(y2_test, predictions2, average=None, zero_division= 0))\n",
    "print(\"F1 score:\\n\", f1_score(y2_test, predictions2, average=None))\n",
    "\n",
    "predicions_proba2 = rnd_for2.predict_proba(X2_test)\n",
    "print(\"Prediccion proba: \\n\",np.round(np.array(predicions_proba2), 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3º MODELO - SVC  --> el mejor resultado es con el kernel = sigmoid >>> 52.26%\n",
    "- Acierto_lin: 48.92 %\n",
    "- Error_lin: 51.08 %\n",
    "-\n",
    "- Acierto_pol: 48.62 %\n",
    "- Error_pol: 51.38 %\n",
    "-\n",
    "- Acierto_rbf: 51.48 %\n",
    "- Error_rbf: 48.52 %\n",
    "-\n",
    "- Acierto_sig: 52.26 %\n",
    "- Error_sig: 47.74 %\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features shape: (5076, 1232)\n",
      "Train features shape: (4060, 1232)\n",
      "Train target shape: (1016, 1232)\n",
      "Test features shape: (4060, 1)\n",
      "Test target shape: (1016, 1)\n"
     ]
    }
   ],
   "source": [
    "## definición \"X\" e \"y\"\n",
    "X3 = df_3.drop([\"variety_100\"], axis = 1)\n",
    "y3 = np.array(df_3[\"variety_100\"]).reshape(-1, 1)\n",
    "\n",
    "## separo en train y test\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size = 0.2, random_state=12)\n",
    "\n",
    "## compruebo las divisiones:\n",
    "print(\"Total features shape:\", X3.shape)\n",
    "print(\"Train features shape:\", X3_train.shape)\n",
    "print(\"Train target shape:\", X3_test.shape)\n",
    "print(\"Test features shape:\", y3_train.shape)\n",
    "print(\"Test target shape:\", y3_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  7.,  1., ...,  7., 29., 11.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## escalo los datos\n",
    "scal = StandardScaler() # Declaro el scaler\n",
    "scal.fit(X3_train) # Lo \"entreno\"\n",
    "X3_train = scal.transform(X3_train) # Aplico el scaler y sobreescribo los datos de train\n",
    "\n",
    "# Aplico el mismo scaler con los datos de test\n",
    "X3_test = scal.transform(X3_test)\n",
    "\n",
    "\n",
    "#Defino el algoritmo a utilizar\n",
    "svc3_lin = SVC(kernel='linear', C=100, probability= True)  \n",
    "svc3_pol = SVC(kernel='poly', C=100, gamma=\"scale\", probability= True)      \n",
    "svc3_rbf = SVC(kernel='rbf', C=100, probability= True)   \n",
    "svc3_sig = SVC(kernel='sigmoid', C=100, probability= True)  \n",
    "\n",
    "\n",
    "#Entreno el modelo\n",
    "svc3_lin.fit(X3_train, y3_train)\n",
    "svc3_pol.fit(X3_train, y3_train)\n",
    "svc3_rbf.fit(X3_train, y3_train)\n",
    "svc3_sig.fit(X3_train, y3_train)\n",
    "\n",
    "#Realizo una predicción\n",
    "predictions3_lin = svc3_lin.predict(X3_test)\n",
    "predictions3_lin\n",
    "\n",
    "predictions3_pol = svc3_pol.predict(X3_test)\n",
    "predictions3_pol\n",
    "\n",
    "predictions3_rbf = svc3_rbf.predict(X3_test)\n",
    "predictions3_rbf\n",
    "\n",
    "predictions3_sig = svc3_sig.predict(X3_test)\n",
    "predictions3_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acierto_lin: 48.92 %\n",
      "Error_lin: 51.08 %\n",
      "------\n",
      "Acierto_pol: 48.62 %\n",
      "Error_pol: 51.38 %\n",
      "------\n",
      "Acierto_rbf: 51.48 %\n",
      "Error_rbf: 48.52 %\n",
      "------\n",
      "Acierto_sig: 52.26 %\n",
      "Error_sig: 47.74 %\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "### metricas\n",
    "\n",
    "acierto3_lin = accuracy_score(y3_test, predictions3_lin)\n",
    "error3_lin = 1 - acierto3_lin\n",
    "\n",
    "print(\"Acierto_lin:\", round(acierto3_lin*100, 2), \"%\")\n",
    "print(\"Error_lin:\", round(error3_lin*100, 2), \"%\")\n",
    "print(\"------\")\n",
    "\n",
    "acierto3_pol = accuracy_score(y3_test, predictions3_pol)\n",
    "error3_pol = 1 - acierto3_pol\n",
    "\n",
    "print(\"Acierto_pol:\", round(acierto3_pol*100, 2), \"%\")\n",
    "print(\"Error_pol:\", round(error3_pol*100, 2), \"%\")\n",
    "print(\"------\")\n",
    "\n",
    "acierto3_rbf = accuracy_score(y3_test, predictions3_rbf)\n",
    "error3_rbf = 1 - acierto3_rbf\n",
    "\n",
    "print(\"Acierto_rbf:\", round(acierto3_rbf*100, 2), \"%\")\n",
    "print(\"Error_rbf:\", round(error3_rbf*100, 2), \"%\")\n",
    "print(\"------\")\n",
    "\n",
    "acierto3_sig = accuracy_score(y3_test, predictions3_sig)\n",
    "error3_sig = 1 - acierto3_sig\n",
    "\n",
    "print(\"Acierto_sig:\", round(acierto3_sig*100, 2), \"%\")\n",
    "print(\"Error_sig:\", round(error3_sig*100, 2), \"%\")\n",
    "print(\"------\")\n",
    "# print(\"Score del modelo (accuracy):\", round(accuracy_score(y3_test, predictions3),3))\n",
    "# print(\"Recall score:\\n\", recall_score(y3_test, predictions3,average=None, zero_division= 0))\n",
    "# print(\"Precision score:\\n\", precision_score(y3_test, predictions3, average=None, zero_division= 0))\n",
    "# print(\"F1 score:\\n\", f1_score(y3_test, predictions3, average=None))\n",
    "\n",
    "# predicions_proba3 = svc3_lin.predict_proba(X3_test)\n",
    "# print(\"Prediccion proba: \\n\",np.round(np.array(predicions_proba3), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4º MODELO - HIST GRADIENT BOOSTING CLASSIFIER  --> el PEOR hasta ahora\n",
    "\n",
    "- Acierto: 12.11 %\n",
    "- Error: 87.89 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features shape: (5076, 1232)\n",
      "Train features shape: (4060, 1232)\n",
      "Train target shape: (1016, 1232)\n",
      "Test features shape: (4060,)\n",
      "Test target shape: (1016,)\n"
     ]
    }
   ],
   "source": [
    "## definición \"X\" e \"y\"\n",
    "X4 = df_3.drop([\"variety_100\"], axis = 1)\n",
    "y4 = np.array(df_3[\"variety_100\"]).reshape(-1,)\n",
    "\n",
    "## separo en train y test\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size = 0.2, random_state=12)\n",
    "\n",
    "## compruebo las divisiones:\n",
    "print(\"Total features shape:\", X4.shape)\n",
    "print(\"Train features shape:\", X4_train.shape)\n",
    "print(\"Train target shape:\", X4_test.shape)\n",
    "print(\"Test features shape:\", y4_train.shape)\n",
    "print(\"Test target shape:\", y4_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## escalo los datos\n",
    "scal = StandardScaler() # Declaro el scaler\n",
    "scal.fit(X4_train) # Lo \"entreno\"\n",
    "X4_train = scal.transform(X4_train) # Aplico el scaler y sobreescribo los datos de train\n",
    "\n",
    "# Aplico el mismo scaler con los datos de test\n",
    "X4_test = scal.transform(X4_test)\n",
    "\n",
    "# entreno el modelo\n",
    "hgb4 = HistGradientBoostingClassifier()\n",
    "hgb4.fit(X4_train, y4_train)\n",
    "\n",
    "## hago las predicciones\n",
    "predictions4 = hgb4.predict(X = X4_test)\n",
    "predictions4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acierto: 12.11 %\n",
      "Error: 87.89 %\n",
      "------\n",
      "Score del modelo (accuracy): 0.121\n",
      "Recall score:\n",
      " [0.94573643 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.0625     0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Precision score:\n",
      " [0.12828601 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01754386 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 score:\n",
      " [0.22592593 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.02739726 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Prediccion proba: \n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "### metricas\n",
    "\n",
    "acierto4 = accuracy_score(y4_test, predictions4)\n",
    "error4 = 1 - acierto4\n",
    "\n",
    "print(\"Acierto:\", round(acierto4*100, 2), \"%\")\n",
    "print(\"Error:\", round(error4*100, 2), \"%\")\n",
    "print(\"------\")\n",
    "print(\"Score del modelo (accuracy):\", round(accuracy_score(y4_test, predictions4),3))\n",
    "print(\"Recall score:\\n\", recall_score(y4_test, predictions4,average=None, zero_division= 0))\n",
    "print(\"Precision score:\\n\", precision_score(y4_test, predictions4, average=None, zero_division= 0))\n",
    "print(\"F1 score:\\n\", f1_score(y4_test, predictions4, average=None))\n",
    "\n",
    "predicions_proba4 = hgb4.predict_proba(X4_test)\n",
    "print(\"Prediccion proba: \\n\",np.round(np.array(predicions_proba4), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5º MODELO - GRADIENT BOOSTING CLASSIFIER\n",
    "\n",
    "- Acierto: 54.23 %\n",
    "- Error: 45.77 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features shape: (5076, 1232)\n",
      "Train features shape: (4060, 1232)\n",
      "Train target shape: (1016, 1232)\n",
      "Test features shape: (4060,)\n",
      "Test target shape: (1016,)\n"
     ]
    }
   ],
   "source": [
    "## definición \"X\" e \"y\"\n",
    "X5 = df_3.drop([\"variety_100\"], axis = 1)\n",
    "y5 = np.array(df_3[\"variety_100\"]).reshape(-1,)\n",
    "\n",
    "## separo en train y test\n",
    "X5_train, X5_test, y5_train, y5_test = train_test_split(X5, y5, test_size = 0.2, random_state=12)\n",
    "\n",
    "## compruebo las divisiones:\n",
    "print(\"Total features shape:\", X5.shape)\n",
    "print(\"Train features shape:\", X5_train.shape)\n",
    "print(\"Train target shape:\", X5_test.shape)\n",
    "print(\"Test features shape:\", y5_train.shape)\n",
    "print(\"Test target shape:\", y5_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  7.,  1., ...,  7.,  2., 11.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## escalo los datos, \n",
    "scal = StandardScaler() # Declaro el scaler\n",
    "scal.fit(X5_train) # Lo \"entreno\"\n",
    "X5_train = scal.transform(X5_train) # Aplico el scaler y sobreescribo los datos de train\n",
    "\n",
    "# Aplico el mismo scaler con los datos de test\n",
    "X5_test = scal.transform(X5_test)\n",
    "\n",
    "#defino el modelo\n",
    "grboost5 = GradientBoostingClassifier(random_state=42)\n",
    "# lo entreno\n",
    "grboost5.fit(X5_train, y5_train)\n",
    "\n",
    "# hago las predicciones\n",
    "predictions5 = grboost5.predict(X5_test)\n",
    "predictions5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acierto: 54.23 %\n",
      "Error: 45.77 %\n",
      "------\n",
      "Score del modelo (accuracy): 0.542\n",
      "Recall score:\n",
      " [0.71317829 0.61428571 0.70588235 0.26086957 0.7        0.39285714\n",
      " 0.72916667 0.63333333 0.33333333 0.75757576 1.         0.24242424\n",
      " 0.75       0.5        1.         0.69230769 0.05128205 0.26666667\n",
      " 0.25       0.9        0.42857143 0.68       0.46428571 1.\n",
      " 0.125      0.375      0.66666667 1.         0.75       0.33333333\n",
      " 0.25       0.         1.         1.         1.         0.\n",
      " 0.42857143 0.33333333 1.         0.66666667 0.         0.11111111\n",
      " 0.         1.         0.        ]\n",
      "Precision score:\n",
      " [0.71875    0.48044693 0.70588235 0.35294118 0.7        0.36666667\n",
      " 0.72164948 0.58461538 0.52631579 0.83333333 1.         0.5\n",
      " 0.46153846 0.5        0.42857143 0.45       0.08       0.42105263\n",
      " 0.19230769 1.         0.42857143 0.65384615 0.34210526 1.\n",
      " 0.28571429 0.3        1.         0.33333333 0.3        1.\n",
      " 0.2        0.         1.         1.         0.5        0.\n",
      " 1.         0.25       1.         1.         0.         0.09090909\n",
      " 0.         1.         0.        ]\n",
      "F1 score:\n",
      " [0.71595331 0.53918495 0.70588235 0.3        0.7        0.37931034\n",
      " 0.7253886  0.608      0.40816327 0.79365079 1.         0.32653061\n",
      " 0.57142857 0.5        0.6        0.54545455 0.0625     0.32653061\n",
      " 0.2173913  0.94736842 0.42857143 0.66666667 0.39393939 1.\n",
      " 0.17391304 0.33333333 0.8        0.5        0.42857143 0.5\n",
      " 0.22222222 0.         1.         1.         0.66666667 0.\n",
      " 0.6        0.28571429 1.         0.8        0.         0.1\n",
      " 0.         1.         0.        ]\n",
      "Prediccion proba: \n",
      " [[0.32 0.49 0.01 ... 0.   0.   0.  ]\n",
      " [0.04 0.11 0.   ... 0.   0.   0.  ]\n",
      " [0.4  0.22 0.01 ... 0.   0.   0.  ]\n",
      " ...\n",
      " [0.01 0.01 0.   ... 0.   0.   0.  ]\n",
      " [0.23 0.31 0.01 ... 0.   0.   0.  ]\n",
      " [0.   0.   0.   ... 0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "### metricas\n",
    "\n",
    "acierto5 = accuracy_score(y5_test, predictions5)\n",
    "error5 = 1 - acierto5\n",
    "\n",
    "print(\"Acierto:\", round(acierto5*100, 2), \"%\")\n",
    "print(\"Error:\", round(error5*100, 2), \"%\")\n",
    "print(\"------\")\n",
    "print(\"Score del modelo (accuracy):\", round(accuracy_score(y5_test, predictions5),3))\n",
    "print(\"Recall score:\\n\", recall_score(y5_test, predictions5,average=None, zero_division= 0))\n",
    "print(\"Precision score:\\n\", precision_score(y5_test, predictions5, average=None, zero_division= 0))\n",
    "print(\"F1 score:\\n\", f1_score(y5_test, predictions5, average=None))\n",
    "\n",
    "predicions_proba5 = grboost5.predict_proba(X5_test)\n",
    "print(\"Prediccion proba: \\n\",np.round(np.array(predicions_proba5), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6º MODELO - XGBOOST CLASSIFIER\n",
    "\n",
    "- Acierto: 51.87 %\n",
    "- Error: 48.13 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features shape: (5076, 1232)\n",
      "Train features shape: (4060, 1232)\n",
      "Train target shape: (1016, 1232)\n",
      "Test features shape: (4060,)\n",
      "Test target shape: (1016,)\n"
     ]
    }
   ],
   "source": [
    "## definición \"X\" e \"y\"\n",
    "X6 = df_3.drop([\"variety_100\"], axis = 1)\n",
    "y6 = np.array(df_3[\"variety_100\"]).reshape(-1,)\n",
    "\n",
    "## separo en train y test\n",
    "X6_train, X6_test, y6_train, y6_test = train_test_split(X6, y6, test_size = 0.2, random_state=12)\n",
    "\n",
    "## compruebo las divisiones:\n",
    "print(\"Total features shape:\", X6.shape)\n",
    "print(\"Train features shape:\", X6_train.shape)\n",
    "print(\"Train target shape:\", X6_test.shape)\n",
    "print(\"Test features shape:\", y6_train.shape)\n",
    "print(\"Test target shape:\", y6_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:51:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1., 18.,  2., ...,  7., 31., 11.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## escalo los datos\n",
    "scal = StandardScaler() # Declaro el scaler\n",
    "scal.fit(X6_train) # Lo \"entreno\". Calculo su media y std para cada feature\n",
    "X6_train = scal.transform(X6_train) # Aplico el scaler y sobreescribo los datos de train\n",
    "\n",
    "# Aplico el mismo scaler con los datos de test\n",
    "X6_test = scal.transform(X6_test)\n",
    "xgb6 = xgboost.XGBClassifier(random_state=42)\n",
    "\n",
    "xgb6.fit(X6_train, y6_train)\n",
    "\n",
    "predictions6 = xgb6.predict(X6_test)\n",
    "predictions6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acierto: 51.87 %\n",
      "Error: 48.13 %\n",
      "------\n",
      "Score del modelo (accuracy): 0.519\n",
      "Recall score:\n",
      " [0.72093023 0.55714286 0.58823529 0.36956522 0.6        0.35714286\n",
      " 0.72916667 0.6        0.33333333 0.74242424 1.         0.27272727\n",
      " 0.5        0.25       0.66666667 0.46153846 0.28205128 0.2\n",
      " 0.15       0.9        0.28571429 0.48       0.42857143 0.\n",
      " 0.375      0.125      1.         0.         0.5        0.33333333\n",
      " 0.         0.         0.         1.         0.         0.\n",
      " 0.28571429 0.22222222 0.         1.         0.         0.\n",
      " 0.         1.         0.        ]\n",
      "Precision score:\n",
      " [0.65034965 0.52348993 0.8        0.34693878 0.57142857 0.3030303\n",
      " 0.7        0.57142857 0.76923077 0.65333333 1.         0.40909091\n",
      " 0.47058824 0.33333333 0.5        0.4        0.2972973  0.26086957\n",
      " 0.15789474 1.         0.33333333 0.57142857 0.4        0.\n",
      " 0.35294118 0.125      0.75       0.         0.15384615 1.\n",
      " 0.         0.         0.         0.5        0.         0.\n",
      " 0.66666667 0.16666667 0.         1.         0.         0.\n",
      " 0.         1.         0.        ]\n",
      "F1 score:\n",
      " [0.68382353 0.53979239 0.6779661  0.35789474 0.58536585 0.32786885\n",
      " 0.71428571 0.58536585 0.46511628 0.69503546 1.         0.32727273\n",
      " 0.48484848 0.28571429 0.57142857 0.42857143 0.28947368 0.22641509\n",
      " 0.15384615 0.94736842 0.30769231 0.52173913 0.4137931  0.\n",
      " 0.36363636 0.125      0.85714286 0.         0.23529412 0.5\n",
      " 0.         0.         0.         0.66666667 0.         0.\n",
      " 0.4        0.19047619 0.         1.         0.         0.\n",
      " 0.         1.         0.        ]\n",
      "Prediccion proba: \n",
      " [[0.53 0.36 0.   ... 0.   0.   0.  ]\n",
      " [0.05 0.01 0.   ... 0.   0.   0.  ]\n",
      " [0.26 0.32 0.   ... 0.   0.   0.  ]\n",
      " ...\n",
      " [0.01 0.   0.   ... 0.   0.   0.  ]\n",
      " [0.01 0.14 0.   ... 0.   0.   0.  ]\n",
      " [0.   0.   0.01 ... 0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "### metricas\n",
    "\n",
    "acierto6 = accuracy_score(y6_test, predictions6)\n",
    "error6 = 1 - acierto6\n",
    "\n",
    "print(\"Acierto:\", round(acierto6*100, 2), \"%\")\n",
    "print(\"Error:\", round(error6*100, 2), \"%\")\n",
    "print(\"------\")\n",
    "print(\"Score del modelo (accuracy):\", round(accuracy_score(y6_test, predictions6),3))\n",
    "print(\"Recall score:\\n\", recall_score(y6_test, predictions6,average=None, zero_division= 0))\n",
    "print(\"Precision score:\\n\", precision_score(y6_test, predictions6, average=None, zero_division= 0))\n",
    "print(\"F1 score:\\n\", f1_score(y6_test, predictions6, average=None))\n",
    "\n",
    "predicions_proba6 = xgb6.predict_proba(X6_test)\n",
    "print(\"Prediccion proba: \\n\",np.round(np.array(predicions_proba6), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7º MODELO - ADABOOST CLASSIFIER    --> el segundo PEOR, después del HistGradientBoost\n",
    "\n",
    "- Acierto: 19.59 %\n",
    "- Error: 80.41 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features shape: (5076, 1232)\n",
      "Train features shape: (4060, 1232)\n",
      "Train target shape: (1016, 1232)\n",
      "Test features shape: (4060,)\n",
      "Test target shape: (1016,)\n"
     ]
    }
   ],
   "source": [
    "## definición \"X\" e \"y\"\n",
    "X7 = df_3.drop([\"variety_100\"], axis = 1)\n",
    "y7 = np.array(df_3[\"variety_100\"]).reshape(-1,)\n",
    "\n",
    "## separo en train y test\n",
    "X7_train, X7_test, y7_train, y7_test = train_test_split(X7, y7, test_size = 0.2, random_state=12)\n",
    "\n",
    "## compruebo las divisiones:\n",
    "print(\"Total features shape:\", X7.shape)\n",
    "print(\"Train features shape:\", X7_train.shape)\n",
    "print(\"Train target shape:\", X7_test.shape)\n",
    "print(\"Test features shape:\", y7_train.shape)\n",
    "print(\"Test target shape:\", y7_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., ..., 2., 2., 2.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## escalo los datos, \n",
    "scal = StandardScaler() # Declaro el scaler\n",
    "scal.fit(X7_train) # Lo entreno\n",
    "X7_train = scal.transform(X7_train) # Aplico el scaler y sobreescribo los datos de train\n",
    "\n",
    "# Aplico el mismo scaler con los datos de test\n",
    "X7_test = scal.transform(X7_test)\n",
    "\n",
    "#defino el modelo\n",
    "adaboost7 = AdaBoostClassifier(random_state=42)\n",
    "# lo entreno\n",
    "adaboost7.fit(X7_train, y7_train)\n",
    "\n",
    "# hago las predicciones\n",
    "predictions7 = adaboost7.predict(X7_test)\n",
    "predictions7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acierto: 19.59 %\n",
      "Error: 80.41 %\n",
      "------\n",
      "Score del modelo (accuracy): 0.196\n",
      "Recall score:\n",
      " [0.         1.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.74242424 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.9        0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.5        0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Precision score:\n",
      " [0.         0.15283843 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.54444444 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 score:\n",
      " [0.         0.26515152 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.62820513 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.94736842 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.66666667 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Prediccion proba: \n",
      " [[0.03 0.03 0.03 ... 0.02 0.   0.02]\n",
      " [0.03 0.03 0.03 ... 0.02 0.   0.02]\n",
      " [0.03 0.03 0.03 ... 0.02 0.   0.02]\n",
      " ...\n",
      " [0.03 0.03 0.03 ... 0.02 0.   0.02]\n",
      " [0.03 0.03 0.03 ... 0.02 0.   0.02]\n",
      " [0.03 0.03 0.03 ... 0.02 0.   0.02]]\n"
     ]
    }
   ],
   "source": [
    "### metricas\n",
    "\n",
    "acierto7 = accuracy_score(y7_test, predictions7)\n",
    "error7 = 1 - acierto7\n",
    "\n",
    "print(\"Acierto:\", round(acierto7*100, 2), \"%\")\n",
    "print(\"Error:\", round(error7*100, 2), \"%\")\n",
    "print(\"------\")\n",
    "print(\"Score del modelo (accuracy):\", round(accuracy_score(y7_test, predictions7),3))\n",
    "print(\"Recall score:\\n\", recall_score(y7_test, predictions7,average=None, zero_division= 0))\n",
    "print(\"Precision score:\\n\", precision_score(y7_test, predictions7, average=None, zero_division= 0))\n",
    "print(\"F1 score:\\n\", f1_score(y7_test, predictions7, average=None))\n",
    "\n",
    "predicions_proba7 = adaboost7.predict_proba(X7_test)\n",
    "print(\"Prediccion proba: \\n\",np.round(np.array(predicions_proba7), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8º MODELO - KNN\n",
    "\n",
    "- Acierto: 47.15 %\n",
    "- Error: 52.85 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features shape: (5076, 1232)\n",
      "Train features shape: (4060, 1232)\n",
      "Train target shape: (1016, 1232)\n",
      "Test features shape: (4060,)\n",
      "Test target shape: (1016,)\n"
     ]
    }
   ],
   "source": [
    "## definición \"X\" e \"y\"\n",
    "X8 = df_3.drop([\"variety_100\"], axis = 1)\n",
    "y8 = np.array(df_3[\"variety_100\"]).reshape(-1,)\n",
    "\n",
    "## separo en train y test\n",
    "X8_train, X8_test, y8_train, y8_test = train_test_split(X8, y8, test_size = 0.2, random_state=12)\n",
    "\n",
    "## compruebo las divisiones:\n",
    "print(\"Total features shape:\", X8.shape)\n",
    "print(\"Train features shape:\", X8_train.shape)\n",
    "print(\"Train target shape:\", X8_test.shape)\n",
    "print(\"Test features shape:\", y8_train.shape)\n",
    "print(\"Test target shape:\", y8_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ...,  7.,  2., 11.])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## escalo los datos, \n",
    "scal = StandardScaler() # Declaro el scaler\n",
    "scal.fit(X8_train) # Lo entreno\n",
    "X8_train = scal.transform(X8_train) # Aplico el scaler y sobreescribo los datos de train\n",
    "\n",
    "# Aplico el mismo scaler con los datos de test\n",
    "X8_test = scal.transform(X8_test)\n",
    "\n",
    "#defino el modelo\n",
    "knn8 = KNeighborsClassifier(2) \n",
    "# lo entreno\n",
    "knn8.fit(X8_train, y8_train)\n",
    "\n",
    "# hago las predicciones\n",
    "predictions8 = knn8.predict(X8_test)\n",
    "predictions8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acierto: 47.15 %\n",
      "Error: 52.85 %\n",
      "------\n",
      "Score del modelo (accuracy): 0.471\n",
      "Recall score:\n",
      " [0.7751938  0.43571429 0.76470588 0.30434783 0.85       0.35714286\n",
      " 0.65625    0.38333333 0.43333333 0.71212121 0.90909091 0.21212121\n",
      " 0.4375     0.25       1.         0.38461538 0.1025641  0.26666667\n",
      " 0.1        0.8        0.14285714 0.44       0.25       0.\n",
      " 0.0625     0.125      0.33333333 0.         0.25       0.33333333\n",
      " 0.25       0.         1.         0.         0.         0.28571429\n",
      " 0.11111111 1.         1.         0.         0.         0.\n",
      " 1.         0.        ]\n",
      "Precision score:\n",
      " [0.54347826 0.37654321 0.61904762 0.23931624 0.68       0.25\n",
      " 0.66315789 0.43396226 0.68421053 0.734375   1.         0.29166667\n",
      " 0.31818182 0.25       0.75       0.38461538 0.19047619 0.32\n",
      " 0.14285714 1.         0.2        0.61111111 0.38888889 0.\n",
      " 0.16666667 0.2        1.         0.         0.5        1.\n",
      " 1.         0.         1.         0.         0.         1.\n",
      " 0.5        1.         1.         0.         0.         0.\n",
      " 1.         0.        ]\n",
      "F1 score:\n",
      " [0.63897764 0.40397351 0.68421053 0.26794258 0.75555556 0.29411765\n",
      " 0.65968586 0.40707965 0.53061224 0.72307692 0.95238095 0.24561404\n",
      " 0.36842105 0.25       0.85714286 0.38461538 0.13333333 0.29090909\n",
      " 0.11764706 0.88888889 0.16666667 0.51162791 0.30434783 0.\n",
      " 0.09090909 0.15384615 0.5        0.         0.33333333 0.5\n",
      " 0.4        0.         1.         0.         0.         0.44444444\n",
      " 0.18181818 1.         1.         0.         0.         0.\n",
      " 1.         0.        ]\n",
      "Prediccion proba: \n",
      " [[0.5 0.5 0.  ... 0.  0.  0. ]\n",
      " [0.5 0.  0.  ... 0.  0.  0. ]\n",
      " [0.5 0.5 0.  ... 0.  0.  0. ]\n",
      " ...\n",
      " [0.  0.  0.  ... 0.  0.  0. ]\n",
      " [0.  0.5 0.  ... 0.  0.  0. ]\n",
      " [0.  0.  0.  ... 0.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "### metricas\n",
    "\n",
    "acierto8 = accuracy_score(y8_test, predictions8)\n",
    "error8 = 1 - acierto8\n",
    "\n",
    "print(\"Acierto:\", round(acierto8*100, 2), \"%\")\n",
    "print(\"Error:\", round(error8*100, 2), \"%\")\n",
    "print(\"------\")\n",
    "print(\"Score del modelo (accuracy):\", round(accuracy_score(y8_test, predictions8),3))\n",
    "print(\"Recall score:\\n\", recall_score(y8_test, predictions8,average=None, zero_division= 0))\n",
    "print(\"Precision score:\\n\", precision_score(y8_test, predictions8, average=None, zero_division= 0))\n",
    "print(\"F1 score:\\n\", f1_score(y8_test, predictions8, average=None))\n",
    "\n",
    "predicions_proba8 = knn8.predict_proba(X8_test)\n",
    "print(\"Prediccion proba: \\n\",np.round(np.array(predicions_proba8), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9º MODELO - NAIVE BAYES\n",
    "\n",
    "- Acierto: 43.7 %\n",
    "- Error: 56.3 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features shape: (5076, 1232)\n",
      "Train features shape: (4060, 1232)\n",
      "Train target shape: (1016, 1232)\n",
      "Test features shape: (4060,)\n",
      "Test target shape: (1016,)\n"
     ]
    }
   ],
   "source": [
    "## definición \"X\" e \"y\"\n",
    "X9 = df_3.drop([\"variety_100\"], axis = 1)\n",
    "y9 = np.array(df_3[\"variety_100\"]).reshape(-1,)\n",
    "\n",
    "## separo en train y test\n",
    "X9_train, X9_test, y9_train, y9_test = train_test_split(X9, y9, test_size = 0.2, random_state=12)\n",
    "\n",
    "## compruebo las divisiones:\n",
    "print(\"Total features shape:\", X9.shape)\n",
    "print(\"Train features shape:\", X9_train.shape)\n",
    "print(\"Train target shape:\", X9_test.shape)\n",
    "print(\"Test features shape:\", y9_train.shape)\n",
    "print(\"Test target shape:\", y9_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  7.,  1., ...,  7., 43., 11.])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## escalo los datos, \n",
    "scal = StandardScaler() # Declaro el scaler\n",
    "scal.fit(X9_train) # Lo entreno\n",
    "X9_train = scal.transform(X9_train) # Aplico el scaler y sobreescribo los datos de train\n",
    "\n",
    "# Aplico el mismo scaler con los datos de test\n",
    "X9_test = scal.transform(X9_test)\n",
    "\n",
    "\n",
    "# creo el modelo\n",
    "nbc9 =GaussianNB()\n",
    "\n",
    "# lo entreno\n",
    "nbc9.fit(X9_train, y9_train)\n",
    "\n",
    "# hago las predicciones\n",
    "predictions9 = nbc9.predict(X9_test)\n",
    "predictions9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acierto: 43.7 %\n",
      "Error: 56.3 %\n",
      "------\n",
      "Score del modelo (accuracy): 0.437\n",
      "Recall score:\n",
      " [0.63565891 0.23571429 0.61764706 0.07608696 0.9        0.32142857\n",
      " 0.33333333 0.55       0.4        0.54545455 1.         0.36363636\n",
      " 0.6875     0.25       1.         0.61538462 0.15384615 0.26666667\n",
      " 0.3        0.9        0.71428571 0.84       0.53571429 1.\n",
      " 0.25       0.5        1.         1.         0.5        0.33333333\n",
      " 0.75       0.         1.         1.         1.         0.\n",
      " 0.71428571 0.44444444 1.         1.         0.         0.55555556\n",
      " 0.5        1.         0.        ]\n",
      "Precision score:\n",
      " [0.66129032 0.73333333 0.67741935 0.30434783 0.72       0.47368421\n",
      " 0.76190476 0.53225806 0.375      0.85714286 1.         0.4137931\n",
      " 0.31428571 0.5        0.5        0.28571429 0.20689655 0.24242424\n",
      " 0.14285714 1.         0.625      0.46666667 0.2        0.16666667\n",
      " 0.23529412 0.18181818 0.5        0.25       0.14285714 0.25\n",
      " 0.125      0.         1.         0.5        0.5        0.\n",
      " 0.20833333 0.10810811 1.         1.         0.         0.17857143\n",
      " 0.25       1.         0.        ]\n",
      "F1 score:\n",
      " [0.64822134 0.35675676 0.64615385 0.12173913 0.8        0.38297872\n",
      " 0.46376812 0.54098361 0.38709677 0.66666667 1.         0.38709677\n",
      " 0.43137255 0.33333333 0.66666667 0.3902439  0.17647059 0.25396825\n",
      " 0.19354839 0.94736842 0.66666667 0.6        0.29126214 0.28571429\n",
      " 0.24242424 0.26666667 0.66666667 0.4        0.22222222 0.28571429\n",
      " 0.21428571 0.         1.         0.66666667 0.66666667 0.\n",
      " 0.32258065 0.17391304 1.         1.         0.         0.27027027\n",
      " 0.33333333 1.         0.        ]\n",
      "Prediccion proba: \n",
      " [[0.5 0.5 0.  ... 0.  0.  0. ]\n",
      " [0.5 0.  0.  ... 0.  0.  0. ]\n",
      " [0.5 0.5 0.  ... 0.  0.  0. ]\n",
      " ...\n",
      " [0.  0.  0.  ... 0.  0.  0. ]\n",
      " [0.  0.5 0.  ... 0.  0.  0. ]\n",
      " [0.  0.  0.  ... 0.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "### metricas\n",
    "\n",
    "acierto9 = accuracy_score(y9_test, predictions9)\n",
    "error9 = 1 - acierto9\n",
    "\n",
    "print(\"Acierto:\", round(acierto9*100, 2), \"%\")\n",
    "print(\"Error:\", round(error9*100, 2), \"%\")\n",
    "print(\"------\")\n",
    "print(\"Score del modelo (accuracy):\", round(accuracy_score(y9_test, predictions9),3))\n",
    "print(\"Recall score:\\n\", recall_score(y9_test, predictions9,average=None, zero_division= 0))\n",
    "print(\"Precision score:\\n\", precision_score(y9_test, predictions9, average=None, zero_division= 0))\n",
    "print(\"F1 score:\\n\", f1_score(y9_test, predictions9, average=None))\n",
    "\n",
    "predicions_proba9 = knn8.predict_proba(X9_test)\n",
    "print(\"Prediccion proba: \\n\",np.round(np.array(predicions_proba9), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10º MODELO - DECISION TREE\n",
    "\n",
    "- Acierto: 47.05 %\n",
    "- Error: 52.95 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features shape: (5076, 1232)\n",
      "Train features shape: (4060, 1232)\n",
      "Train target shape: (1016, 1232)\n",
      "Test features shape: (4060,)\n",
      "Test target shape: (1016,)\n"
     ]
    }
   ],
   "source": [
    "## definición \"X\" e \"y\"\n",
    "X10 = df_3.drop([\"variety_100\"], axis = 1)\n",
    "y10 = np.array(df_3[\"variety_100\"]).reshape(-1,)\n",
    "\n",
    "## separo en train y test\n",
    "X10_train, X10_test, y10_train, y10_test = train_test_split(X10, y10, test_size = 0.2, random_state=12)\n",
    "\n",
    "## compruebo las divisiones:\n",
    "print(\"Total features shape:\", X10.shape)\n",
    "print(\"Train features shape:\", X10_train.shape)\n",
    "print(\"Train target shape:\", X10_test.shape)\n",
    "print(\"Test features shape:\", y10_train.shape)\n",
    "print(\"Test target shape:\", y10_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2., 18.,  1., ...,  7., 29., 11.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## escalo los datos, \n",
    "scal = StandardScaler() # Declaro el scaler\n",
    "scal.fit(X10_train) # Lo entreno\n",
    "X10_train = scal.transform(X10_train) # Aplico el scaler y sobreescribo los datos de train\n",
    "\n",
    "# Aplico el mismo scaler con los datos de test\n",
    "X10_test = scal.transform(X10_test)\n",
    "\n",
    "\n",
    "# contruimos el modelo\n",
    "decTr10 = DecisionTreeClassifier()\n",
    "\n",
    "# entrenamos el modelo\n",
    "decTr10.fit(X10_train, y10_train)\n",
    "\n",
    "# hago la prediccion\n",
    "predictions10 = decTr10.predict(X10_test)\n",
    "predictions10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acierto: 47.05 %\n",
      "Error: 52.95 %\n",
      "------\n",
      "Score del modelo (accuracy): 0.47\n",
      "Recall score:\n",
      " [0.72093023 0.45714286 0.70588235 0.26086957 0.65       0.32142857\n",
      " 0.59375    0.43333333 0.36666667 0.62121212 0.90909091 0.12121212\n",
      " 0.375      0.25       0.66666667 0.76923077 0.20512821 0.26666667\n",
      " 0.15       0.9        0.42857143 0.64       0.46428571 1.\n",
      " 0.125      0.125      1.         1.         0.75       0.33333333\n",
      " 0.         0.         0.         1.         0.         0.\n",
      " 0.28571429 0.11111111 1.         0.66666667 0.         0.22222222\n",
      " 0.         1.         0.        ]\n",
      "Precision score:\n",
      " [0.61589404 0.40764331 0.75       0.26373626 0.61904762 0.16363636\n",
      " 0.6627907  0.56521739 0.52380952 0.77358491 1.         0.28571429\n",
      " 0.375      0.33333333 0.66666667 0.71428571 0.25       0.30769231\n",
      " 0.125      1.         0.42857143 0.53333333 0.39393939 0.5\n",
      " 0.14285714 0.11111111 1.         0.5        0.375      1.\n",
      " 0.         0.         0.         0.5        0.         0.\n",
      " 0.66666667 0.07692308 1.         1.         0.         0.28571429\n",
      " 0.         1.         0.        ]\n",
      "F1 score:\n",
      " [0.66428571 0.43097643 0.72727273 0.26229508 0.63414634 0.21686747\n",
      " 0.62637363 0.49056604 0.43137255 0.68907563 0.95238095 0.17021277\n",
      " 0.375      0.28571429 0.66666667 0.74074074 0.22535211 0.28571429\n",
      " 0.13636364 0.94736842 0.42857143 0.58181818 0.42622951 0.66666667\n",
      " 0.13333333 0.11764706 1.         0.66666667 0.5        0.5\n",
      " 0.         0.         0.         0.66666667 0.         0.\n",
      " 0.4        0.09090909 1.         0.8        0.         0.25\n",
      " 0.         1.         0.        ]\n",
      "Prediccion proba: \n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "### metricas\n",
    "\n",
    "acierto10 = accuracy_score(y10_test, predictions10)\n",
    "error10 = 1 - acierto10\n",
    "\n",
    "print(\"Acierto:\", round(acierto10*100, 2), \"%\")\n",
    "print(\"Error:\", round(error10*100, 2), \"%\")\n",
    "print(\"------\")\n",
    "print(\"Score del modelo (accuracy):\", round(accuracy_score(y10_test, predictions10),3))\n",
    "print(\"Recall score:\\n\", recall_score(y10_test, predictions10,average=None, zero_division= 0))\n",
    "print(\"Precision score:\\n\", precision_score(y10_test, predictions10, average=None, zero_division= 0))\n",
    "print(\"F1 score:\\n\", f1_score(y10_test, predictions10, average=None))\n",
    "\n",
    "predicions_proba10 = decTr10.predict_proba(X10_test)\n",
    "print(\"Prediccion proba: \\n\",np.round(np.array(predicions_proba10), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1º selección de modelos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me quedo con los 4 mejores. Curiosamente son de los primeros que probé, pero ha sido pura casualidad.\n",
    "Iba a quedarme solo con los 3 mejores, pero el tercer y cuarto puesto están muy igualados así que mantengo ambos (de momento)\n",
    "\n",
    "Los demás modelos los ignoro directamentente.\n",
    "\n",
    "Me quedo con (en orden del mejor al peor):\n",
    "- LOGISTIC REGRESSION               (acierto 56.69 %)\n",
    "- GRADIENT BOOSTING CLASSIFIER      (acierto 54.23 %)\n",
    "- SVC, con kernel = sigmoid         (acierto 52,26 %)\n",
    "- RANDOM FOREST CLASSIFIER          (acierto 52.17 %)\n",
    "\n",
    "\n",
    "Ahora haré un Pipeline con Gridsearch para ver los mejores parámetros de cada uno y y adespués \"jugaré\" con las columna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defino 4 modelos, cada uno con su pipeline (menos él de randomForest , que no lo necesita, \n",
    "# simplemente no le aplico más parametros que el simple modelo)\n",
    "\n",
    "## modelo1\n",
    "reg_log = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer()),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"reglog\", LogisticRegression())\n",
    "])\n",
    "\n",
    "## modelo2\n",
    "grad_boost = Pipeline(steps = [\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"grad_boost\", GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "## modelo3\n",
    "svm = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"selectkbest\", SelectKBest()),\n",
    "    (\"svm\", SVC())\n",
    "])\n",
    "\n",
    "## modelo4\n",
    "rand_forest = RandomForestClassifier()\n",
    "\n",
    "\n",
    "##busco los mejores parametros(lo indico con doble barra baja y el nombre del parametro del que quiero encontrar el mejor valor\n",
    "reg_log_param = {\n",
    "    \"imputer__strategy\": ['mean', 'median'],\n",
    "    \"reglog__penalty\": ['l1', 'l2'],\n",
    "    \"reglog__C\": np.logspace(0, 4, 10)\n",
    "}\n",
    "\n",
    "grad_boost_param = {\n",
    "    \"grad_boost__learning_rate\": [0.01, 0.1, 0.2, 1],\n",
    "    \"grad_boost__max_features\": [1,2,3]\n",
    "}\n",
    "\n",
    "svm_param = {\n",
    "    'selectkbest__k': [2, 3, 4],\n",
    "    'svm__kernel': ['linear', 'rbf', 'sigmoid', 'poly'],\n",
    "    'svm__C': [0.001, 0.1, 0.5, 1, 5, 10, 100],\n",
    "    'svm__degree': [1,2,3,4],\n",
    "    'svm__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "rand_forest_param = {\n",
    "    \"n_estimators\": [10, 100, 1000],\n",
    "    \"max_features\": [1,2,3]\n",
    "}\n",
    "\n",
    "\n",
    "## ahora hago los gridseraches de cada modelo, cada uno con sus parametros\n",
    "gs_reg_log = GridSearchCV(reg_log,\n",
    "                         reg_log_param,\n",
    "                         cv = 10,\n",
    "                         scoring = 'accuracy',\n",
    "                         verbose = 1,\n",
    "                         n_jobs = -1)\n",
    "\n",
    "gs_grad_boost = GridSearchCV(grad_boost,\n",
    "                         grad_boost_param,\n",
    "                         cv = 10,\n",
    "                         scoring = 'accuracy',\n",
    "                         verbose = 1,\n",
    "                         n_jobs = -1)\n",
    "\n",
    "gs_svm = GridSearchCV(svm,\n",
    "                         svm_param,\n",
    "                         cv = 10,\n",
    "                         scoring = 'accuracy',\n",
    "                         verbose = 1,\n",
    "                         n_jobs = -1)\n",
    "\n",
    "gs_rand_forest = GridSearchCV(rand_forest,\n",
    "                         rand_forest_param,\n",
    "                         cv = 10,\n",
    "                         scoring = 'accuracy',\n",
    "                         verbose = 1,\n",
    "                         n_jobs = -1)\n",
    "\n",
    "##ahora ya nos guardamos cada uno de nuestros gridsearches en un diccionario\n",
    "grids = {\"gs_reg_log\": gs_reg_log,\n",
    "        \"gs_grad_boost\": gs_grad_boost,\n",
    "        \"gs_svm\": gs_svm,\n",
    "        \"gs_rand_forest\": gs_rand_forest}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'scaler', 'grad_boost', 'scaler__copy', 'scaler__with_mean', 'scaler__with_std', 'grad_boost__ccp_alpha', 'grad_boost__criterion', 'grad_boost__init', 'grad_boost__learning_rate', 'grad_boost__loss', 'grad_boost__max_depth', 'grad_boost__max_features', 'grad_boost__max_leaf_nodes', 'grad_boost__min_impurity_decrease', 'grad_boost__min_samples_leaf', 'grad_boost__min_samples_split', 'grad_boost__min_weight_fraction_leaf', 'grad_boost__n_estimators', 'grad_boost__n_iter_no_change', 'grad_boost__random_state', 'grad_boost__subsample', 'grad_boost__tol', 'grad_boost__validation_fraction', 'grad_boost__verbose', 'grad_boost__warm_start'])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grad_boost.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Así con todas las columnas me está dando un problema de memoria:   MemoryError: Unable to allocate 3.81 MiB for an array with shape (1231, 406) and data type int64\n",
    "\n",
    "# ## \n",
    "# X = df_3.drop([\"variety_100\"], axis = 1)\n",
    "# y = np.array(df_3[\"variety_100\"]).reshape(-1,)\n",
    "\n",
    "# ## separo en train y test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. voy a ver si teniendo las columnas mínimas para X me da el mismo fallo de memoria\n",
    "####### luego, dependiendo del resultado, iré viendo si añadir/cambiar columnas  >>> resulta que dió un score bajísimo\n",
    "\n",
    "## Defino \"X\" e \"y\"\n",
    "X = df_3[[\"points\", \"price\", \"year\"]]\n",
    "y = np.array(df_3[\"variety_100\"]).reshape(-1,)\n",
    "\n",
    "## separo en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  UserWarning,\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "200 fits failed out of a total of 400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan 0.21009852        nan 0.21034483        nan 0.21009852\n",
      "        nan 0.21009852        nan 0.21059113        nan 0.20985222\n",
      "        nan 0.21034483        nan 0.21034483        nan 0.21059113\n",
      "        nan 0.20985222        nan 0.21009852        nan 0.21034483\n",
      "        nan 0.21009852        nan 0.21009852        nan 0.21059113\n",
      "        nan 0.20985222        nan 0.21034483        nan 0.21034483\n",
      "        nan 0.21059113        nan 0.20985222]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "28 fits failed out of a total of 90.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "24 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 458, in fit\n",
      "    for i, t in enumerate(trees)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 1044, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 209, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 184, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 420, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 133, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 229, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 729, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 701, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 41, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 1507328 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 458, in fit\n",
      "    for i, t in enumerate(trees)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 1044, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 209, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 184, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 420, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 133, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 229, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 729, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 701, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 41, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 1474560 bytes\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.13152709 0.13965517        nan 0.14655172        nan        nan\n",
      " 0.14261084 0.141133          nan]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 672 candidates, totalling 6720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "2240 fits failed out of a total of 6720.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 355, in _fit\n",
      "    **fit_params_steps[name],\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 891, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\base.py\", line 847, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\", line 407, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\", line 606, in _check_params\n",
      "    \"Use k='all' to return all features.\" % (X.shape[1], self.k)\n",
      "ValueError: k should be >=0, <= n_features = 3; got 4. Use k='all' to return all features.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.17807882 0.1453202  0.1453202  0.1453202  0.17807882 0.1453202\n",
      " 0.1453202  0.1453202  0.17807882 0.1453202  0.1453202  0.1453202\n",
      " 0.17807882 0.1453202  0.1453202  0.1453202  0.17807882 0.1453202\n",
      " 0.1453202  0.16995074 0.17807882 0.1453202  0.1453202  0.16995074\n",
      " 0.17807882 0.1453202  0.1453202  0.16625616 0.17807882 0.1453202\n",
      " 0.1453202  0.16625616 0.20788177 0.20812808 0.18423645 0.19433498\n",
      " 0.20788177 0.20812808 0.18423645 0.19458128 0.20788177 0.20812808\n",
      " 0.18423645 0.17783251 0.20788177 0.20812808 0.18423645 0.17783251\n",
      " 0.20788177 0.20812808 0.18423645 0.17857143 0.20788177 0.20812808\n",
      " 0.18423645 0.17857143 0.20788177 0.20812808 0.18423645 0.17339901\n",
      " 0.20788177 0.20812808 0.18423645 0.17339901 0.208867   0.20738916\n",
      " 0.18522167 0.2091133  0.208867   0.20738916 0.18522167 0.2091133\n",
      " 0.208867   0.20738916 0.18522167 0.17684729 0.208867   0.20738916\n",
      " 0.18522167 0.17684729 0.208867   0.20738916 0.18522167 0.18275862\n",
      " 0.208867   0.20738916 0.18522167 0.18275862 0.208867   0.20738916\n",
      " 0.18522167 0.17339901 0.208867   0.20738916 0.18522167 0.17339901\n",
      " 0.20935961 0.20788177 0.18128079 0.208867   0.20935961 0.20788177\n",
      " 0.18128079 0.208867   0.20935961 0.20788177 0.18128079 0.17684729\n",
      " 0.20935961 0.20788177 0.18128079 0.17684729 0.20935961 0.20788177\n",
      " 0.18128079 0.18546798 0.20935961 0.20788177 0.18128079 0.18546798\n",
      " 0.20935961 0.20788177 0.18128079 0.17364532 0.20935961 0.20788177\n",
      " 0.18128079 0.17364532 0.20862069 0.20960591 0.17906404 0.2091133\n",
      " 0.20862069 0.20960591 0.17906404 0.2091133  0.20862069 0.20960591\n",
      " 0.17906404 0.17684729 0.20862069 0.20960591 0.17906404 0.17684729\n",
      " 0.20862069 0.20960591 0.17906404 0.18990148 0.20862069 0.20960591\n",
      " 0.17906404 0.18990148 0.20862069 0.20960591 0.17906404 0.1729064\n",
      " 0.20862069 0.20960591 0.17906404 0.17339901 0.208867   0.20862069\n",
      " 0.17684729 0.20862069 0.208867   0.20862069 0.17684729 0.20862069\n",
      " 0.208867   0.20862069 0.17684729 0.17684729 0.208867   0.20862069\n",
      " 0.17684729 0.17684729 0.208867   0.20862069 0.17684729 0.19014778\n",
      " 0.208867   0.20862069 0.17684729 0.19014778 0.208867   0.20862069\n",
      " 0.17684729 0.1729064  0.208867   0.20862069 0.17684729 0.17339901\n",
      " 0.20862069 0.20738916 0.18054187 0.20862069 0.20862069 0.20738916\n",
      " 0.18054187 0.20862069 0.20862069 0.20738916 0.18054187 0.17660099\n",
      " 0.20862069 0.20738916 0.18054187 0.17660099 0.20862069 0.20738916\n",
      " 0.18054187 0.19014778 0.20862069 0.20738916 0.18054187 0.19014778\n",
      " 0.20862069 0.20738916 0.18054187 0.1726601  0.20862069 0.20738916\n",
      " 0.18054187 0.1726601  0.18152709 0.1453202  0.1453202  0.1453202\n",
      " 0.18152709 0.1453202  0.1453202  0.1453202  0.18152709 0.1453202\n",
      " 0.1453202  0.1453202  0.18152709 0.1453202  0.1453202  0.1453202\n",
      " 0.18152709 0.1453202  0.1453202  0.16576355 0.18152709 0.1453202\n",
      " 0.1453202  0.16576355 0.18152709 0.1453202  0.1453202  0.16280788\n",
      " 0.18152709 0.1453202  0.1453202  0.16280788 0.21059113 0.20985222\n",
      " 0.16600985 0.19581281 0.21059113 0.20985222 0.16600985 0.19581281\n",
      " 0.21059113 0.20985222 0.16600985 0.17758621 0.21059113 0.20985222\n",
      " 0.16600985 0.17758621 0.21059113 0.20985222 0.16600985 0.17857143\n",
      " 0.21059113 0.20985222 0.16600985 0.17857143 0.21059113 0.20985222\n",
      " 0.16600985 0.17118227 0.21059113 0.20985222 0.16600985 0.17118227\n",
      " 0.21009852 0.21453202 0.14975369 0.21083744 0.21009852 0.21453202\n",
      " 0.14975369 0.21083744 0.21009852 0.21453202 0.14975369 0.17660099\n",
      " 0.21009852 0.21453202 0.14975369 0.17660099 0.21009852 0.21453202\n",
      " 0.14975369 0.1820197  0.21009852 0.21453202 0.14975369 0.1820197\n",
      " 0.21009852 0.21453202 0.14975369 0.17192118 0.21009852 0.21453202\n",
      " 0.14975369 0.17192118 0.20985222 0.21453202 0.15049261 0.21083744\n",
      " 0.20985222 0.21453202 0.15049261 0.21083744 0.20985222 0.21453202\n",
      " 0.15049261 0.17610837 0.20985222 0.21453202 0.15049261 0.17610837\n",
      " 0.20985222 0.21453202 0.15049261 0.18965517 0.20985222 0.21453202\n",
      " 0.15049261 0.18965517 0.20985222 0.21453202 0.15049261 0.17315271\n",
      " 0.20985222 0.21453202 0.15049261 0.17315271 0.20935961 0.21995074\n",
      " 0.15197044 0.21009852 0.20935961 0.21995074 0.15197044 0.21009852\n",
      " 0.20935961 0.21995074 0.15197044 0.1773399  0.20935961 0.21995074\n",
      " 0.15197044 0.1773399  0.20935961 0.21995074 0.15197044 0.19408867\n",
      " 0.20935961 0.21995074 0.15197044 0.19408867 0.20935961 0.21995074\n",
      " 0.15197044 0.17463054 0.20935961 0.21995074 0.15197044 0.17463054\n",
      " 0.20960591 0.21945813 0.15295567 0.20985222 0.20960591 0.21945813\n",
      " 0.15295567 0.20985222 0.20960591 0.21945813 0.15295567 0.1770936\n",
      " 0.20960591 0.21945813 0.15295567 0.1770936  0.20960591 0.21945813\n",
      " 0.15295567 0.19458128 0.20960591 0.21945813 0.15295567 0.19458128\n",
      " 0.20960591 0.21945813 0.15295567 0.17389163 0.20960591 0.21945813\n",
      " 0.15295567 0.17389163 0.20935961 0.21650246 0.15369458 0.20960591\n",
      " 0.20935961 0.21650246 0.15369458 0.20960591 0.20935961 0.21650246\n",
      " 0.15369458 0.1770936  0.20935961 0.21650246 0.15369458 0.1770936\n",
      " 0.20935961 0.21650246 0.15369458 0.1953202  0.20935961 0.21650246\n",
      " 0.15369458 0.1953202  0.20935961 0.21650246 0.15369458 0.1729064\n",
      " 0.20935961 0.21650246 0.15369458 0.1729064         nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    }
   ],
   "source": [
    "## ahora con el bucle for hago que me entrene todos los modelos que he definido, con sus gridsearches\n",
    "for nombre, grid_search in grids.items():\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "### HA TARDADO 205min Y HA IDO FATAL... SCORE DE 0.21 APROX EN TODOS LOS MODELOS\n",
    "## TENGO QUE PROBAR MAS COSAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2105911330049261\n",
      "{'imputer__strategy': 'mean', 'reglog__C': 59.94842503189409, 'reglog__penalty': 'l2'}\n",
      "Pipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler()),\n",
      "                ('reglog', LogisticRegression(C=59.94842503189409))])\n",
      "LogisticRegression(C=59.94842503189409)\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "0.2199507389162562\n",
      "{'selectkbest__k': 3, 'svm__C': 5, 'svm__degree': 1, 'svm__gamma': 'scale', 'svm__kernel': 'rbf'}\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('selectkbest', SelectKBest(k=3)),\n",
      "                ('svm', SVC(C=5, degree=1))])\n",
      "SVC(C=5, degree=1)\n",
      "0.14655172413793102\n",
      "{'max_features': 2, 'n_estimators': 10}\n",
      "RandomForestClassifier(max_features=2, n_estimators=10)\n"
     ]
    }
   ],
   "source": [
    "## aquí tenemos el best score, param y estimator de la Logistic Regression\n",
    "print(gs_reg_log.best_score_)\n",
    "print(gs_reg_log.best_params_)\n",
    "print(gs_reg_log.best_estimator_) ## aquí nos devuelve al pipeline, no el modelo en sí. Si quiero ver el modelo, hago lo de la linea de aqui abajo\n",
    "print(gs_reg_log.best_estimator_['reglog'])\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "## aquí tenemos el best score, param y estimator del Random forest\n",
    "# print(gs_grad_boost.best_score_)\n",
    "# print(gs_grad_boost.best_params_)\n",
    "# print(gs_grad_boost.best_estimator_) ## aquí nos devuelve al pipeline, no el modelo en sí. Si quiero ver el modelo, hago lo de la linea de aqui abajo\n",
    "# print(gs_grad_boost.best_estimator_['grad_boost'])\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "## aquí tenemos el best score, param y estimator de mi tercer modelo, el Support Vector Machine\n",
    "print(gs_svm.best_score_)\n",
    "print(gs_svm.best_params_)\n",
    "print(gs_svm.best_estimator_) ## aquí nos devuelve al pipeline, no el modelo en sí. Si quiero ver el modelo, hago lo de la linea de aqui abajo\n",
    "print(gs_svm.best_estimator_['svm'])\n",
    "## aquí tenemos el best score, param y estimator del Random forest\n",
    "print(gs_rand_forest.best_score_)\n",
    "print(gs_rand_forest.best_params_)\n",
    "print(gs_rand_forest.best_estimator_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grid</th>\n",
       "      <th>Best score_TRAIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gs_svm</td>\n",
       "      <td>0.219951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gs_reg_log</td>\n",
       "      <td>0.210591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gs_rand_forest</td>\n",
       "      <td>0.146552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Grid  Best score_TRAIN\n",
       "2          gs_svm          0.219951\n",
       "0      gs_reg_log          0.210591\n",
       "1  gs_rand_forest          0.146552"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## aquí es para ver todos juntos en un df, el mejor score de los modelos, CON LOS DATOS DE TRAIN\n",
    "best_grids_TRAIN = [(i, j.best_score_) for i, j in grids.items()]\n",
    "\n",
    "best_grids_TRAIN = pd.DataFrame(best_grids_TRAIN, columns=[\"Grid\", \"Best score_TRAIN\"]).sort_values(by=\"Best score_TRAIN\", ascending=False)\n",
    "best_grids_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20669291338582677"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El mejor modelo de regresion logistica con los datos de train, AHORA TENGO QUE VERLOS CON LOS DATOS DE TEST:\n",
    "best_model_RL = gs_reg_log.best_estimator_  ## con esto, le estoy aplicando a los datos de TEST las mismas ediciones que apliqué al principio a los datos de TRAIN\n",
    "                                            ## o sea le aplico todo el pipeline\n",
    "best_model_RL.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gs_grad_boost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19256/1741484702.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# El mejor modelo de gradient boosting con los datos de train, AHORA TENGO QUE VERLOS CON LOS DATOS DE TEST:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbest_model_GB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs_grad_boost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m  \u001b[1;31m## con esto, le estoy aplicando a los datos de TEST las mismas ediciones que apliqué al principio a los datos de TRAIN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m                                             \u001b[1;31m## o sea le aplico todo el pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbest_model_GB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gs_grad_boost' is not defined"
     ]
    }
   ],
   "source": [
    "# El mejor modelo de gradient boosting con los datos de train, AHORA TENGO QUE VERLOS CON LOS DATOS DE TEST:\n",
    "best_model_GB = gs_grad_boost.best_estimator_  ## con esto, le estoy aplicando a los datos de TEST las mismas ediciones que apliqué al principio a los datos de TRAIN\n",
    "                                            ## o sea le aplico todo el pipeline\n",
    "best_model_GB.score(X_test, y_test)\n",
    "\n",
    "\n",
    "## no entiendo porque me dice \"NameError: name 'gs_grad_boost' is not defined\" , porque sí que está definido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21161417322834647"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El mejor modelo de svm con los datos de train, AHORA TENGO QUE VERLOS CON LOS DATOS DE TEST:\n",
    "best_model_SVM = gs_svm.best_estimator_  ## con esto, le estoy aplicando a los datos de TEST las mismas ediciones que apliqué al principio a los datos de TRAIN\n",
    "                                            ## o sea le aplico todo el pipeline\n",
    "best_model_SVM.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1437007874015748"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El mejor modelo de random forest con los datos de train, AHORA TENGO QUE VERLOS CON LOS DATOS DE TEST:\n",
    "best_model_RF = gs_rand_forest.best_estimator_  ## con esto, le estoy aplicando a los datos de TEST las mismas ediciones que apliqué al principio a los datos de TRAIN\n",
    "                                            ## o sea le aplico todo el pipeline\n",
    "best_model_RF.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'RandomForestClassifier' and 'Pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19256/704141759.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbest_grids_TEST\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mbest_grids_TEST\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_grids_TEST\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Grid\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Best score_TEST\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Best score_TEST\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mbest_grids_TEST\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36msort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   6269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6270\u001b[0m             indexer = nargsort(\n\u001b[1;32m-> 6271\u001b[1;33m                 \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_position\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_position\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6272\u001b[0m             )\n\u001b[0;32m   6273\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\sorting.py\u001b[0m in \u001b[0;36mnargsort\u001b[1;34m(items, kind, ascending, na_position, key, mask)\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[0mnon_nans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnon_nans\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mnon_nan_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnon_nan_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m     \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnon_nan_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnon_nans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'RandomForestClassifier' and 'Pipeline'"
     ]
    }
   ],
   "source": [
    "## aquí es para ver todos juntos en un df, el mejor score de los modelos, CON LOS DATOS DE TEST\n",
    "best_grids_TEST = [(i, j.best_estimator_) for i, j in grids.items()]\n",
    "\n",
    "best_grids_TEST = pd.DataFrame(best_grids_TEST, columns=[\"Grid\", \"Best score_TEST\"]).sort_values(by=\"Best score_TEST\", ascending=False)\n",
    "best_grids_TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2º Intento --> no está mal, un 53% con logistic regression, pero tengo que hacer mas pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a repetir la misma prueba que antes, con alguna variante:\n",
    "- usaré TODO el dataframe, menos la columna de winery (con el get dummies me daría casi 900 columnas, a ver si consigo arreglar el problema de memoria). Claramente quito también description, como antes\n",
    "- No incluiré el modelo \"Gradient Boosting Classifier\", ya que en la prueba anterior, después del entrenamiento, no me sacaba los resultados porque no reconocía el modelo (desconozco la razó y no encuentro una solución)\n",
    "\n",
    "- Haré una prueba a parte, en otro pc en paralelo a este, SOLO con el modelo de Gradient Boosting, a ver si me da un score decente (y usando todas las columnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vuelvo a cargarme el dataframe inicial para poder borrar facilmente la columna \"winery\" ANTES de hacer el get dummies\n",
    "df_4 = pd.read_csv(\"..\\\\data\\\\processed\\\\vinos_var100_des20_win3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>year</th>\n",
       "      <th>variety_100</th>\n",
       "      <th>designation_20</th>\n",
       "      <th>winery_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>Subtle notes of clean, fresh lemon zest promis...</td>\n",
       "      <td>92</td>\n",
       "      <td>39.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Anne Krebiehl MW</td>\n",
       "      <td>2014</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France</td>\n",
       "      <td>This is a rich wine, labeled sweet but with a ...</td>\n",
       "      <td>93</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>2012</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country                                        description  points  price  \\\n",
       "0  France  Subtle notes of clean, fresh lemon zest promis...      92   39.0   \n",
       "1  France  This is a rich wine, labeled sweet but with a ...      93   59.0   \n",
       "\n",
       "  province region_1       taster_name  year  variety_100  \\\n",
       "0   Alsace   Alsace  Anne Krebiehl MW  2014          7.0   \n",
       "1   Alsace   Alsace        Roger Voss  2012         23.0   \n",
       "\n",
       "          designation_20    winery_3  \n",
       "0  Pfersigberg Grand Cru  Kuentz-Bas  \n",
       "1  Pfersigberg Grand Cru  Kuentz-Bas  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_4.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## borro columnas description y winery_3\n",
    "df_4.drop([\"description\", \"winery_3\"], axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# he decidido que borraré también el taster_name, no creo que me vaya a influir mucho en el modelo\n",
    "df_4.drop([\"taster_name\"], axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>year</th>\n",
       "      <th>variety_100</th>\n",
       "      <th>designation_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>92</td>\n",
       "      <td>39.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>2014</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France</td>\n",
       "      <td>93</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>2012</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>France</td>\n",
       "      <td>90</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>2014</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>France</td>\n",
       "      <td>91</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>2013</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>France</td>\n",
       "      <td>88</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>2011</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Tradition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country  points  price province region_1  year  variety_100  \\\n",
       "0  France      92   39.0   Alsace   Alsace  2014          7.0   \n",
       "1  France      93   59.0   Alsace   Alsace  2012         23.0   \n",
       "2  France      90   45.0   Alsace   Alsace  2014         23.0   \n",
       "3  France      91   59.0   Alsace   Alsace  2013         23.0   \n",
       "4  France      88   17.0   Alsace   Alsace  2011          7.0   \n",
       "\n",
       "          designation_20  \n",
       "0  Pfersigberg Grand Cru  \n",
       "1  Pfersigberg Grand Cru  \n",
       "2  Pfersigberg Grand Cru  \n",
       "3  Pfersigberg Grand Cru  \n",
       "4              Tradition  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ahora procedo a hacer un get_dummies\n",
    "df_4_dumm = pd.get_dummies(data = df_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len original: 5297\n",
      "Len sin outliers en price: 5076\n"
     ]
    }
   ],
   "source": [
    "## ahora quito outliers de price\n",
    "def outliers_quantie(df, feature, param=1.5):  \n",
    "        \n",
    "    iqr_ = iqr(df[feature], nan_policy='omit')\n",
    "    q1 = np.nanpercentile(df[feature], 25)\n",
    "    q3 = np.nanpercentile(df[feature], 75)\n",
    "    \n",
    "    th1 = q1 - iqr_*param\n",
    "    th2 = q3 + iqr_*param\n",
    "    \n",
    "    return df[(df[feature] >= th1) & (df[feature] <= th2)].reset_index(drop=True)\n",
    "\n",
    "df_4_dumm_no_out = outliers_quantie(df_4_dumm, 'price')  ## dejo el parámetro por defect de 1.5, así veo que me ha quitado muchos más outliers que en las pruebas anteriores\n",
    "print(\"Len original:\", len(df_4_dumm))\n",
    "print(\"Len sin outliers en price:\", len(df_4_dumm_no_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>variety_100</th>\n",
       "      <th>country_Argentina</th>\n",
       "      <th>country_Australia</th>\n",
       "      <th>country_Canada</th>\n",
       "      <th>country_France</th>\n",
       "      <th>country_Italy</th>\n",
       "      <th>country_Spain</th>\n",
       "      <th>...</th>\n",
       "      <th>designation_20_Stagecoach Vineyard</th>\n",
       "      <th>designation_20_Stillwater Creek Vineyard</th>\n",
       "      <th>designation_20_Sur Lie</th>\n",
       "      <th>designation_20_Tinto</th>\n",
       "      <th>designation_20_Tradition</th>\n",
       "      <th>designation_20_Traditions</th>\n",
       "      <th>designation_20_Unoaked</th>\n",
       "      <th>designation_20_Vendimia Seleccionada</th>\n",
       "      <th>designation_20_Vieilles Vignes</th>\n",
       "      <th>designation_20_Vintner's Reserve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5071</th>\n",
       "      <td>90</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5072</th>\n",
       "      <td>91</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5073</th>\n",
       "      <td>95</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5074</th>\n",
       "      <td>91</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5075</th>\n",
       "      <td>94</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5076 rows × 401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      points  price  year  variety_100  country_Argentina  country_Australia  \\\n",
       "0         92   39.0  2014          7.0                  0                  0   \n",
       "1         93   59.0  2012         23.0                  0                  0   \n",
       "2         90   45.0  2014         23.0                  0                  0   \n",
       "3         91   59.0  2013         23.0                  0                  0   \n",
       "4         88   17.0  2011          7.0                  0                  0   \n",
       "...      ...    ...   ...          ...                ...                ...   \n",
       "5071      90   49.0  2013         11.0                  0                  0   \n",
       "5072      91   70.0  2012         11.0                  0                  0   \n",
       "5073      95   65.0  2010         11.0                  0                  0   \n",
       "5074      91   70.0  2011         11.0                  0                  0   \n",
       "5075      94   70.0  2012         11.0                  0                  0   \n",
       "\n",
       "      country_Canada  country_France  country_Italy  country_Spain  ...  \\\n",
       "0                  0               1              0              0  ...   \n",
       "1                  0               1              0              0  ...   \n",
       "2                  0               1              0              0  ...   \n",
       "3                  0               1              0              0  ...   \n",
       "4                  0               1              0              0  ...   \n",
       "...              ...             ...            ...            ...  ...   \n",
       "5071               0               0              1              0  ...   \n",
       "5072               0               0              1              0  ...   \n",
       "5073               0               0              1              0  ...   \n",
       "5074               0               0              1              0  ...   \n",
       "5075               0               0              1              0  ...   \n",
       "\n",
       "      designation_20_Stagecoach Vineyard  \\\n",
       "0                                      0   \n",
       "1                                      0   \n",
       "2                                      0   \n",
       "3                                      0   \n",
       "4                                      0   \n",
       "...                                  ...   \n",
       "5071                                   0   \n",
       "5072                                   0   \n",
       "5073                                   0   \n",
       "5074                                   0   \n",
       "5075                                   0   \n",
       "\n",
       "      designation_20_Stillwater Creek Vineyard  designation_20_Sur Lie  \\\n",
       "0                                            0                       0   \n",
       "1                                            0                       0   \n",
       "2                                            0                       0   \n",
       "3                                            0                       0   \n",
       "4                                            0                       0   \n",
       "...                                        ...                     ...   \n",
       "5071                                         0                       0   \n",
       "5072                                         0                       0   \n",
       "5073                                         0                       0   \n",
       "5074                                         0                       0   \n",
       "5075                                         0                       0   \n",
       "\n",
       "      designation_20_Tinto  designation_20_Tradition  \\\n",
       "0                        0                         0   \n",
       "1                        0                         0   \n",
       "2                        0                         0   \n",
       "3                        0                         0   \n",
       "4                        0                         1   \n",
       "...                    ...                       ...   \n",
       "5071                     0                         0   \n",
       "5072                     0                         0   \n",
       "5073                     0                         0   \n",
       "5074                     0                         0   \n",
       "5075                     0                         0   \n",
       "\n",
       "      designation_20_Traditions  designation_20_Unoaked  \\\n",
       "0                             0                       0   \n",
       "1                             0                       0   \n",
       "2                             0                       0   \n",
       "3                             0                       0   \n",
       "4                             0                       0   \n",
       "...                         ...                     ...   \n",
       "5071                          0                       0   \n",
       "5072                          0                       0   \n",
       "5073                          0                       0   \n",
       "5074                          0                       0   \n",
       "5075                          0                       0   \n",
       "\n",
       "      designation_20_Vendimia Seleccionada  designation_20_Vieilles Vignes  \\\n",
       "0                                        0                               0   \n",
       "1                                        0                               0   \n",
       "2                                        0                               0   \n",
       "3                                        0                               0   \n",
       "4                                        0                               0   \n",
       "...                                    ...                             ...   \n",
       "5071                                     0                               0   \n",
       "5072                                     0                               0   \n",
       "5073                                     0                               0   \n",
       "5074                                     0                               0   \n",
       "5075                                     0                               0   \n",
       "\n",
       "      designation_20_Vintner's Reserve  \n",
       "0                                    0  \n",
       "1                                    0  \n",
       "2                                    0  \n",
       "3                                    0  \n",
       "4                                    0  \n",
       "...                                ...  \n",
       "5071                                 0  \n",
       "5072                                 0  \n",
       "5073                                 0  \n",
       "5074                                 0  \n",
       "5075                                 0  \n",
       "\n",
       "[5076 rows x 401 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_4_dumm_no_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defino 3 modelos, cada uno con su pipeline (menos él de randomForest , que no lo necesita, \n",
    "# simplemente no le aplico más parametros que el simple modelo)\n",
    "\n",
    "## modelo1\n",
    "reg_log2 = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer()),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"reglog\", LogisticRegression())\n",
    "])\n",
    "\n",
    "## modelo2\n",
    "svm2 = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"selectkbest\", SelectKBest()),\n",
    "    (\"svm\", SVC())\n",
    "])\n",
    "\n",
    "## modelo3\n",
    "rand_forest2 = RandomForestClassifier()\n",
    "\n",
    "\n",
    "##busco los mejores parametros(lo indico con doble barra baja y el nombre del parametro del que quiero encontrar el mejor valor\n",
    "reg_log2_param = {\n",
    "    \"imputer__strategy\": ['mean', 'median'],\n",
    "    \"reglog__penalty\": ['l1', 'l2'],\n",
    "    \"reglog__C\": np.logspace(0, 4, 10)\n",
    "}\n",
    "\n",
    "svm2_param = {\n",
    "    'selectkbest__k': [2, 3, 4],\n",
    "    'svm__kernel': ['linear', 'rbf', 'sigmoid', 'poly'],\n",
    "    'svm__C': [0.001, 0.1, 0.5, 1, 5, 10, 100],\n",
    "    'svm__degree': [1,2,3,4],\n",
    "    'svm__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "rand_forest2_param = {\n",
    "    \"n_estimators\": [10, 100, 1000],\n",
    "    \"max_features\": [1,2,3]\n",
    "}\n",
    "\n",
    "\n",
    "## ahora hago los gridseraches de cada modelo, cada uno con sus parametros\n",
    "gs_reg_log2 = GridSearchCV(reg_log2,\n",
    "                         reg_log2_param,\n",
    "                         cv = 10,\n",
    "                         scoring = 'accuracy',\n",
    "                         verbose = 1,\n",
    "                         n_jobs = -1)\n",
    "\n",
    "gs_svm2 = GridSearchCV(svm2,\n",
    "                         svm2_param,\n",
    "                         cv = 10,\n",
    "                         scoring = 'accuracy',\n",
    "                         verbose = 1,\n",
    "                         n_jobs = -1)\n",
    "\n",
    "gs_rand_forest2 = GridSearchCV(rand_forest2,\n",
    "                         rand_forest2_param,\n",
    "                         cv = 10,\n",
    "                         scoring = 'accuracy',\n",
    "                         verbose = 1,\n",
    "                         n_jobs = -1)\n",
    "\n",
    "##ahora ya nos guardamos cada uno de nuestros gridsearches en un diccionario\n",
    "grids2 = {\"gs_reg_log2\": gs_reg_log2,\n",
    "        \"gs_svm2\": gs_svm2,\n",
    "        \"gs_rand_forest2\": gs_rand_forest2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = df_4_dumm_no_out.drop([\"variety_100\"], axis = 1)\n",
    "y2 = np.array(df_4_dumm_no_out[\"variety_100\"]).reshape(-1,)\n",
    "\n",
    "## separo en train y test\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size = 0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  UserWarning,\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "200 fits failed out of a total of 400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan 0.51330049        nan 0.51009852        nan 0.51231527\n",
      "        nan 0.51133005        nan 0.51083744        nan 0.51182266\n",
      "        nan 0.50985222        nan 0.50985222        nan 0.51108374\n",
      "        nan 0.51133005        nan 0.51330049        nan 0.51009852\n",
      "        nan 0.51231527        nan 0.51133005        nan 0.51083744\n",
      "        nan 0.51182266        nan 0.50985222        nan 0.50985222\n",
      "        nan 0.51108374        nan 0.51133005]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 672 candidates, totalling 6720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [ 38  48  56  93 103 111 116 127 157 166 189 192 200 203 217 218 258 262\n",
      " 269 272 280 281] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  UserWarning,\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "37 fits failed out of a total of 90.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "28 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 458, in fit\n",
      "    for i, t in enumerate(trees)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 1044, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 209, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 184, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 420, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 133, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 229, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 729, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 701, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 41, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 1507328 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "8 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 458, in fit\n",
      "    for i, t in enumerate(trees)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 1044, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 209, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 184, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 420, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 133, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 229, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 729, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 701, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 41, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 1474560 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 327, in fit\n",
      "    X, y, multi_output=True, accept_sparse=\"csc\", dtype=DTYPE\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\base.py\", line 572, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 968, in check_X_y\n",
      "    estimator=estimator,\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 738, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\generic.py\", line 1993, in __array__\n",
      "    return np.asarray(self._values, dtype=dtype)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 5.58 MiB for an array with shape (3654, 400) and data type float32\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.47660099 0.48669951        nan 0.47019704        nan        nan\n",
      "        nan 0.48694581        nan]\n",
      "  category=UserWarning,\n"
     ]
    }
   ],
   "source": [
    "## ahora con el bucle for hago que me entrene todos los modelos que he definido, con sus gridsearches\n",
    "for nombre, grid_search in grids2.items():\n",
    "    grid_search.fit(X2_train, y2_train)\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5133004926108373\n",
      "{'imputer__strategy': 'mean', 'reglog__C': 1.0, 'reglog__penalty': 'l2'}\n",
      "Pipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler()),\n",
      "                ('reglog', LogisticRegression())])\n",
      "LogisticRegression()\n",
      "----------------------------------------------------------------------\n",
      "0.14827586206896554\n",
      "{'selectkbest__k': 3, 'svm__C': 0.001, 'svm__degree': 1, 'svm__gamma': 'scale', 'svm__kernel': 'linear'}\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('selectkbest', SelectKBest(k=3)),\n",
      "                ('svm', SVC(C=0.001, degree=1, kernel='linear'))])\n",
      "SVC(C=0.001, degree=1, kernel='linear')\n",
      "----------------------------------------------------------------------\n",
      "0.48694581280788174\n",
      "{'max_features': 3, 'n_estimators': 100}\n",
      "RandomForestClassifier(max_features=3)\n"
     ]
    }
   ],
   "source": [
    "## aquí tenemos el best score, param y estimator de la Logistic Regression\n",
    "print(gs_reg_log2.best_score_)\n",
    "print(gs_reg_log2.best_params_)\n",
    "print(gs_reg_log2.best_estimator_) ## aquí nos devuelve al pipeline, no el modelo en sí. Si quiero ver el modelo, hago lo de la linea de aqui abajo\n",
    "print(gs_reg_log2.best_estimator_['reglog'])\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "## aquí tenemos el best score, param y estimator de mi tercer modelo, el Support Vector Machine\n",
    "print(gs_svm2.best_score_)\n",
    "print(gs_svm2.best_params_)\n",
    "print(gs_svm2.best_estimator_) ## aquí nos devuelve al pipeline, no el modelo en sí. Si quiero ver el modelo, hago lo de la linea de aqui abajo\n",
    "print(gs_svm2.best_estimator_['svm'])\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "## aquí tenemos el best score, param y estimator del Random forest\n",
    "print(gs_rand_forest2.best_score_)\n",
    "print(gs_rand_forest2.best_params_)\n",
    "print(gs_rand_forest2.best_estimator_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grid</th>\n",
       "      <th>Best score_TRAIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gs_reg_log2</td>\n",
       "      <td>0.513300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gs_rand_forest2</td>\n",
       "      <td>0.486946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gs_svm2</td>\n",
       "      <td>0.148276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Grid  Best score_TRAIN\n",
       "0      gs_reg_log2          0.513300\n",
       "2  gs_rand_forest2          0.486946\n",
       "1          gs_svm2          0.148276"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## aquí es para ver todos juntos en un df, el mejor score de los modelos, CON LOS DATOS DE TRAIN\n",
    "best_grids2_TRAIN = [(i, j.best_score_) for i, j in grids2.items()]\n",
    "\n",
    "best_grids2_TRAIN = pd.DataFrame(best_grids2_TRAIN, columns=[\"Grid\", \"Best score_TRAIN\"]).sort_values(by=\"Best score_TRAIN\", ascending=False)\n",
    "best_grids2_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.531496062992126"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El mejor modelo de regresion logistica con los datos de train, AHORA TENGO QUE VERLOS CON LOS DATOS DE TEST:\n",
    "best_model_RL2 = gs_reg_log2.best_estimator_  ## con esto, le estoy aplicando a los datos de TEST las mismas ediciones que apliqué al principio a los datos de TRAIN\n",
    "                                            ## o sea le aplico todo el pipeline\n",
    "best_model_RL2.score(X2_test, y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13976377952755906"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El mejor modelo de svm con los datos de train, AHORA TENGO QUE VERLOS CON LOS DATOS DE TEST:\n",
    "best_model_SVM2 = gs_svm2.best_estimator_  ## con esto, le estoy aplicando a los datos de TEST las mismas ediciones que apliqué al principio a los datos de TRAIN\n",
    "                                            ## o sea le aplico todo el pipeline\n",
    "best_model_SVM2.score(X2_test, y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4704724409448819"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El mejor modelo de random forest con los datos de train, AHORA TENGO QUE VERLOS CON LOS DATOS DE TEST:\n",
    "best_model_RF2 = gs_rand_forest2.best_estimator_  ## con esto, le estoy aplicando a los datos de TEST las mismas ediciones que apliqué al principio a los datos de TRAIN\n",
    "                                            ## o sea le aplico todo el pipeline\n",
    "best_model_RF2.score(X2_test, y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "## de momento el mejor resultado lo tengo con Logistic Regression, con datos de Test me sale un score del 53%\n",
    "## pero sigue bajo, tengo que hacer más pruebas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3º Intento ---> he llegado a un 55,80 % de aciertos. La cosa va aumentando pero aún no es suficiente\n",
    "Nota antes de empezar: Intento usar TODO el data frame (menos description, claramente), a ver si al no tener el 4ºmodelo(gradient boosting) no me da el problema de memoria como al principio\n",
    "Nota después del resultado: Finalmente he ido quitando modelos y me he quedado solo con el Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>variety_100</th>\n",
       "      <th>province_Alsace</th>\n",
       "      <th>province_Australia Other</th>\n",
       "      <th>province_Beaujolais</th>\n",
       "      <th>province_Bordeaux</th>\n",
       "      <th>province_British Columbia</th>\n",
       "      <th>province_Burgundy</th>\n",
       "      <th>...</th>\n",
       "      <th>winery_3_Yering Station</th>\n",
       "      <th>winery_3_Ysios</th>\n",
       "      <th>winery_3_ZD</th>\n",
       "      <th>winery_3_Zaca Mesa</th>\n",
       "      <th>winery_3_Zenith</th>\n",
       "      <th>winery_3_Zinfandelic</th>\n",
       "      <th>winery_3_Zolo</th>\n",
       "      <th>winery_3_Zotovich Cellars</th>\n",
       "      <th>winery_3_Zuani</th>\n",
       "      <th>winery_3_àMaurice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1233 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   points  price  year  variety_100  province_Alsace  \\\n",
       "0      92   39.0  2014          7.0                1   \n",
       "1      93   59.0  2012         23.0                1   \n",
       "\n",
       "   province_Australia Other  province_Beaujolais  province_Bordeaux  \\\n",
       "0                         0                    0                  0   \n",
       "1                         0                    0                  0   \n",
       "\n",
       "   province_British Columbia  province_Burgundy  ...  winery_3_Yering Station  \\\n",
       "0                          0                  0  ...                        0   \n",
       "1                          0                  0  ...                        0   \n",
       "\n",
       "   winery_3_Ysios  winery_3_ZD  winery_3_Zaca Mesa  winery_3_Zenith  \\\n",
       "0               0            0                   0                0   \n",
       "1               0            0                   0                0   \n",
       "\n",
       "   winery_3_Zinfandelic  winery_3_Zolo  winery_3_Zotovich Cellars  \\\n",
       "0                     0              0                          0   \n",
       "1                     0              0                          0   \n",
       "\n",
       "   winery_3_Zuani  winery_3_àMaurice  \n",
       "0               0                  0  \n",
       "1               0                  0  \n",
       "\n",
       "[2 rows x 1233 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3.head(2)  ## usaré este dataframe (recuerdo que estaba llamando al csv \"vinos_var100_des20_win3_noOutliers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defino 3 modelos, cada uno con su pipeline (menos él de randomForest , que no lo necesita, \n",
    "# simplemente no le aplico más parametros que el simple modelo)\n",
    "\n",
    "#### 1. después de 25minutos me da este fallo -->> MemoryError: Unable to allocate 34.3 MiB for an array with shape (1231, 3654) and data type int64\n",
    "#### pero quiero usar todas las columnas, así que haré otra prueba \"aligerando\" el proceso y quitando el SVC, que de todas formas en la última prueba me había dado resultados pésimos\n",
    "#### 2. lo dejo este código, pero comentado, para que no se pierda el trabajo y volveré a ejecutar las celdas\n",
    "#### 3. Quitando SVC he conseguido un score del 55% en test, mejor que antes, y como veo que random forest sigue por debajo, ya me quedo solo con LOGISTIC REGRESSION y voy haciendo pruebas de quitar columnas\n",
    "########### Mantengo esta celda, pero comentando también random forest\n",
    "\n",
    "## modelo1\n",
    "reg_log3 = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer()),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"reglog\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# ## modelo2\n",
    "# svm3 = Pipeline(steps=[\n",
    "#     (\"scaler\", StandardScaler()),\n",
    "#     (\"selectkbest\", SelectKBest()),\n",
    "#     (\"svm\", SVC())\n",
    "# ])\n",
    "\n",
    "## modelo3\n",
    "# rand_forest3 = RandomForestClassifier()\n",
    "\n",
    "\n",
    "##busco los mejores parametros(lo indico con doble barra baja y el nombre del parametro del que quiero encontrar el mejor valor\n",
    "reg_log3_param = {\n",
    "    \"imputer__strategy\": ['mean', 'median'],\n",
    "    \"reglog__penalty\": ['l1', 'l2'],\n",
    "    \"reglog__C\": np.logspace(0, 4, 10)\n",
    "}\n",
    "\n",
    "# svm3_param = {\n",
    "#     'selectkbest__k': [2, 3, 4],\n",
    "#     'svm__kernel': ['linear', 'rbf', 'sigmoid', 'poly'],\n",
    "#     'svm__C': [0.001, 0.1, 0.5, 1, 5, 10, 100],\n",
    "#     'svm__degree': [1,2,3,4],\n",
    "#     'svm__gamma': ['scale', 'auto']\n",
    "# }\n",
    "\n",
    "# rand_forest3_param = {\n",
    "#     \"n_estimators\": [10, 100, 1000],\n",
    "#     \"max_features\": [1,2,3]\n",
    "# }\n",
    "\n",
    "\n",
    "## ahora hago los gridseraches de cada modelo, cada uno con sus parametros\n",
    "gs_reg_log3 = GridSearchCV(reg_log3,\n",
    "                         reg_log3_param,\n",
    "                         cv = 10,\n",
    "                         scoring = 'accuracy',\n",
    "                         verbose = 1,\n",
    "                         n_jobs = -1)\n",
    "\n",
    "# gs_svm3 = GridSearchCV(svm3,\n",
    "#                          svm3_param,\n",
    "#                          cv = 10,\n",
    "#                          scoring = 'accuracy',\n",
    "#                          verbose = 1,\n",
    "#                          n_jobs = -1)\n",
    "\n",
    "# gs_rand_forest3 = GridSearchCV(rand_forest3,\n",
    "#                          rand_forest3_param,\n",
    "#                          cv = 10,\n",
    "#                          scoring = 'accuracy',\n",
    "#                          verbose = 1,\n",
    "#                          n_jobs = -1)\n",
    "\n",
    "##ahora ya nos guardamos cada uno de nuestros gridsearches en un diccionario\n",
    "grids3 = {\"gs_reg_log3\": gs_reg_log3,\n",
    "        # \"gs_svm2\": gs_svm3,\n",
    "        # \"gs_rand_forest2\": gs_rand_forest3\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = df_3.drop([\"variety_100\"], axis = 1)\n",
    "y3 = np.array(df_3[\"variety_100\"]).reshape(-1,)\n",
    "\n",
    "## separo en train y test\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size = 0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  UserWarning,\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "200 fits failed out of a total of 400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan 0.54704433        nan 0.54827586        nan 0.54827586\n",
      "        nan 0.54852217        nan 0.54901478        nan 0.54950739\n",
      "        nan 0.54827586        nan 0.54827586        nan 0.54802956\n",
      "        nan 0.54827586        nan 0.54704433        nan 0.54827586\n",
      "        nan 0.54827586        nan 0.54852217        nan 0.54901478\n",
      "        nan 0.54950739        nan 0.54827586        nan 0.54827586\n",
      "        nan 0.54802956        nan 0.54827586]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "## ahora con el bucle for hago que me entrene todos los modelos que he definido, con sus gridsearches\n",
    "for nombre, grid_search in grids3.items():\n",
    "    grid_search.fit(X3_train, y3_train)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5495073891625615\n",
      "{'imputer__strategy': 'mean', 'reglog__C': 166.81005372000593, 'reglog__penalty': 'l2'}\n",
      "Pipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler()),\n",
      "                ('reglog', LogisticRegression(C=166.81005372000593))])\n",
      "LogisticRegression(C=166.81005372000593)\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## aquí tenemos el best score, param y estimator de la Logistic Regression\n",
    "print(gs_reg_log3.best_score_)\n",
    "print(gs_reg_log3.best_params_)\n",
    "print(gs_reg_log3.best_estimator_) ## aquí nos devuelve al pipeline, no el modelo en sí. Si quiero ver el modelo, hago lo de la linea de aqui abajo\n",
    "print(gs_reg_log3.best_estimator_['reglog'])\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "## aquí tenemos el best score, param y estimator de mi tercer modelo, el Support Vector Machine\n",
    "# print(gs_svm3.best_score_)\n",
    "# print(gs_svm3.best_params_)\n",
    "# print(gs_svm3.best_estimator_) ## aquí nos devuelve al pipeline, no el modelo en sí. Si quiero ver el modelo, hago lo de la linea de aqui abajo\n",
    "# print(gs_svm3.best_estimator_['svm'])\n",
    "# print(\"----------------------------------------------------------------------\")\n",
    "## aquí tenemos el best score, param y estimator del Random forest\n",
    "# print(gs_rand_forest3.best_score_)\n",
    "# print(gs_rand_forest3.best_params_)\n",
    "# print(gs_rand_forest3.best_estimator_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grid</th>\n",
       "      <th>Best score_TRAIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gs_reg_log2</td>\n",
       "      <td>0.549507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Grid  Best score_TRAIN\n",
       "0  gs_reg_log2          0.549507"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## aquí es para ver todos juntos en un df, el mejor score de los modelos, CON LOS DATOS DE TRAIN\n",
    "best_grids3_TRAIN = [(i, j.best_score_) for i, j in grids3.items()]\n",
    "\n",
    "best_grids3_TRAIN = pd.DataFrame(best_grids3_TRAIN, columns=[\"Grid\", \"Best score_TRAIN\"]).sort_values(by=\"Best score_TRAIN\", ascending=False)\n",
    "best_grids3_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5580708661417323"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El mejor modelo de regresion logistica con los datos de train, AHORA TENGO QUE VERLOS CON LOS DATOS DE TEST:\n",
    "best_model_RL3 = gs_reg_log3.best_estimator_  ## con esto, le estoy aplicando a los datos de TEST las mismas ediciones que apliqué al principio a los datos de TRAIN\n",
    "                                            ## o sea le aplico todo el pipeline\n",
    "best_model_RL3.score(X3_test, y3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # El mejor modelo de svm con los datos de train, AHORA TENGO QUE VERLOS CON LOS DATOS DE TEST:\n",
    "# best_model_SVM3 = gs_svm3.best_estimator_  ## con esto, le estoy aplicando a los datos de TEST las mismas ediciones que apliqué al principio a los datos de TRAIN\n",
    "#                                             ## o sea le aplico todo el pipeline\n",
    "# best_model_SVM3.score(X3_test, y3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.515748031496063"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El mejor modelo de random forest con los datos de train, AHORA TENGO QUE VERLOS CON LOS DATOS DE TEST:\n",
    "# best_model_RF3 = gs_rand_forest3.best_estimator_  ## con esto, le estoy aplicando a los datos de TEST las mismas ediciones que apliqué al principio a los datos de TRAIN\n",
    "#                                             ## o sea le aplico todo el pipeline\n",
    "# best_model_RF3.score(X3_test, y3_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4º Intento *ME QUEDO CON ESTE*\n",
    "\n",
    ">>> 57,67% (quitando country y province)  <<< es el mejor % de aciertos obtenido en todo el estudio>>>\n",
    "(NOTA antes de empezar con el modelo: Como de zonas geográficas tengo Country, Province y Region, voy haciendo pruebas quitando de estas columnas, primero solo una y luego otra, a ver.\n",
    "NOTA después de las varias pruebas: relativo a la localización geográfica, me quedo SOLO con la columna \"region_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uso este df y voy quitando cosas\n",
    "df_4 = pd.read_csv(\"..\\\\data\\\\processed\\\\vinos_var100_des20_win3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. quito country, me da mismo score que intento 3\n",
    "# 2. quito tambien province\n",
    "df_4.drop([\"description\",\"province\", \"taster_name\", \"country\"], axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>region_1</th>\n",
       "      <th>year</th>\n",
       "      <th>variety_100</th>\n",
       "      <th>designation_20</th>\n",
       "      <th>winery_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "      <td>39.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>2014</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>2012</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>2014</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>2013</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>2011</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Tradition</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5292</th>\n",
       "      <td>91</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>2011</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Bel Colle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5293</th>\n",
       "      <td>94</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>2012</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Bel Colle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5294</th>\n",
       "      <td>92</td>\n",
       "      <td>76.0</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>2012</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Fratelli Alessandria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5295</th>\n",
       "      <td>91</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>2011</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Fratelli Alessandria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5296</th>\n",
       "      <td>95</td>\n",
       "      <td>78.0</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>2013</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Fratelli Alessandria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5297 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      points  price region_1  year  variety_100         designation_20  \\\n",
       "0         92   39.0   Alsace  2014          7.0  Pfersigberg Grand Cru   \n",
       "1         93   59.0   Alsace  2012         23.0  Pfersigberg Grand Cru   \n",
       "2         90   45.0   Alsace  2014         23.0  Pfersigberg Grand Cru   \n",
       "3         91   59.0   Alsace  2013         23.0  Pfersigberg Grand Cru   \n",
       "4         88   17.0   Alsace  2011          7.0              Tradition   \n",
       "...      ...    ...      ...   ...          ...                    ...   \n",
       "5292      91   70.0   Barolo  2011         11.0            Monvigliero   \n",
       "5293      94   70.0   Barolo  2012         11.0            Monvigliero   \n",
       "5294      92   76.0   Barolo  2012         11.0            Monvigliero   \n",
       "5295      91   75.0   Barolo  2011         11.0            Monvigliero   \n",
       "5296      95   78.0   Barolo  2013         11.0            Monvigliero   \n",
       "\n",
       "                  winery_3  \n",
       "0               Kuentz-Bas  \n",
       "1               Kuentz-Bas  \n",
       "2               Kuentz-Bas  \n",
       "3               Kuentz-Bas  \n",
       "4               Kuentz-Bas  \n",
       "...                    ...  \n",
       "5292             Bel Colle  \n",
       "5293             Bel Colle  \n",
       "5294  Fratelli Alessandria  \n",
       "5295  Fratelli Alessandria  \n",
       "5296  Fratelli Alessandria  \n",
       "\n",
       "[5297 rows x 7 columns]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ahora procedo a hacer un get_dummies\n",
    "df_4_dumm = pd.get_dummies(data = df_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len original: 5297\n",
      "Len sin outliers en price: 5076\n"
     ]
    }
   ],
   "source": [
    "## quito outliers de la columna price\n",
    "def outliers_quantie(df, feature, param=1.5):  \n",
    "        \n",
    "    iqr_ = iqr(df[feature], nan_policy='omit')\n",
    "    q1 = np.nanpercentile(df[feature], 25)\n",
    "    q3 = np.nanpercentile(df[feature], 75)\n",
    "    \n",
    "    th1 = q1 - iqr_*param\n",
    "    th2 = q3 + iqr_*param\n",
    "    \n",
    "    return df[(df[feature] >= th1) & (df[feature] <= th2)].reset_index(drop=True)\n",
    "\n",
    "df_4_dumm_no_out = outliers_quantie(df_4_dumm, 'price')  ## dejo el parámetro por defect de 1.5, así veo que me ha quitado muchos más outliers que en las pruebas anteriores\n",
    "print(\"Len original:\", len(df_4_dumm))\n",
    "print(\"Len sin outliers en price:\", len(df_4_dumm_no_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precio máximo: 74.0 $\n",
      "Precio mínimo: 4.0 $\n",
      "Puntuación máxima: 98 $\n",
      "Puntuación mínima: 80 $\n"
     ]
    }
   ],
   "source": [
    "print(\"Precio máximo: {} $\".format(df_4_dumm_no_out[\"price\"].max()))\n",
    "print(\"Precio mínimo: {} $\".format(df_4_dumm_no_out[\"price\"].min()))\n",
    "\n",
    "print(\"Puntuación máxima: {} $\".format(df_4_dumm_no_out[\"points\"].max()))\n",
    "print(\"Puntuación mínima: {} $\".format(df_4_dumm_no_out[\"points\"].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pipeline y gridsearch\n",
    "\n",
    "\n",
    "reg_log4 = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer()),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"reglog4\", LogisticRegression())\n",
    "])\n",
    "\n",
    "reg_log4_param = {\n",
    "    \"imputer__strategy\": ['mean', 'median'],\n",
    "    \"reglog4__max_iter\" : [100, 200, 500, 1000],\n",
    "    \"reglog4__penalty\": ['l1', 'l2'],\n",
    "    \"reglog4__C\": np.logspace(0, 4, 10)\n",
    "}\n",
    "\n",
    "gs_reg_log4 = GridSearchCV(reg_log4,\n",
    "                         reg_log4_param,\n",
    "                         cv = 10,\n",
    "                         scoring = 'accuracy',\n",
    "                         verbose = 1,\n",
    "                         n_jobs = -1)\n",
    "\n",
    "grids4 = {\"gs_reg_log4\": gs_reg_log4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'imputer', 'scaler', 'reglog4', 'imputer__add_indicator', 'imputer__copy', 'imputer__fill_value', 'imputer__missing_values', 'imputer__strategy', 'imputer__verbose', 'scaler__copy', 'scaler__with_mean', 'scaler__with_std', 'reglog4__C', 'reglog4__class_weight', 'reglog4__dual', 'reglog4__fit_intercept', 'reglog4__intercept_scaling', 'reglog4__l1_ratio', 'reglog4__max_iter', 'reglog4__multi_class', 'reglog4__n_jobs', 'reglog4__penalty', 'reglog4__random_state', 'reglog4__solver', 'reglog4__tol', 'reglog4__verbose', 'reglog4__warm_start'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_iter = 1000\n",
    "# reg_log4.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 = df_4_dumm_no_out.drop([\"variety_100\"], axis = 1)\n",
    "y4 = np.array(df_4_dumm_no_out[\"variety_100\"]).reshape(-1,)\n",
    "\n",
    "## separo en train y test\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size = 0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X4_train (4060, 1199)\n",
      "X4_test (1016, 1199)\n",
      "y4_train (4060,)\n",
      "y4_test (1016,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X4_train\", X4_train.shape)\n",
    "print(\"X4_test\", X4_test.shape)\n",
    "print(\"y4_train\", y4_train.shape)\n",
    "print(\"y4_test\", y4_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 160 candidates, totalling 1600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  UserWarning,\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "800 fits failed out of a total of 1600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "800 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan 0.54605911        nan 0.54704433        nan 0.54729064\n",
      "        nan 0.54704433        nan 0.54827586        nan 0.55172414\n",
      "        nan 0.55              nan 0.54950739        nan 0.54655172\n",
      "        nan 0.55              nan 0.55123153        nan 0.55197044\n",
      "        nan 0.5453202         nan 0.54802956        nan 0.55073892\n",
      "        nan 0.55246305        nan 0.54605911        nan 0.54802956\n",
      "        nan 0.5453202         nan 0.54655172        nan 0.54729064\n",
      "        nan 0.54876847        nan 0.54310345        nan 0.53891626\n",
      "        nan 0.54704433        nan 0.54802956        nan 0.54408867\n",
      "        nan 0.53103448        nan 0.54630542        nan 0.54852217\n",
      "        nan 0.54310345        nan 0.53004926        nan 0.54778325\n",
      "        nan 0.54827586        nan 0.54187192        nan 0.52881773\n",
      "        nan 0.54655172        nan 0.54802956        nan 0.54064039\n",
      "        nan 0.52906404        nan 0.54605911        nan 0.54704433\n",
      "        nan 0.54729064        nan 0.54704433        nan 0.54827586\n",
      "        nan 0.55172414        nan 0.55              nan 0.54950739\n",
      "        nan 0.54655172        nan 0.55              nan 0.55123153\n",
      "        nan 0.55197044        nan 0.5453202         nan 0.54802956\n",
      "        nan 0.55073892        nan 0.55246305        nan 0.54605911\n",
      "        nan 0.54802956        nan 0.5453202         nan 0.54655172\n",
      "        nan 0.54729064        nan 0.54876847        nan 0.54310345\n",
      "        nan 0.53891626        nan 0.54704433        nan 0.54802956\n",
      "        nan 0.54408867        nan 0.53103448        nan 0.54630542\n",
      "        nan 0.54852217        nan 0.54310345        nan 0.53004926\n",
      "        nan 0.54778325        nan 0.54827586        nan 0.54187192\n",
      "        nan 0.52881773        nan 0.54655172        nan 0.54802956\n",
      "        nan 0.54064039        nan 0.52906404]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "## ahora con el bucle for hago que me entrene todos los modelos que he definido, con sus gridsearches\n",
    "for nombre, grid_search in grids4.items():\n",
    "    grid_search.fit(X4_train, y4_train)\n",
    "    \n",
    "### ha tardado 65minutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5524630541871922\n",
      "{'imputer__strategy': 'mean', 'reglog4__C': 21.544346900318832, 'reglog4__max_iter': 1000, 'reglog4__penalty': 'l2'}\n",
      "Pipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler()),\n",
      "                ('reglog4',\n",
      "                 LogisticRegression(C=21.544346900318832, max_iter=1000))])\n",
      "LogisticRegression(C=21.544346900318832, max_iter=1000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grid</th>\n",
       "      <th>Best score_TRAIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gs_reg_log4</td>\n",
       "      <td>0.552463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Grid  Best score_TRAIN\n",
       "0  gs_reg_log4          0.552463"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## aquí tenemos el best score, param y estimator de la Logistic Regression\n",
    "print(gs_reg_log4.best_score_)\n",
    "print(gs_reg_log4.best_params_)\n",
    "print(gs_reg_log4.best_estimator_) ## aquí nos devuelve al pipeline, no el modelo en sí. Si quiero ver el modelo, hago lo de la linea de aqui abajo\n",
    "print(gs_reg_log4.best_estimator_['reglog4'])\n",
    "\n",
    "## aquí es para ver todos juntos en un df, el mejor score de los modelos, CON LOS DATOS DE TRAIN\n",
    "best_grids4_TRAIN = [(i, j.best_score_) for i, j in grids4.items()]\n",
    "\n",
    "best_grids4_TRAIN = pd.DataFrame(best_grids4_TRAIN, columns=[\"Grid\", \"Best score_TRAIN\"]).sort_values(by=\"Best score_TRAIN\", ascending=False)\n",
    "best_grids4_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5767716535433071"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El mejor modelo de regresion logistica con los datos de train, AHORA TENGO QUE VERLOS CON LOS DATOS DE TEST:\n",
    "best_model_RL4 = gs_reg_log4.best_estimator_  ## con esto, le estoy aplicando a los datos de TEST las mismas ediciones que apliqué al principio a los datos de TRAIN\n",
    "                                            ## o sea le aplico todo el pipeline\n",
    "best_model_RL4.score(X4_test, y4_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5º Intento  --> ha bajado el score a 53%, tengo que mantener la columna region, sin country ni province\n",
    "Copio tal cual el 4 pero quitando también region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uso este df y voy quitando cosas\n",
    "df_5 = pd.read_csv(\"..\\\\data\\\\processed\\\\vinos_var100_des20_win3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_5.drop([\"description\",\"province\",\"region_1\", \"taster_name\", \"country\"], axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>variety_100</th>\n",
       "      <th>designation_20</th>\n",
       "      <th>winery_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Pfersigberg Grand Cru</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Tradition</td>\n",
       "      <td>Kuentz-Bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5292</th>\n",
       "      <td>91</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Bel Colle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5293</th>\n",
       "      <td>94</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Bel Colle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5294</th>\n",
       "      <td>92</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Fratelli Alessandria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5295</th>\n",
       "      <td>91</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Fratelli Alessandria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5296</th>\n",
       "      <td>95</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Monvigliero</td>\n",
       "      <td>Fratelli Alessandria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5297 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      points  price  year  variety_100         designation_20  \\\n",
       "0         92   39.0  2014          7.0  Pfersigberg Grand Cru   \n",
       "1         93   59.0  2012         23.0  Pfersigberg Grand Cru   \n",
       "2         90   45.0  2014         23.0  Pfersigberg Grand Cru   \n",
       "3         91   59.0  2013         23.0  Pfersigberg Grand Cru   \n",
       "4         88   17.0  2011          7.0              Tradition   \n",
       "...      ...    ...   ...          ...                    ...   \n",
       "5292      91   70.0  2011         11.0            Monvigliero   \n",
       "5293      94   70.0  2012         11.0            Monvigliero   \n",
       "5294      92   76.0  2012         11.0            Monvigliero   \n",
       "5295      91   75.0  2011         11.0            Monvigliero   \n",
       "5296      95   78.0  2013         11.0            Monvigliero   \n",
       "\n",
       "                  winery_3  \n",
       "0               Kuentz-Bas  \n",
       "1               Kuentz-Bas  \n",
       "2               Kuentz-Bas  \n",
       "3               Kuentz-Bas  \n",
       "4               Kuentz-Bas  \n",
       "...                    ...  \n",
       "5292             Bel Colle  \n",
       "5293             Bel Colle  \n",
       "5294  Fratelli Alessandria  \n",
       "5295  Fratelli Alessandria  \n",
       "5296  Fratelli Alessandria  \n",
       "\n",
       "[5297 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ahora procedo a hacer un get_dummies\n",
    "df_5_dumm = pd.get_dummies(data = df_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len original: 5297\n",
      "Len sin outliers en price: 5076\n"
     ]
    }
   ],
   "source": [
    "## quito outliers de la columna price\n",
    "def outliers_quantie(df, feature, param=1.5):  \n",
    "        \n",
    "    iqr_ = iqr(df[feature], nan_policy='omit')\n",
    "    q1 = np.nanpercentile(df[feature], 25)\n",
    "    q3 = np.nanpercentile(df[feature], 75)\n",
    "    \n",
    "    th1 = q1 - iqr_*param\n",
    "    th2 = q3 + iqr_*param\n",
    "    \n",
    "    return df[(df[feature] >= th1) & (df[feature] <= th2)].reset_index(drop=True)\n",
    "\n",
    "df_5_dumm_no_out = outliers_quantie(df_5_dumm, 'price')  ## dejo el parámetro por defect de 1.5, así veo que me ha quitado muchos más outliers que en las pruebas anteriores\n",
    "print(\"Len original:\", len(df_5_dumm))\n",
    "print(\"Len sin outliers en price:\", len(df_5_dumm_no_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pipeline y gridsearch\n",
    "\n",
    "\n",
    "reg_log5 = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer()),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"reglog5\", LogisticRegression())\n",
    "])\n",
    "\n",
    "reg_log5_param = {\n",
    "    \"imputer__strategy\": ['mean', 'median'],\n",
    "    \"reglog5__penalty\": ['l1', 'l2'],\n",
    "    \"reglog5__C\": np.logspace(0, 4, 10)\n",
    "}\n",
    "\n",
    "gs_reg_log5 = GridSearchCV(reg_log5,\n",
    "                         reg_log5_param,\n",
    "                         cv = 10,\n",
    "                         scoring = 'accuracy',\n",
    "                         verbose = 1,\n",
    "                         n_jobs = -1)\n",
    "\n",
    "grids5 = {\"gs_reg_log5\": gs_reg_log5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X5 = df_5_dumm_no_out.drop([\"variety_100\"], axis = 1)\n",
    "y5 = np.array(df_5_dumm_no_out[\"variety_100\"]).reshape(-1,)\n",
    "\n",
    "## separo en train y test\n",
    "X5_train, X5_test, y5_train, y5_test = train_test_split(X5, y5, test_size = 0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  UserWarning,\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "200 fits failed out of a total of 400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan 0.52413793        nan 0.52561576        nan 0.52512315\n",
      "        nan 0.52315271        nan 0.52364532        nan 0.52216749\n",
      "        nan 0.52216749        nan 0.52389163        nan 0.5229064\n",
      "        nan 0.5229064         nan 0.52413793        nan 0.52561576\n",
      "        nan 0.52512315        nan 0.52315271        nan 0.52364532\n",
      "        nan 0.52216749        nan 0.52216749        nan 0.52389163\n",
      "        nan 0.5229064         nan 0.5229064 ]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "## ahora con el bucle for hago que me entrene todos los modelos que he definido, con sus gridsearches\n",
    "for nombre, grid_search in grids5.items():\n",
    "    grid_search.fit(X5_train, y5_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.525615763546798\n",
      "{'imputer__strategy': 'mean', 'reglog5__C': 2.7825594022071245, 'reglog5__penalty': 'l2'}\n",
      "Pipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler()),\n",
      "                ('reglog5', LogisticRegression(C=2.7825594022071245))])\n",
      "LogisticRegression(C=2.7825594022071245)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grid</th>\n",
       "      <th>Best score_TRAIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gs_reg_log5</td>\n",
       "      <td>0.525616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Grid  Best score_TRAIN\n",
       "0  gs_reg_log5          0.525616"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## aquí tenemos el best score, param y estimator de la Logistic Regression\n",
    "print(gs_reg_log5.best_score_)\n",
    "print(gs_reg_log5.best_params_)\n",
    "print(gs_reg_log5.best_estimator_) ## aquí nos devuelve al pipeline, no el modelo en sí. Si quiero ver el modelo, hago lo de la linea de aqui abajo\n",
    "print(gs_reg_log5.best_estimator_['reglog5'])\n",
    "\n",
    "## aquí es para ver todos juntos en un df, el mejor score de los modelos, CON LOS DATOS DE TRAIN\n",
    "best_grids5_TRAIN = [(i, j.best_score_) for i, j in grids5.items()]\n",
    "\n",
    "best_grids5_TRAIN = pd.DataFrame(best_grids5_TRAIN, columns=[\"Grid\", \"Best score_TRAIN\"]).sort_values(by=\"Best score_TRAIN\", ascending=False)\n",
    "best_grids5_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53248031496063"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El mejor modelo de regresion logistica con los datos de train, AHORA TENGO QUE VERLOS CON LOS DATOS DE TEST:\n",
    "best_model_RL5 = gs_reg_log5.best_estimator_  ## con esto, le estoy aplicando a los datos de TEST las mismas ediciones que apliqué al principio a los datos de TRAIN\n",
    "                                            ## o sea le aplico todo el pipeline\n",
    "best_model_RL5.score(X5_test, y5_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6º Intento  --> voy a dejar solo \"designation\" y \"winery\"...   --- PEOR NO SE PUEDE, ha bajado a 47%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uso este df y voy quitando cosas\n",
    "temp = pd.read_csv(\"..\\\\data\\\\processed\\\\vinos_var100_des20_win3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6 = temp[[\"variety_100\", \"designation_20\", \"winery_3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ahora procedo a hacer un get_dummies\n",
    "df_6_dumm = pd.get_dummies(data = df_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## al no tener la columna price ya no necesito quitar los outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pipeline y gridsearch\n",
    "\n",
    "\n",
    "reg_log6 = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer()),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"reglog6\", LogisticRegression())\n",
    "])\n",
    "\n",
    "reg_log6_param = {\n",
    "    \"imputer__strategy\": ['mean', 'median'],\n",
    "    \"reglog6__penalty\": ['l1', 'l2'],\n",
    "    \"reglog6__C\": np.logspace(0, 4, 10)\n",
    "}\n",
    "\n",
    "gs_reg_log6 = GridSearchCV(reg_log6,\n",
    "                         reg_log6_param,\n",
    "                         cv = 10,\n",
    "                         scoring = 'accuracy',\n",
    "                         verbose = 1,\n",
    "                         n_jobs = -1)\n",
    "\n",
    "grids6 = {\"gs_reg_log6\": gs_reg_log6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X6 = df_6_dumm.drop([\"variety_100\"], axis = 1)\n",
    "y6 = np.array(df_6_dumm[\"variety_100\"]).reshape(-1,)\n",
    "\n",
    "## separo en train y test\n",
    "X6_train, X6_test, y6_train, y6_test = train_test_split(X6, y6, test_size = 0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  UserWarning,\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "200 fits failed out of a total of 400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan 0.47108089        nan 0.47155315        nan 0.47202206\n",
      "        nan 0.47273128        nan 0.47131451        nan 0.46871906\n",
      "        nan 0.46942772        nan 0.47155036        nan 0.4661197\n",
      "        nan 0.46895546        nan 0.47108089        nan 0.47155315\n",
      "        nan 0.47202206        nan 0.47273128        nan 0.47131451\n",
      "        nan 0.46871906        nan 0.46942772        nan 0.47155036\n",
      "        nan 0.4661197         nan 0.46895546]\n",
      "  category=UserWarning,\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "## ahora con el bucle for hago que me entrene todos los modelos que he definido, con sus gridsearches\n",
    "for nombre, grid_search in grids6.items():\n",
    "    grid_search.fit(X6_train, y6_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47273127704179496\n",
      "{'imputer__strategy': 'mean', 'reglog6__C': 21.544346900318832, 'reglog6__penalty': 'l2'}\n",
      "Pipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler()),\n",
      "                ('reglog6', LogisticRegression(C=21.544346900318832))])\n",
      "LogisticRegression(C=21.544346900318832)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grid</th>\n",
       "      <th>Best score_TRAIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gs_reg_log6</td>\n",
       "      <td>0.472731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Grid  Best score_TRAIN\n",
       "0  gs_reg_log6          0.472731"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## aquí tenemos el best score, param y estimator de la Logistic Regression\n",
    "print(gs_reg_log6.best_score_)\n",
    "print(gs_reg_log6.best_params_)\n",
    "print(gs_reg_log6.best_estimator_) ## aquí nos devuelve al pipeline, no el modelo en sí. Si quiero ver el modelo, hago lo de la linea de aqui abajo\n",
    "print(gs_reg_log6.best_estimator_['reglog6'])\n",
    "\n",
    "## aquí es para ver todos juntos en un df, el mejor score de los modelos, CON LOS DATOS DE TRAIN\n",
    "best_grids6_TRAIN = [(i, j.best_score_) for i, j in grids6.items()]\n",
    "\n",
    "best_grids6_TRAIN = pd.DataFrame(best_grids6_TRAIN, columns=[\"Grid\", \"Best score_TRAIN\"]).sort_values(by=\"Best score_TRAIN\", ascending=False)\n",
    "best_grids6_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4377358490566038"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El mejor modelo de regresion logistica con los datos de train, AHORA TENGO QUE VERLOS CON LOS DATOS DE TEST:\n",
    "best_model_RL6 = gs_reg_log6.best_estimator_  ## con esto, le estoy aplicando a los datos de TEST las mismas ediciones que apliqué al principio a los datos de TRAIN\n",
    "                                            ## o sea le aplico todo el pipeline\n",
    "best_model_RL6.score(X6_test, y6_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONSIDERACIONES A ESTE PUNTO DEL PROYECTO\n",
    "No tengo forma de mejorar el score de 57,67%. No me queda más remedio que utilizar la columna \"description\", usando lo que hemos aprendido en clase relativo al NLP y usando la vectorización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>>>Imprimo por pantalla los resultados y también creo un dataframe con las columnas de las predicciones vs valores reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estas son las 1016 predicciones: [ 1.  1.  2. ...  7. 29. 11.]\n",
      "Con los datos de TEST he obtenido un score de:  0.5767716535433071\n",
      "Redondeando, tengo una precisión del 58%\n"
     ]
    }
   ],
   "source": [
    "## resultados del mejor modelo\n",
    "predictions = gs_reg_log4.predict(X4_test)\n",
    "accuracy = accuracy_score(y4_test, predictions)\n",
    "score = best_model_RL4.score(X4_test, y4_test)\n",
    "print(\"Estas son las {} predicciones: {}\".format(len(predictions),predictions))\n",
    "print(\"Con los datos de TEST he obtenido un score de: \",score)\n",
    "print(\"Redondeando, tengo una precisión del {}%\".format(round(accuracy*100)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.6000606531638769\n",
      "recall:  0.5767716535433071\n",
      "f1:  0.5832846253000225\n",
      "accuracy:  0.5767716535433071\n"
     ]
    }
   ],
   "source": [
    "print(\"precision: \", precision_score(y4_test, predictions, average='weighted', zero_division= 0))\n",
    "print(\"recall: \", recall_score(y4_test, predictions, average='weighted', zero_division= 0))\n",
    "print(\"f1: \", f1_score(y4_test, predictions, average='weighted', zero_division= 0))\n",
    "print(\"accuracy: \",accuracy_score(y4_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[97 12  0 ...  0  0  0]\n",
      " [20 83  1 ...  0  0  0]\n",
      " [ 0  1 25 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  1  0  0]\n",
      " [ 0  0  0 ...  0  2  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# matriz de confusión\n",
    "c_matrix = confusion_matrix(y4_test, gs_reg_log4.predict(X4_test))\n",
    "print(c_matrix)\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "sns.heatmap(c_matrix, annot=True);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PREDICCIONES</th>\n",
       "      <th>VALOR REAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PREDICCIONES  VALOR REAL\n",
       "0           1.0         2.0\n",
       "1           1.0        23.0\n",
       "2           2.0         1.0\n",
       "3           8.0         8.0\n",
       "4           7.0        23.0"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### DESPUÉS DE HABER VISTO QUE EL MODELO DEFINITIVO ES EL 4º\n",
    "## quiero guardar los resultados de predicciones vs valores reales en un dataframe\n",
    "## PERO, como se puede ver, los tengo aún con valores numéricos y si bien puedo ver si dos \n",
    "## valores son iguales o no, lo que yo quiero ver es a qué nombre de vino corresponde cada número\n",
    "###### lo haré en la siguiente celda aquí abajo\n",
    "df_preds = pd.DataFrame(predictions, columns = [\"PREDICCIONES\"])\n",
    "df_preds[\"VALOR REAL\"] = pd.Series(y4_test)\n",
    "df_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variety</th>\n",
       "      <th>variety_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Red Blend</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      variety  variety_100\n",
       "0  Pinot Noir            1\n",
       "1  Chardonnay            2\n",
       "2   Red Blend            3"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars_100.head(3)  ## este es el dataframe con la leyenda (a cada variedad corresponde un número)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx   ## este es el diccionario con la leyenda de los vinos\n",
    "\n",
    "## creo la función que me aplica, a cada número, su nombre real\n",
    "def decodificar_valores(x):\n",
    "    for pos,val in num_to_string.items():\n",
    "        if x==val:\n",
    "            x=pos\n",
    "            return x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplico la función a las 2 columnas del dataframe de las predicciones\n",
    "\n",
    "## defino el diccionario con la leyenda de los valores (char2idx  --> lo creé al principio del notebook para transformar la columna \"variety\" a numérica)\n",
    "num_to_string = char2idx\n",
    "## aplico la función\n",
    "df_preds[\"VALOR REAL nombre\"] = df_preds[\"VALOR REAL\"].apply(decodificar_valores)\n",
    "df_preds[\"PREDICCIONES nombre\"] = df_preds[\"PREDICCIONES\"].apply(decodificar_valores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALOR REAL nombre</th>\n",
       "      <th>PREDICCIONES nombre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Red Blend</td>\n",
       "      <td>Chardonnay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shiraz</td>\n",
       "      <td>Chardonnay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>Red Blend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rosé</td>\n",
       "      <td>Rosé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shiraz</td>\n",
       "      <td>Malbec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  VALOR REAL nombre PREDICCIONES nombre\n",
       "0         Red Blend          Chardonnay\n",
       "1            Shiraz          Chardonnay\n",
       "2        Chardonnay           Red Blend\n",
       "3              Rosé                Rosé\n",
       "4            Shiraz              Malbec"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## como resultado final, ya no me interesa tener números, quiero ver el nombre del vino, si el modelo lo ha adivinado o no\n",
    "df_preds_variety = df_preds[[\"VALOR REAL nombre\", \"PREDICCIONES nombre\"]]\n",
    "df_preds_variety.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "## guardo este dataframe con las predicciones\n",
    "df_preds_variety.to_csv(\"..\\\\data\\\\processed\\\\predicciones_vs_reales.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALOR REAL nombre</th>\n",
       "      <th>PREDICCIONES nombre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Red Blend</td>\n",
       "      <td>Chardonnay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shiraz</td>\n",
       "      <td>Chardonnay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>Red Blend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rosé</td>\n",
       "      <td>Rosé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shiraz</td>\n",
       "      <td>Malbec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  VALOR REAL nombre PREDICCIONES nombre\n",
       "0         Red Blend          Chardonnay\n",
       "1            Shiraz          Chardonnay\n",
       "2        Chardonnay           Red Blend\n",
       "3              Rosé                Rosé\n",
       "4            Shiraz              Malbec"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicciones = pd.read_csv(\"..\\\\data\\\\processed\\\\predicciones_vs_reales.csv\")\n",
    "predicciones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALOR REAL nombre</th>\n",
       "      <th>PREDICCIONES nombre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rosé</td>\n",
       "      <td>Rosé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rosé</td>\n",
       "      <td>Rosé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Syrah</td>\n",
       "      <td>Syrah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Cabernet Franc</td>\n",
       "      <td>Cabernet Franc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shiraz</td>\n",
       "      <td>Shiraz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VALOR REAL nombre PREDICCIONES nombre\n",
       "3               Rosé                Rosé\n",
       "7               Rosé                Rosé\n",
       "8              Syrah               Syrah\n",
       "10    Cabernet Franc      Cabernet Franc\n",
       "11            Shiraz              Shiraz"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### quiero ver, de los 1016 registros, cuántos he acertado:\n",
    "aciertos = predicciones[predicciones[\"VALOR REAL nombre\"] == predicciones[\"PREDICCIONES nombre\"]]\n",
    "aciertos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De los 1016 registros de test, el modelo ha acertado 586\n"
     ]
    }
   ],
   "source": [
    "print(\"De los {} registros de test, el modelo ha acertado {}\".format(len(predicciones), len(aciertos)))\n",
    "### Efectivamente, poco más de la mitad, me cuadra que el score fuera de un 58% aprox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardo el modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../models/Modelo_Reg_Log.model'   ### es un archivo tipo pickel que me está guardando mi modelo (se crea cuando ejecuto esta celda)\n",
    "\n",
    "with open(filename, 'wb') as archivo_salida:\n",
    "    pickle.dump(best_model_RL4, archivo_salida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hago una prueba de cargar ese modelo guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler()),\n",
      "                ('reglog4',\n",
      "                 LogisticRegression(C=21.544346900318832, max_iter=1000))])\n"
     ]
    }
   ],
   "source": [
    "with open('../models/Modelo_Reg_Log.model', \"rb\") as archivo_entrada:\n",
    "    pipeline_importada = pickle.load(archivo_entrada)\n",
    "    \n",
    "print(pipeline_importada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSIDERACIONES FINALES Y DECLARACIÓN DEL MODELO ELEGIDO\n",
    "Después de hacer la última prueba, usando las review (en el notebook 04), veo que me da hasta peores resultados que los que he obtenido hasta aquí, no llegando ni a un 50% de aciertos.\n",
    "\n",
    "Finalmente mi modelo definitivo es él que he guardado aquí, el de Logistic Regression, con un porcentaje de aciertos de casi un 58%.\n",
    "\n",
    "Al principio estaba un poco mosqueada con el resultado, pero reflexionando mejor puedo sascar unos puntos positivos a mi proyectos:\n",
    "- he partido de unos scores bajísimos cuando era un problema de regresion (un 0.44 de score y el RMSE mas bajo era de 21, o sea un error de 21$ sobre los precios, para nada bueno) y los he ido mejorando conforme iba cambiando el enfoque de mi proyecto, pasando finalmente a un problema de clasificación, intentando predecir el tipo de vino (la variedad) en vez que su precio.\n",
    "- También he ido mejorando poco a poco los scores con el problema de clasificación, pasando de un 43% aprox (con alguna caída hasta el 12% y 19% de aciertos) hasta llegar finalmente a un casi 58% de aciertos (57,67% para ser exactos).\n",
    "- Está claro que hasta el mejor sommelier, frente a una cata a ciega, no siempre (o practicamente nunca) llega a un porcentaje de aciertos del 100%, así que me conformaré con mi resultado y puede que, más adelante, lo varia revisando con más técnicas que vaya aprendiendo por el camino y, quizas, pueda obtener unos resultados mejores."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "075b29a0b0cbaa9c7a84b41610d8b6dbe2b274ccaf0a9c61c1820b677d81dee1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
